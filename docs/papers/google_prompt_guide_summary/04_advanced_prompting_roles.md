# ğŸ“˜ 04. ê³ ê¸‰ í”„ë¡¬í”„íŒ… - ì‹œìŠ¤í…œ, ì—­í• , ë¬¸ë§¥ í”„ë¡¬í”„íŒ… (Advanced Prompting: System, Role & Contextual)

## í•µì‹¬ ìš”ì•½
- **System, Role, Contextual í”„ë¡¬í”„íŒ…**ì€ ëª¨ë‘ LLMì˜ í…ìŠ¤íŠ¸ ìƒì„±ì„ ì•ˆë‚´í•˜ì§€ë§Œ, ê°ê° ë‹¤ë¥¸ ì¸¡ë©´ì— ì´ˆì ì„ ë§ì¶¤
- **System í”„ë¡¬í”„íŒ…**ì€ ëª¨ë¸ì˜ ì „ë°˜ì ì¸ ë§¥ë½ê³¼ ëª©ì ì„ ì„¤ì •í•˜ì—¬ ê¸°ë³¸ì ì¸ ëŠ¥ë ¥ê³¼ ëª©í‘œë¥¼ ì •ì˜
- **Role í”„ë¡¬í”„íŒ…**ì€ ëª¨ë¸ì—ê²Œ íŠ¹ì •í•œ ìºë¦­í„°ë‚˜ ì •ì²´ì„±ì„ ë¶€ì—¬í•˜ì—¬ ì¼ê´€ëœ ì‘ë‹µ ìŠ¤íƒ€ì¼ê³¼ ì „ë¬¸ì„±ì„ ì œê³µ
- **Contextual í”„ë¡¬í”„íŒ…**ì€ í˜„ì¬ ì‘ì—…ì— íŠ¹ì •í•œ ì„¸ë¶€ ì •ë³´ì™€ ë°°ê²½ ì •ë³´ë¥¼ ì œê³µí•˜ì—¬ ì‘ë‹µì˜ ì •í™•ì„±ê³¼ ê´€ë ¨ì„±ì„ í–¥ìƒ

## ì£¼ìš” ê°œë…ê³¼ ì„¤ëª…

### ğŸ”§ **System í”„ë¡¬í”„íŒ… (System Prompting)**
- **ëª©ì **: ëª¨ë¸ì˜ ê¸°ë³¸ì ì¸ ëŠ¥ë ¥ê³¼ ì „ì²´ì ì¸ ëª©í‘œ ì •ì˜
- **íŠ¹ì§•**: ì¶œë ¥ í˜•ì‹, ìŠ¤íƒ€ì¼, êµ¬ì¡°ì— ëŒ€í•œ ì¶”ê°€ì ì¸ ìš”êµ¬ì‚¬í•­ ëª…ì‹œ
- **í™œìš©**: íŠ¹ì • í˜•ì‹(JSON, XML), ì•ˆì „ì„± ì œì–´, ë…ì„± ë°©ì§€ ë“±
- **ì˜ˆì‹œ**: "ì˜í™” ë¦¬ë·°ë¥¼ ê¸ì •/ë¶€ì •/ì¤‘ë¦½ìœ¼ë¡œë§Œ ë¶„ë¥˜í•˜ì—¬ ëŒ€ë¬¸ìë¡œ ë°˜í™˜"

### ğŸ­ **Role í”„ë¡¬í”„íŒ… (Role Prompting)**
- **ëª©ì **: ëª¨ë¸ì—ê²Œ íŠ¹ì • ì—­í• ì´ë‚˜ ì „ë¬¸ì„±ì„ ë¶€ì—¬
- **ì¥ì **: ì¼ê´€ëœ ì–´ì¡°, ìŠ¤íƒ€ì¼, ì „ë¬¸ ì§€ì‹ì„ ê°€ì§„ ì‘ë‹µ ìƒì„±
- **í™œìš© ë¶„ì•¼**: êµìœ¡ì, ì—¬í–‰ ê°€ì´ë“œ, ì»¨ì„¤í„´íŠ¸, ì½”ë”© ë„ìš°ë¯¸ ë“±
- **ìŠ¤íƒ€ì¼ ì˜µì…˜**: ëŒ€ë¦½ì , ì„¤ëª…ì , ì§ì ‘ì , í˜•ì‹ì , ìœ ë¨¸ëŸ¬ìŠ¤, ì˜í–¥ë ¥ ìˆëŠ”, ë¹„ê³µì‹ì , ì˜ê°ì„ ì£¼ëŠ”, ì„¤ë“ì 

### ğŸ“‹ **Contextual í”„ë¡¬í”„íŒ… (Contextual Prompting)**
- **ëª©ì **: í˜„ì¬ ëŒ€í™”ë‚˜ ì‘ì—…ê³¼ ê´€ë ¨ëœ êµ¬ì²´ì ì¸ ì„¸ë¶€ì‚¬í•­ê³¼ ë°°ê²½ ì •ë³´ ì œê³µ
- **íŠ¹ì§•**: ì‘ì—…ë³„ë¡œ ë™ì ì´ë©° ë†’ì€ íŠ¹ìˆ˜ì„±ì„ ê°€ì§
- **íš¨ê³¼**: ëª¨ë¸ì˜ ìš”ì²­ ì´í•´ë ¥ í–¥ìƒ, ë” ì •í™•í•˜ê³  ê´€ë ¨ì„± ë†’ì€ ì‘ë‹µ ìƒì„±

## í”„ë¡¬í”„íŠ¸ ì˜ˆì‹œ

### System í”„ë¡¬í”„íŒ… - ê¸°ë³¸ ë¶„ë¥˜
```plaintext
# ì‹œìŠ¤í…œ ì„¤ì •
ì˜í™” ë¦¬ë·°ë¥¼ ê¸ì •, ë¶€ì •, ì¤‘ë¦½ìœ¼ë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤. 
ê²°ê³¼ëŠ” ëŒ€ë¬¸ìë¡œë§Œ ë°˜í™˜í•˜ì„¸ìš”.

# ì…ë ¥
ë¦¬ë·°: "Her"ëŠ” AIê°€ ê³„ì† ì§„í™”í•  ê²½ìš° ì¸ë¥˜ê°€ í–¥í•˜ëŠ” ë°©í–¥ì„ 
ë³´ì—¬ì£¼ëŠ” ì¶©ê²©ì ì¸ ì—°êµ¬ì…ë‹ˆë‹¤. ë„ˆë¬´ ì¶©ê²©ì ì´ì–´ì„œ ëê¹Œì§€ ë³¼ ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤.

ê°ì •:
```
**ì¶œë ¥**: `NEGATIVE`

### System í”„ë¡¬í”„íŒ… - JSON í˜•ì‹
```plaintext
# ì‹œìŠ¤í…œ ì„¤ì •
ì˜í™” ë¦¬ë·°ë¥¼ ê¸ì •, ë¶€ì •, ì¤‘ë¦½ìœ¼ë¡œ ë¶„ë¥˜í•˜ê³  ìœ íš¨í•œ JSONìœ¼ë¡œ ë°˜í™˜í•˜ì„¸ìš”.

# ìŠ¤í‚¤ë§ˆ
```
MOVIE: {
  "sentiment": String "POSITIVE" | "NEGATIVE" | "NEUTRAL",
  "name": String
}
```

# ì…ë ¥
ë¦¬ë·°: "Her"ëŠ” AIê°€ ê³„ì† ì§„í™”í•  ê²½ìš° ì¸ë¥˜ê°€ í–¥í•˜ëŠ” ë°©í–¥ì„ 
ë³´ì—¬ì£¼ëŠ” ì¶©ê²©ì ì¸ ì—°êµ¬ì…ë‹ˆë‹¤. ë„ˆë¬´ ì¶©ê²©ì ì´ì–´ì„œ ëê¹Œì§€ ë³¼ ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤.

JSON ì‘ë‹µ:
```
**ì¶œë ¥**:
```json
{
  "sentiment": "NEGATIVE",
  "name": "Her"
}
```

### Role í”„ë¡¬í”„íŒ… - ì—¬í–‰ ê°€ì´ë“œ
```plaintext
# ì—­í•  ì„¤ì •
ë‹¹ì‹ ì€ ì—¬í–‰ ê°€ì´ë“œì…ë‹ˆë‹¤. ì œê°€ ìœ„ì¹˜ë¥¼ ì•Œë ¤ë“œë¦¬ë©´ 
ê·¼ì²˜ì—ì„œ ë°©ë¬¸í• ë§Œí•œ 3ê³³ì„ ì¶”ì²œí•´ì£¼ì„¸ìš”.

# ìš”ì²­
ì €ëŠ” ì§€ê¸ˆ ì•”ìŠ¤í…Œë¥´ë‹´ì— ìˆê³  ë°•ë¬¼ê´€ë§Œ ë°©ë¬¸í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤.

ì—¬í–‰ ì œì•ˆ:
```
**ì¶œë ¥**:
1. **ë¼ìµìŠ¤ë®¤ì§€ì—„**: ë ˜ë¸Œë€íŠ¸ì˜ "ì•¼ê²½"ì„ í¬í•¨í•œ ë„¤ëœë€ë“œ ê±¸ì‘ë“¤ë¡œ ìœ ëª…í•œ ì„¸ê³„ì ì¸ ë°•ë¬¼ê´€
2. **ë°˜ ê³ í ë®¤ì§€ì—„**: "ë³„ì´ ë¹›ë‚˜ëŠ” ë°¤"ê³¼ "í•´ë°”ë¼ê¸°"ë¥¼ í¬í•¨í•œ ë°˜ ê³ í ì‘í’ˆì˜ ìµœëŒ€ ì»¬ë ‰ì…˜
3. **ìŠ¤í…Œë¸ë¦­ í˜„ëŒ€ë¯¸ìˆ ê´€**: í”¼ì¹´ì†Œ, ì¹¸ë”˜ìŠ¤í‚¤, ë¦¬íŠ¸ë²¨íŠ¸ ì‘í’ˆì´ ìˆëŠ” í˜„ëŒ€ ê±´ì¶•ë¬¼ì˜ í˜„ëŒ€ë¯¸ìˆ ê´€

### Role í”„ë¡¬í”„íŒ… - ìœ ë¨¸ëŸ¬ìŠ¤ ìŠ¤íƒ€ì¼
```plaintext
# ì—­í•  ì„¤ì •
ìœ ë¨¸ëŸ¬ìŠ¤í•œ ìŠ¤íƒ€ì¼ë¡œ ì—¬í–‰ ê°€ì´ë“œ ì—­í• ì„ í•´ì£¼ì„¸ìš”. 
ì œê°€ ìœ„ì¹˜ë¥¼ ì•Œë ¤ë“œë¦¬ë©´ ê·¼ì²˜ì—ì„œ ë°©ë¬¸í• ë§Œí•œ 3ê³³ì„ ì¶”ì²œí•´ì£¼ì„¸ìš”.

# ìš”ì²­
ì €ëŠ” ì§€ê¸ˆ ë§¨í•´íŠ¼ì— ìˆìŠµë‹ˆë‹¤.

ì—¬í–‰ ì œì•ˆ:
```
**ì¶œë ¥**:
1. **ì— íŒŒì´ì–´ ìŠ¤í…Œì´íŠ¸ ë¹Œë”©**: ê±°ëŒ€í•œ ë°”ë‚˜ë‚˜ ì—†ëŠ” í‚¹ì½©ì´ ëœ ê¸°ë¶„ìœ¼ë¡œ ë§¨í•´íŠ¼ ìŠ¤ì¹´ì´ë¼ì¸ì„ ê°ìƒí•˜ì„¸ìš”
2. **ëª¨ë§ˆ(MoMA) ë¯¸ìˆ ê´€**: ë‹¹ì‹ ì˜ ë§‰ëŒ€ê¸° ê·¸ë¦¼ì´ ì˜ˆìˆ ì  ê°€ì¹˜ê°€ ìˆëŠ”ì§€ ì˜ë¬¸ì„ ê°–ê²Œ ë§Œë“œëŠ” ê±¸ì‘ë“¤ì„ ê°ìƒí•˜ì„¸ìš”
3. **5ë²ˆê°€ ì‡¼í•‘**: ì§€ê°‘ì´ ìš¸ê²Œ ë§Œë“œëŠ” ë””ìì´ë„ˆ ë¶€í‹°í¬ì—ì„œ ì‡¼í•‘í•˜ë©° ë§¨í•´íŠ¼ì˜ í™”ë ¤í•¨ì„ ë§Œë½í•˜ì„¸ìš”

### Contextual í”„ë¡¬í”„íŒ… - ë¸”ë¡œê·¸ ì£¼ì œ ì œì•ˆ
```plaintext
# ë¬¸ë§¥ ì„¤ì •
ë§¥ë½: ë‹¹ì‹ ì€ 1980ë…„ëŒ€ ë ˆíŠ¸ë¡œ ì•„ì¼€ì´ë“œ ë¹„ë””ì˜¤ ê²Œì„ì— ê´€í•œ ë¸”ë¡œê·¸ë¥¼ ì‘ì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤.

# ìš”ì²­
ê¸°ì‚¬ë¡œ ì“¸ 3ê°€ì§€ ì£¼ì œë¥¼ ëª‡ ì¤„ì˜ ì„¤ëª…ê³¼ í•¨ê»˜ ì œì•ˆí•´ì£¼ì„¸ìš”.

```
**ì¶œë ¥**:
1. **ì•„ì¼€ì´ë“œ ìºë¹„ë‹› ë””ìì¸ì˜ ì§„í™”**: 1970ë…„ëŒ€ ëª©ì¬ì™€ ê¸ˆì† ìºë¹„ë‹›ë¶€í„° 1980ë…„ëŒ€ ë„¤ì˜¨ ë¼ì´íŠ¸ê°€ ìˆëŠ” ì„¸ë ¨ëœ ë””ìì¸ê¹Œì§€ì˜ ë³€í™” íƒêµ¬
2. **80ë…„ëŒ€ ì•„ì´ì½”ë‹‰ ì•„ì¼€ì´ë“œ ê²Œì„ë“¤**: ì—­ì‚¬ì  ì˜í–¥ê³¼ ìœ ì‚°ì„ ë‚¨ê¸´ ê°€ì¥ ìƒì§•ì ì¸ ì•„ì¼€ì´ë“œ ê²Œì„ë“¤ì˜ í˜ì‹ ê³¼ ì¸ê¸° ë©”ì»¤ë‹ˆì¦˜ ì†Œê°œ
3. **í”½ì…€ ì•„íŠ¸ì˜ ë¶€ìƒê³¼ ë ˆíŠ¸ë¡œ ë¶€í™œ**: ì´ˆê¸° ì•„ì¼€ì´ë“œ ê²Œì„ì˜ ì‹œê°ì  ìŠ¤íƒ€ì¼ë¡œì„œ í”½ì…€ ì•„íŠ¸ì˜ ì§„í™”ì™€ í˜„ëŒ€ ê²Œì„ì—ì„œì˜ ë¶€í™œ

## í™œìš© íŒ

### ğŸš€ **LangChainì—ì„œì˜ êµ¬í˜„**

#### System í”„ë¡¬í”„íŒ… êµ¬í˜„
```python
from langchain.prompts import PromptTemplate
from langchain.llms import VertexAI
from langchain.chains import LLMChain

# System í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
system_template = """
ì‹œìŠ¤í…œ ì§€ì‹œì‚¬í•­: {system_instruction}

ì‘ì—…: {task}
ì…ë ¥: {input}

{output_format}
"""

system_prompt = PromptTemplate(
    input_variables=["system_instruction", "task", "input", "output_format"],
    template=system_template
)

# JSON ì¶œë ¥ì„ ìœ„í•œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
json_chain = LLMChain(
    llm=VertexAI(temperature=0.1),
    prompt=system_prompt
)

result = json_chain.run(
    system_instruction="ì˜í™” ë¦¬ë·°ë¥¼ ë¶„ë¥˜í•˜ê³  JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•˜ì„¸ìš”.",
    task="ê°ì • ë¶„ì„",
    input="ì´ ì˜í™”ëŠ” ì •ë§ í›Œë¥­í–ˆìŠµë‹ˆë‹¤!",
    output_format='JSON ì‘ë‹µ: {"sentiment": "...", "confidence": 0.95}'
)
```

#### Role í”„ë¡¬í”„íŒ… êµ¬í˜„
```python
from langchain.prompts import PromptTemplate

# Role ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
role_template = """
ì—­í• : {role}
ìŠ¤íƒ€ì¼: {style}
ì „ë¬¸ ë¶„ì•¼: {expertise}

{context}

ìš”ì²­: {request}
ì‘ë‹µ:"""

role_prompt = PromptTemplate(
    input_variables=["role", "style", "expertise", "context", "request"],
    template=role_template
)

# ì—¬í–‰ ê°€ì´ë“œ ì²´ì¸
travel_chain = LLMChain(llm=VertexAI(temperature=0.8), prompt=role_prompt)

result = travel_chain.run(
    role="ê²½í—˜ì´ í’ë¶€í•œ ì—¬í–‰ ê°€ì´ë“œ",
    style="ì¹œê·¼í•˜ê³  ìœ ë¨¸ëŸ¬ìŠ¤",
    expertise="ìœ ëŸ½ ë¬¸í™” ê´€ê´‘",
    context="ê³ ê°ì€ 3ì„¸ ì•„ì´ì™€ í•¨ê»˜ ì—¬í–‰ ì¤‘",
    request="ì•”ìŠ¤í…Œë¥´ë‹´ì—ì„œ ê°€ì¡±ì´ í•¨ê»˜ ì¦ê¸¸ ìˆ˜ ìˆëŠ” ì¥ì†Œ 3ê³³ ì¶”ì²œ"
)
```

#### Contextual í”„ë¡¬í”„íŒ… êµ¬í˜„
```python
# ë™ì  ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ í´ë˜ìŠ¤
class ContextualPromptManager:
    def __init__(self):
        self.context_history = []
        
    def add_context(self, context_type: str, content: str):
        self.context_history.append({
            "type": context_type,
            "content": content,
            "timestamp": datetime.now()
        })
    
    def build_contextual_prompt(self, current_request: str):
        context_str = "\n".join([
            f"{ctx['type']}: {ctx['content']}" 
            for ctx in self.context_history[-3:]  # ìµœê·¼ 3ê°œ ì»¨í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©
        ])
        
        return f"""
ë°°ê²½ ì •ë³´:
{context_str}

í˜„ì¬ ìš”ì²­: {current_request}

ìœ„ ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬ ì‘ë‹µí•´ì£¼ì„¸ìš”:"""

# ì‚¬ìš© ì˜ˆì‹œ
context_manager = ContextualPromptManager()
context_manager.add_context("í”„ë¡œì íŠ¸", "LangChainì„ ì‚¬ìš©í•œ ì±—ë´‡ ê°œë°œ")
context_manager.add_context("ëª©í‘œ", "ê³ ê° ë¬¸ì˜ ìë™ ì‘ë‹µ ì‹œìŠ¤í…œ êµ¬ì¶•")
context_manager.add_context("ì œì•½ì‚¬í•­", "í•œêµ­ì–´ ì§€ì› í•„ìˆ˜, JSON í˜•ì‹ ì¶œë ¥")

prompt = context_manager.build_contextual_prompt(
    "ì‚¬ìš©ì ì˜ë„ ë¶„ë¥˜ë¥¼ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”"
)
```

### ğŸ¯ **Gemini API í™œìš©**

#### ë‹¤ì¤‘ í”„ë¡¬í”„íŒ… ê¸°ë²• ê²°í•©
```python
import vertexai
from vertexai.generative_models import GenerativeModel, GenerationConfig

class AdvancedPromptEngine:
    def __init__(self, model_name="gemini-pro"):
        self.model = GenerativeModel(model_name)
        
    def system_role_contextual_prompt(self, system_config, role_config, context_config, user_request):
        """System + Role + Contextual í”„ë¡¬í”„íŒ… í†µí•©"""
        
        prompt = f"""
=== ì‹œìŠ¤í…œ ì„¤ì • ===
{system_config['instruction']}
ì¶œë ¥ í˜•ì‹: {system_config.get('format', 'ììœ  í˜•ì‹')}
ì œì•½ì‚¬í•­: {system_config.get('constraints', 'ì—†ìŒ')}

=== ì—­í•  ì„¤ì • ===
ì—­í• : {role_config['role']}
ì „ë¬¸ì„±: {role_config['expertise']}
ì–´ì¡°: {role_config.get('tone', 'ì „ë¬¸ì ')}

=== ë§¥ë½ ì •ë³´ ===
{context_config['background']}

=== ì‚¬ìš©ì ìš”ì²­ ===
{user_request}

=== ì‘ë‹µ ===
"""
        
        config = GenerationConfig(
            temperature=system_config.get('temperature', 0.3),
            max_output_tokens=system_config.get('max_tokens', 1024)
        )
        
        response = self.model.generate_content(prompt, generation_config=config)
        return response.text

# ì‚¬ìš© ì˜ˆì‹œ
engine = AdvancedPromptEngine()

result = engine.system_role_contextual_prompt(
    system_config={
        'instruction': 'ì½”ë“œ ì„¤ëª…ì„ í•œêµ­ì–´ë¡œ ì œê³µí•˜ë˜, ì´ˆë³´ìë„ ì´í•´í•  ìˆ˜ ìˆê²Œ ì‘ì„±',
        'format': 'ë‹¨ê³„ë³„ ì„¤ëª… + ì½”ë“œ ì˜ˆì‹œ',
        'temperature': 0.2,
        'max_tokens': 1500
    },
    role_config={
        'role': 'ì¹œì ˆí•œ í”„ë¡œê·¸ë˜ë° ê°•ì‚¬',
        'expertise': 'Python, LangChain',
        'tone': 'ì¹œê·¼í•˜ê³  ì„¤ëª…ì '
    },
    context_config={
        'background': 'LangChainì„ ì²˜ìŒ ë°°ìš°ëŠ” ê°œë°œìë¥¼ ìœ„í•œ íŠœí† ë¦¬ì–¼ ì œì‘ ì¤‘'
    },
    user_request='LangChainì˜ PromptTemplate ì‚¬ìš©ë²•ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”'
)
```

### ğŸ“Š **í”„ë¡¬í”„íŠ¸ ìœ í˜•ë³„ ìµœì í™” ê°€ì´ë“œ**

| í”„ë¡¬í”„íŠ¸ ìœ í˜• | ìµœì  Temperature | ê¶Œì¥ í† í° ìˆ˜ | ì£¼ìš” í™œìš© ë¶„ì•¼ |
|---------------|------------------|--------------|----------------|
| System | 0.0-0.2 | 50-200 | ë¶„ë¥˜, ë³€í™˜, í˜•ì‹í™” |
| Role | 0.3-0.7 | 200-1000 | ì°½ì‘, ìƒë‹´, êµìœ¡ |
| Contextual | 0.2-0.5 | 300-1500 | ë¶„ì„, ìš”ì•½, ë§ì¶¤ ì‘ë‹µ |
| í˜¼í•© | 0.1-0.4 | 500-2000 | ë³µì¡í•œ ì‘ì—…, ì „ë¬¸ ì„œë¹„ìŠ¤ |

### ğŸ”„ **ë™ì  í”„ë¡¬í”„íŠ¸ ì„ íƒ ì „ëµ**
```python
class SmartPromptSelector:
    def __init__(self):
        self.prompt_strategies = {
            'classification': self._system_prompt,
            'creative': self._role_prompt,
            'analysis': self._contextual_prompt,
            'complex': self._hybrid_prompt
        }
    
    def select_strategy(self, task_type: str, complexity: str, creativity_needed: bool):
        """ì‘ì—… íŠ¹ì„±ì— ë”°ë¥¸ ìµœì  í”„ë¡¬í”„íŠ¸ ì „ëµ ì„ íƒ"""
        
        if task_type in ['classify', 'extract', 'format']:
            return 'classification'
        elif creativity_needed and complexity == 'low':
            return 'creative'
        elif complexity == 'high' and not creativity_needed:
            return 'analysis'
        else:
            return 'complex'
    
    def _system_prompt(self, **kwargs):
        return f"ì‹œìŠ¤í…œ ì§€ì‹œ: {kwargs['task']}\nì…ë ¥: {kwargs['input']}"
    
    def _role_prompt(self, **kwargs):
        return f"ì—­í• : {kwargs['role']}\nìš”ì²­: {kwargs['request']}"
    
    def _contextual_prompt(self, **kwargs):
        return f"ë§¥ë½: {kwargs['context']}\në¶„ì„ ìš”ì²­: {kwargs['request']}"
    
    def _hybrid_prompt(self, **kwargs):
        return f"""
ì‹œìŠ¤í…œ: {kwargs['system']}
ì—­í• : {kwargs['role']}  
ë§¥ë½: {kwargs['context']}
ìš”ì²­: {kwargs['request']}
"""

# ì‚¬ìš© ì˜ˆì‹œ
selector = SmartPromptSelector()
strategy = selector.select_strategy('creative', 'medium', True)
```

### ğŸ›¡ï¸ **ì•ˆì „ì„± ë° í’ˆì§ˆ ê´€ë¦¬**
```python
class SafePromptValidator:
    def __init__(self):
        self.safety_checks = [
            self._check_harmful_content,
            self._check_bias_potential,
            self._validate_output_format
        ]
    
    def validate_prompt(self, prompt_text: str, expected_output_type: str):
        """í”„ë¡¬í”„íŠ¸ ì•ˆì „ì„± ë° í’ˆì§ˆ ê²€ì¦"""
        results = []
        
        for check in self.safety_checks:
            result = check(prompt_text, expected_output_type)
            results.append(result)
        
        return {
            'is_safe': all(r['passed'] for r in results),
            'issues': [r['issue'] for r in results if not r['passed']],
            'suggestions': [r['suggestion'] for r in results if r.get('suggestion')]
        }
    
    def _check_harmful_content(self, prompt, output_type):
        # ìœ í•´ ì½˜í…ì¸  ì²´í¬ ë¡œì§
        return {'passed': True, 'issue': None}
    
    def _check_bias_potential(self, prompt, output_type):
        # í¸í–¥ì„± ì²´í¬ ë¡œì§
        return {'passed': True, 'issue': None}
    
    def _validate_output_format(self, prompt, expected_type):
        # ì¶œë ¥ í˜•ì‹ ìœ íš¨ì„± ì²´í¬
        return {'passed': True, 'issue': None}

# ì‹œìŠ¤í…œì— ì•ˆì „ì„± ê²€ì¦ ì¶”ê°€
validator = SafePromptValidator()
validation_result = validator.validate_prompt(prompt, 'json')

if validation_result['is_safe']:
    # í”„ë¡¬í”„íŠ¸ ì‹¤í–‰
    response = model.generate_content(prompt)
else:
    print(f"í”„ë¡¬í”„íŠ¸ ì•ˆì „ì„± ë¬¸ì œ: {validation_result['issues']}")
```

---

* ì¶œì²˜
  * [1] [Prompt Engineering from Google](https://www.kaggle.com/whitepaper-prompt-engineering)
