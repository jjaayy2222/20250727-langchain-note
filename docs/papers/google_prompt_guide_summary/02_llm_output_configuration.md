# ğŸ“˜ 02. LLM ì¶œë ¥ ì„¤ì • (LLM Output Configuration)

## í•µì‹¬ ìš”ì•½
- **ì¶œë ¥ ê¸¸ì´ ì œì–´**ëŠ” ì„±ëŠ¥ê³¼ ë¹„ìš©ì— ì§ì ‘ì ì¸ ì˜í–¥ì„ ë¯¸ì¹˜ë©°, í† í° ìˆ˜ê°€ ë§ì„ìˆ˜ë¡ ë” ë§ì€ ê³„ì‚° ë¹„ìš©ê³¼ ì‹œê°„ì´ ì†Œìš”ë¨
- **Temperature, Top-K, Top-P**ëŠ” LLMì˜ ì°½ì˜ì„±ê³¼ ì¼ê´€ì„±ì„ ì¡°ì ˆí•˜ëŠ” í•µì‹¬ ì„¤ì •ìœ¼ë¡œ, ì‘ì—… ìœ í˜•ì— ë”°ë¼ ìµœì í™”ê°€ í•„ìš”
- **ì„¤ì •ë“¤ì˜ ìƒí˜¸ì‘ìš©**ì„ ì´í•´í•˜ê³  ì¡°í•©í•˜ì—¬ ì‚¬ìš©í•´ì•¼ í•˜ë©°, ê·¹ë‹¨ì ì¸ ê°’ ì„¤ì • ì‹œ ë‹¤ë¥¸ ì„¤ì •ë“¤ì´ ë¬´íš¨í™”ë  ìˆ˜ ìˆìŒ
- **ë°˜ë³µ ë£¨í”„ ë²„ê·¸** ë°©ì§€ë¥¼ ìœ„í•´ ì ì ˆí•œ ì„¤ì • ê· í˜•ì„ ë§ì¶°ì•¼ í•¨

---

## ì£¼ìš” ê°œë…ê³¼ ì„¤ëª…

### ğŸ¯ **ì¶œë ¥ ê¸¸ì´ (Output Length)**
- **í† í° ì œí•œ**: ì‘ë‹µì—ì„œ ìƒì„±í•  í† í° ìˆ˜ë¥¼ ì œì–´
- **ë¹„ìš© ì˜í–¥**: ë” ë§ì€ í† í° = ë” ë†’ì€ ì—ë„ˆì§€ ì†Œë¹„ + ëŠë¦° ì‘ë‹µ + ë†’ì€ ë¹„ìš©
- **ì£¼ì˜ì‚¬í•­**: ì¶œë ¥ ê¸¸ì´ ì œí•œì€ ëª¨ë¸ì„ ê°„ê²°í•˜ê²Œ ë§Œë“¤ì§€ ì•Šê³ , ë‹¨ìˆœíˆ í† í° í•œê³„ì—ì„œ ìƒì„±ì„ ì¤‘ë‹¨ì‹œí‚´

### ğŸŒ¡ï¸ **Temperature (ì˜¨ë„)**
- **ë²”ìœ„**: 0 (ê²°ì •ì ) ~ 1+ (ì°½ì˜ì )
- **0 ì„¤ì •**: ê·¸ë¦¬ë”” ë””ì½”ë”© - ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ í† í° ì„ íƒ
- **ë†’ì€ ê°’**: ë” ë‹¤ì–‘í•˜ê³  ì˜ˆì¸¡í•˜ê¸° ì–´ë ¤ìš´ ê²°ê³¼
- **Softmax í•¨ìˆ˜**ì™€ ìœ ì‚¬í•œ ë™ì‘ ì›ë¦¬

### ğŸ² **Top-K ìƒ˜í”Œë§**
- **ê¸°ëŠ¥**: ìƒìœ„ Kê°œì˜ ê°€ëŠ¥ì„± ë†’ì€ í† í°ì—ì„œë§Œ ì„ íƒ
- **ë†’ì€ Top-K**: ë” ì°½ì˜ì ì´ê³  ë‹¤ì–‘í•œ ì¶œë ¥
- **ë‚®ì€ Top-K**: ë” ì œí•œì ì´ê³  ì‚¬ì‹¤ì ì¸ ì¶œë ¥
- **K=1**: ê·¸ë¦¬ë”” ë””ì½”ë”©ê³¼ ë™ì¼

### ğŸ¯ **Top-P ìƒ˜í”Œë§ (Nucleus Sampling)**
- **ê¸°ëŠ¥**: ëˆ„ì  í™•ë¥ ì´ Pê°’ì„ ë„˜ì§€ ì•ŠëŠ” í† í°ë“¤ì—ì„œ ì„ íƒ
- **ë²”ìœ„**: 0 (ê·¸ë¦¬ë””) ~ 1 (ëª¨ë“  í† í°)
- **ë™ì  ì„ íƒ**: ìƒí™©ì— ë”°ë¼ ì„ íƒë˜ëŠ” í† í° ìˆ˜ê°€ ë³€í•¨

---

## í”„ë¡¬í”„íŠ¸ ì˜ˆì‹œ

### ê¸°ë³¸ ì„¤ì • ì¡°í•©
```plaintext
# ì˜í™” ë¦¬ë·° ë¶„ë¥˜ - ì°½ì˜ì„± ë¶ˆí•„ìš”
Temperature: 0.1
Top-K: 20
Top-P: 0.9
Token Limit: 5

# ì°½ì˜ì  ì‘ì—… - ìŠ¤í† ë¦¬ ìƒì„±
Temperature: 0.9
Top-K: 40
Top-P: 0.99
Token Limit: 1024

# ìˆ˜í•™ ë¬¸ì œ - ì •í™•í•œ ë‹µ í•„ìš”
Temperature: 0
Top-K: N/A (ë¬´ê´€)
Top-P: N/A (ë¬´ê´€)
```

### ì„¤ì •ë³„ ë¹„êµ ì˜ˆì‹œ
```plaintext
í”„ë¡¬í”„íŠ¸: "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ì•„ì„œ"

# ë‚®ì€ ì°½ì˜ì„± (Temperature: 0.1)
â†’ "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ì•„ì„œ ê³µì›ì— ì‚°ì±…ì„ ê°€ê¸°ë¡œ í–ˆìŠµë‹ˆë‹¤."

# ë†’ì€ ì°½ì˜ì„± (Temperature: 0.9)
â†’ "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ì•„ì„œ í•˜ëŠ˜ì„ ë‚˜ëŠ” ê¿ˆì„ ê¾¸ë©° ë¬´ì§€ê°œ ë‹¤ë¦¬ë¥¼ ê±´ë„œìŠµë‹ˆë‹¤."
```

---

## í™œìš© íŒ

### ğŸš€ **LangChainì—ì„œì˜ ì„¤ì • ì ìš©**

```python
from langchain.llms import VertexAI
from langchain.chat_models import ChatVertexAI

# ì‚¬ì‹¤ì  ì‘ì—…ìš© ì„¤ì •
factual_llm = VertexAI(
    model_name="gemini-pro",
    temperature=0.1,                            # temperature ë‚®ê²Œ ì„¤ì •
    max_output_tokens=256,
    top_k=20,
    top_p=0.9
)

# ì°½ì˜ì  ì‘ì—…ìš© ì„¤ì •
creative_llm = VertexAI(
    model_name="gemini-pro",
    temperature=0.8,                            # temperature ë†’ê²Œ ì„¤ì •
    max_output_tokens=1024,
    top_k=40,
    top_p=0.95
)

# ì •í™•í•œ ë‹µë³€ í•„ìš”í•œ ì‘ì—…
precise_llm = VertexAI(
    model_name="gemini-pro",
    temperature=0.0,                            # temperature = 0
    max_output_tokens=100
)
```

### ğŸ¯ **Gemini API ì§ì ‘ í™œìš©**

```python
import vertexai
from vertexai.generative_models import GenerativeModel, GenerationConfig

# ì„¤ì • ê°ì²´ ìƒì„±
config = GenerationConfig(
    temperature=0.2,
    top_p=0.95,
    top_k=30,
    max_output_tokens=1024,
)

# ëª¨ë¸ ì´ˆê¸°í™”
model = GenerativeModel(
    model_name="gemini-pro",
    generation_config=config
)

# ì‘ë‹µ ìƒì„±
response = model.generate_content("í”„ë¡¬í”„íŠ¸ ë‚´ìš©")
```

### ğŸ“Š **ì‘ì—…ë³„ ê¶Œì¥ ì„¤ì •**

| ì‘ì—… ìœ í˜• | Temperature | Top-K | Top-P | ì„¤ëª… |
|-----------|-------------|-------|-------|------|
| ë°ì´í„° ë¶„ë¥˜ | 0.0-0.1 | 20 | 0.9 | ì¼ê´€ëœ ì •í™•í•œ ë¶„ë¥˜ |
| ìš”ì•½ | 0.1-0.3 | 30 | 0.9 | ì‚¬ì‹¤ì ì´ì§€ë§Œ ì•½ê°„ì˜ ë‹¤ì–‘ì„± |
| ì°½ì‘ | 0.7-0.9 | 40 | 0.99 | ë†’ì€ ì°½ì˜ì„±ê³¼ ë‹¤ì–‘ì„± |
| ì½”ë“œ ìƒì„± | 0.1-0.3 | 30 | 0.95 | ì •í™•í•˜ì§€ë§Œ ë‹¤ì–‘í•œ ì ‘ê·¼ë²• |
| ìˆ˜í•™ ë¬¸ì œ | 0.0 | N/A | N/A | ì •í™•í•œ ë‹¨ì¼ ë‹µ |

### âš ï¸ **ë°˜ë³µ ë£¨í”„ ë²„ê·¸ ë°©ì§€**

```python
# ë¬¸ì œê°€ ë˜ëŠ” ì„¤ì • ì¡°í•© í”¼í•˜ê¸°
# 1. ë„ˆë¬´ ë‚®ì€ Temperature + ë†’ì€ Top-K
# 2. ë„ˆë¬´ ë†’ì€ Temperature + ì œí•œì ì¸ Top-P

# ê¶Œì¥ ì•ˆì „ ì„¤ì •
safe_config = GenerationConfig(
    temperature=0.3,                    # ë„ˆë¬´ ê·¹ë‹¨ì ì´ì§€ ì•Šì€ ê°’
    top_p=0.95,                         # ì ë‹¹í•œ ë‹¤ì–‘ì„± í—ˆìš©
    top_k=30,                           # ì¤‘ê°„ ìˆ˜ì¤€ì˜ ì„ íƒê¶Œ
    max_output_tokens=512               # ì ì ˆí•œ ê¸¸ì´ ì œí•œ
)
```

### ğŸ”„ **ë™ì  ì„¤ì • ì¡°ì •**

```python
def get_optimal_config(task_type: str) -> GenerationConfig:
    """ì‘ì—… ìœ í˜•ì— ë”°ë¥¸ ìµœì  ì„¤ì • ë°˜í™˜"""
    
    configs = {
        "classification": GenerationConfig(
            temperature=0.1, top_k=20, top_p=0.9, max_output_tokens=10
        ),
        "creative_writing": GenerationConfig(
            temperature=0.8, top_k=40, top_p=0.99, max_output_tokens=1024
        ),
        "code_generation": GenerationConfig(
            temperature=0.2, top_k=30, top_p=0.95, max_output_tokens=2048
        ),
        "analysis": GenerationConfig(
            temperature=0.3, top_k=25, top_p=0.9, max_output_tokens=512
        )
    }
    
    return configs.get(task_type, configs["analysis"])

# ì‚¬ìš© ì˜ˆì‹œ
config = get_optimal_config("creative_writing")
model = GenerativeModel("gemini-pro", generation_config=config)
```

### ğŸ“ **ì„¤ì • ì‹¤í—˜ ë° ë¬¸ì„œí™”**

```python
# ì„¤ì • ì‹¤í—˜ì„ ìœ„í•œ ìœ í‹¸ë¦¬í‹°
class ConfigExperiment:
    def __init__(self, prompt: str):
        self.prompt = prompt
        self.results = []
    
    def test_config(self, config: GenerationConfig, description: str):
        model = GenerativeModel("gemini-pro", generation_config=config)
        response = model.generate_content(self.prompt)
        
        self.results.append({
            "description": description,
            "config": config,
            "response": response.text,
            "token_count": response.usage_metadata.candidates_token_count
        })
    
    def compare_results(self):
        for result in self.results:
            print(f"ì„¤ì •: {result['description']}")
            print(f"ì‘ë‹µ: {result['response'][:100]}...")
            print(f"í† í° ìˆ˜: {result['token_count']}")
            print("-" * 50)

# ì‹¤í—˜ ì‹¤í–‰
experiment = ConfigExperiment("ì°½ì˜ì ì¸ ìŠ¤í† ë¦¬ë¥¼ ì¨ì£¼ì„¸ìš”.")
experiment.test_config(
    GenerationConfig(temperature=0.1, top_k=20, top_p=0.9),
    "ë³´ìˆ˜ì  ì„¤ì •"
)
experiment.test_config(
    GenerationConfig(temperature=0.8, top_k=40, top_p=0.99),
    "ì°½ì˜ì  ì„¤ì •"
)
experiment.compare_results()
```

---

* ì¶œì²˜
  * [1] [Prompt Engineering from Google](https://www.kaggle.com/whitepaper-prompt-engineering)
