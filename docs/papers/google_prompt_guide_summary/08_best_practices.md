# ğŸ“˜ 08. ëª¨ë²” ì‚¬ë¡€ ë° íŒ (Best Practices & Tips)

## í•µì‹¬ ìš”ì•½
- **ì˜ˆì‹œ ì œê³µ**ì€ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ëª¨ë²” ì‚¬ë¡€ë¡œ, Few-shot í”„ë¡¬í”„íŒ…ì„ í†µí•´ ëª¨ë¸ ì„±ëŠ¥ì„ ê·¹ì ìœ¼ë¡œ í–¥ìƒì‹œí‚´
- **ëª…í™•í•˜ê³  êµ¬ì²´ì ì¸ ì§€ì‹œì‚¬í•­**ì´ ì œì•½ì‚¬í•­ë³´ë‹¤ íš¨ê³¼ì ì´ë©°, ëª¨ë¸ì´ ì›í•˜ëŠ” ê²°ê³¼ë¥¼ ì •í™•íˆ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ë„ì›€
- **êµ¬ì¡°í™”ëœ ì¶œë ¥ í˜•ì‹(JSON, XML)**ì€ ì¼ê´€ì„±ì„ ë³´ì¥í•˜ê³  í• ë£¨ì‹œë„¤ì´ì…˜ì„ ì¤„ì´ë©° í›„ì²˜ë¦¬ ì‘ì—…ì„ ìš©ì´í•˜ê²Œ í•¨
- **ë°˜ë³µì  ì‹¤í—˜ê³¼ ë¬¸ì„œí™”**ê°€ í•„ìˆ˜ì´ë©°, í”„ë¡¬í”„íŠ¸ ë²„ì „ ê´€ë¦¬ì™€ ì„±ëŠ¥ ì¶”ì ì„ í†µí•´ ì§€ì†ì ì¸ ê°œì„ ì´ ê°€ëŠ¥
- **ëª¨ë¸ë³„ ìµœì í™”**ê°€ ì¤‘ìš”í•˜ë©°, ëª¨ë¸ ì—…ë°ì´íŠ¸ì™€ ìƒˆë¡œìš´ ê¸°ëŠ¥ì— ë§ì¶° í”„ë¡¬í”„íŠ¸ë¥¼ ì¡°ì •í•´ì•¼ í•¨

## ì£¼ìš” ê°œë…ê³¼ ì„¤ëª…

### ğŸ¯ **ì˜ˆì‹œ ì œê³µì˜ ì¤‘ìš”ì„±**
- **í•™ìŠµ íš¨ê³¼**: ëª¨ë¸ì´ ì›í•˜ëŠ” ì¶œë ¥ íŒ¨í„´ì„ ì´í•´í•˜ê³  ëª¨ë°©í•  ìˆ˜ ìˆê²Œ í•¨
- **ì •í™•ì„± í–¥ìƒ**: êµ¬ì²´ì ì¸ ì˜ˆì‹œë¥¼ í†µí•´ ëª¨ë¸ì˜ ì¶”ì¸¡ì„ ì¤„ì´ê³  ì •í™•í•œ ê²°ê³¼ ìƒì„±
- **ì¼ê´€ì„± í™•ë³´**: ë™ì¼í•œ ìœ í˜•ì˜ ì‘ì—…ì—ì„œ ì¼ê´€ëœ í’ˆì§ˆê³¼ í˜•ì‹ ìœ ì§€

### ğŸ“ **ëª…í™•í•œ ì§€ì‹œì‚¬í•­ ì„¤ê³„**
- **ë‹¨ìˆœì„± ì›ì¹™**: ë³µì¡í•œ ì–¸ì–´ë‚˜ ë¶ˆí•„ìš”í•œ ì •ë³´ ì œê±°
- **í–‰ë™ ì§€í–¥ì  ë™ì‚¬**: Analyze, Create, Extract, Generate ë“± ëª…í™•í•œ ë™ì‘ ì§€ì‹œ
- **êµ¬ì²´ì  ì¶œë ¥ ìš”êµ¬**: ì›í•˜ëŠ” ê²°ê³¼ì˜ í˜•ì‹, ê¸¸ì´, ìŠ¤íƒ€ì¼ ëª…ì‹œ

### ğŸ”§ **êµ¬ì¡°í™”ëœ ì¶œë ¥ í™œìš©**
- **JSON/XML í˜•ì‹**: íŒŒì‹± ìš©ì´ì„±, ë°ì´í„° íƒ€ì… ë³´ì¥, ê´€ê³„ ì¸ì‹ í–¥ìƒ
- **ìŠ¤í‚¤ë§ˆ ì •ì˜**: ì…ë ¥ê³¼ ì¶œë ¥ ëª¨ë‘ì— ëŒ€í•œ ëª…í™•í•œ êµ¬ì¡° ì œê³µ
- **ì˜¤ë¥˜ ë°©ì§€**: êµ¬ì¡°í™”ë¥¼ í†µí•œ í• ë£¨ì‹œë„¤ì´ì…˜ ê°ì†Œ

## í”„ë¡¬í”„íŠ¸ ì˜ˆì‹œ

### ëª¨ë²” ì‚¬ë¡€ ì ìš© - BEFORE/AFTER

#### âŒ ê°œì„  ì „ (ë¹„íš¨ìœ¨ì )
```plaintext
ë‰´ìš•ì— ì§€ê¸ˆ ê°€ê³  ìˆëŠ”ë°, ì¢‹ì€ ì¥ì†Œë“¤ì— ëŒ€í•´ ë” ë“£ê³  ì‹¶ìŠµë‹ˆë‹¤. 
3ì‚´ì§œë¦¬ ì•„ì´ ë‘˜ê³¼ í•¨ê»˜ ìˆì–´ìš”. íœ´ê°€ ë™ì•ˆ ì–´ë””ë¡œ ê°€ì•¼ í• ê¹Œìš”?
```

#### âœ… ê°œì„  í›„ (íš¨ìœ¨ì )
```plaintext
ì—­í• : ê´€ê´‘ê°ì„ ìœ„í•œ ì—¬í–‰ ê°€ì´ë“œë¡œ í™œë™í•˜ì„¸ìš”.

ì‘ì—…: 3ì„¸ ì•„ì´ì™€ í•¨ê»˜ ë‰´ìš• ë§¨í•´íŠ¼ì—ì„œ ë°©ë¬¸í•˜ê¸° ì¢‹ì€ ì¥ì†Œë“¤ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.

ì¶œë ¥ í˜•ì‹:
- ì¥ì†Œëª…: [ì´ë¦„]
- ìœ„ì¹˜: [ì£¼ì†Œ/ì§€ì—­]
- ì•„ì´ ì¹œí™”ë„: [1-5ì ]
- ì¶”ì²œ ì´ìœ : [ê°„ë‹¨í•œ ì„¤ëª…]
- ì†Œìš” ì‹œê°„: [ì˜ˆìƒ ì‹œê°„]

ì¥ì†Œ 3ê³³ì„ ì¶”ì²œí•´ì£¼ì„¸ìš”.
```

### êµ¬ì¡°í™”ëœ ì¶œë ¥ - ì œí’ˆ ë¶„ì„
```plaintext
ë‹¤ìŒ ì œí’ˆ ì •ë³´ë¥¼ ë¶„ì„í•˜ê³  JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•˜ì„¸ìš”:

ì…ë ¥ ìŠ¤í‚¤ë§ˆ:
```
{
  "name": "ì œí’ˆëª…",
  "category": "ì¹´í…Œê³ ë¦¬",
  "price": ê°€ê²©(ìˆ«ì),
  "features": ["ê¸°ëŠ¥1", "ê¸°ëŠ¥2"],
  "release_date": "YYYY-MM-DD"
}
```

ë¶„ì„í•  ì œí’ˆ:
"iPhone 15 ProëŠ” 2023ë…„ 9ì›” ì¶œì‹œëœ ì• í”Œì˜ ìµœì‹  ìŠ¤ë§ˆíŠ¸í°ì…ë‹ˆë‹¤. 
ê°€ê²©ì€ $999ì´ë©°, A17 Pro ì¹©, 48MP ì¹´ë©”ë¼, í‹°íƒ€ëŠ„ ë””ìì¸ì´ íŠ¹ì§•ì…ë‹ˆë‹¤."

ì¶œë ¥ JSON:
```

**ì¶œë ¥**:
```json
{
  "name": "iPhone 15 Pro",
  "category": "ìŠ¤ë§ˆíŠ¸í°",
  "price": 999,
  "features": ["A17 Pro ì¹©", "48MP ì¹´ë©”ë¼", "í‹°íƒ€ëŠ„ ë””ìì¸"],
  "release_date": "2023-09-01",
  "analysis": {
    "target_market": "í”„ë¦¬ë¯¸ì—„ ì‚¬ìš©ì",
    "competitive_advantage": "ê³ ì„±ëŠ¥ ì¹©ì…‹ê³¼ í”„ë¡œê¸‰ ì¹´ë©”ë¼",
    "price_segment": "í”Œë˜ê·¸ì‹­"
  }
}
```

## í™œìš© íŒ

### ğŸš€ **LangChain ëª¨ë²” ì‚¬ë¡€ êµ¬í˜„**

#### í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ê´€ë¦¬ ì‹œìŠ¤í…œ
```python
from langchain.prompts import PromptTemplate
from langchain.prompts.few_shot import FewShotPromptTemplate
from dataclasses import dataclass
from typing import List, Dict, Any
import json

@dataclass
class PromptConfig:
    """í”„ë¡¬í”„íŠ¸ ì„¤ì • ê´€ë¦¬"""
    name: str
    goal: str
    model: str
    temperature: float
    max_tokens: int
    top_k: int
    top_p: float
    version: str

class BestPracticePromptManager:
    def __init__(self):
        self.templates = {}
        self.examples = {}
        self.configs = {}
    
    def create_few_shot_template(self, name: str, examples: List[Dict], 
                               example_template: str, prefix: str, 
                               suffix: str, input_variables: List[str]):
        """Few-shot í…œí”Œë¦¿ ìƒì„± (ëª¨ë²” ì‚¬ë¡€: ì˜ˆì‹œ ì œê³µ)"""
        
        example_prompt = PromptTemplate(
            input_variables=list(examples[0].keys()),
            template=example_template
        )
        
        few_shot_prompt = FewShotPromptTemplate(
            examples=examples,
            example_prompt=example_prompt,
            prefix=prefix,
            suffix=suffix,
            input_variables=input_variables,
            example_separator="\n---\n"  # ì˜ˆì‹œ êµ¬ë¶„ì„ ëª…í™•íˆ
        )
        
        self.templates[name] = few_shot_prompt
        self.examples[name] = examples
        
        return few_shot_prompt
    
    def create_structured_output_template(self, name: str, task: str, 
                                        output_schema: Dict, input_vars: List[str]):
        """êµ¬ì¡°í™”ëœ ì¶œë ¥ í…œí”Œë¦¿ ìƒì„±"""
        
        schema_str = json.dumps(output_schema, indent=2, ensure_ascii=False)
        
        template = f"""
ì‘ì—…: {task}

ì¶œë ¥ ìŠ¤í‚¤ë§ˆ:
```
{schema_str}
```

ì…ë ¥: {{input_data}}

ìœ„ ìŠ¤í‚¤ë§ˆì— ë§ëŠ” ì •í™•í•œ JSONì„ ìƒì„±í•˜ì„¸ìš”. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.

JSON ì¶œë ¥:
"""
        
        prompt = PromptTemplate(
            input_variables=input_vars,
            template=template
        )
        
        self.templates[name] = prompt
        return prompt
    
    def create_instruction_based_template(self, name: str, role: str, 
                                        instructions: List[str], 
                                        output_format: str):
        """ì§€ì‹œì‚¬í•­ ê¸°ë°˜ í…œí”Œë¦¿ (ì œì•½ë³´ë‹¤ ì§€ì‹œì‚¬í•­ ìš°ì„ )"""
        
        instructions_str = "\n".join([f"- {inst}" for inst in instructions])
        
        template = f"""
ì—­í• : {role}

ì§€ì‹œì‚¬í•­:
{instructions_str}

ì¶œë ¥ í˜•ì‹: {output_format}

ì…ë ¥: {{input}}

ì‘ë‹µ:
"""
        
        prompt = PromptTemplate(
            input_variables=["input"],
            template=template
        )
        
        self.templates[name] = prompt
        return prompt

# ì‚¬ìš© ì˜ˆì‹œ
manager = BestPracticePromptManager()

# 1. Few-shot ë¶„ë¥˜ í…œí”Œë¦¿ ìƒì„±
classification_examples = [
    {"input": "ì´ ì œí’ˆ ì •ë§ ì¢‹ì•„ìš”! ì¶”ì²œí•©ë‹ˆë‹¤.", "output": "POSITIVE"},
    {"input": "ê·¸ëƒ¥ ê·¸ëŸ° ê²ƒ ê°™ì•„ìš”.", "output": "NEUTRAL"},
    {"input": "ì™„ì „ ì‹¤ë§ì´ì—ìš”. ëˆ ì•„ê¹Œì›Œìš”.", "output": "NEGATIVE"},
    {"input": "ê¸°ëŒ€í–ˆë˜ ê²ƒë³´ë‹¤ í›¨ì”¬ ì¢‹ë„¤ìš”!", "output": "POSITIVE"},
    {"input": "ë³´í†µ ìˆ˜ì¤€ì´ë„¤ìš”. íŠ¹ë³„í•˜ì§€ ì•Šì•„ìš”.", "output": "NEUTRAL"}
]

sentiment_template = manager.create_few_shot_template(
    name="sentiment_classification",
    examples=classification_examples,
    example_template="ì…ë ¥: {input}\në¶„ë¥˜: {output}",
    prefix="ê³ ê° ë¦¬ë·°ë¥¼ POSITIVE, NEUTRAL, NEGATIVEë¡œ ë¶„ë¥˜í•˜ì„¸ìš”.\n\nì˜ˆì‹œ:",
    suffix="\nì…ë ¥: {review}\në¶„ë¥˜:",
    input_variables=["review"]
)

# 2. êµ¬ì¡°í™”ëœ ì¶œë ¥ í…œí”Œë¦¿
product_schema = {
    "name": "ì œí’ˆëª…",
    "category": "ì¹´í…Œê³ ë¦¬",
    "sentiment": "POSITIVE|NEUTRAL|NEGATIVE",
    "key_features": ["ê¸°ëŠ¥1", "ê¸°ëŠ¥2"],
    "price_mentioned": "boolean",
    "recommendation_score": "1-5 ìˆ«ì"
}

product_analysis_template = manager.create_structured_output_template(
    name="product_analysis",
    task="ì œí’ˆ ë¦¬ë·°ë¥¼ ë¶„ì„í•˜ì—¬ êµ¬ì¡°í™”ëœ ì •ë³´ ì¶”ì¶œ",
    output_schema=product_schema,
    input_vars=["input_data"]
)
```

#### ë™ì  í”„ë¡¬í”„íŠ¸ ìµœì í™” ì‹œìŠ¤í…œ
```python
from langchain.llms import VertexAI
from langchain.chains import LLMChain
import time
from typing import Optional

class DynamicPromptOptimizer:
    def __init__(self, model_name="gemini-pro"):
        self.llm = VertexAI(model_name=model_name)
        self.performance_history = []
        
    def optimize_for_task(self, task_type: str, complexity: str, 
                         creativity_level: str) -> Dict[str, Any]:
        """ì‘ì—… ìœ í˜•ì— ë”°ë¥¸ ìµœì  ì„¤ì • ì¶”ì²œ"""
        
        config_map = {
            ("classification", "low", "low"): {
                "temperature": 0.1,
                "max_tokens": 50,
                "top_k": 20,
                "top_p": 0.9,
                "style": "ì •í™•í•˜ê³  ê°„ê²°í•˜ê²Œ"
            },
            ("creative", "high", "high"): {
                "temperature": 0.8,
                "max_tokens": 1024,
                "top_k": 40,
                "top_p": 0.95,
                "style": "ì°½ì˜ì ì´ê³  ë‹¤ì–‘í•˜ê²Œ"
            },
            ("analysis", "medium", "low"): {
                "temperature": 0.3,
                "max_tokens": 512,
                "top_k": 30,
                "top_p": 0.9,
                "style": "ë…¼ë¦¬ì ì´ê³  ì²´ê³„ì ìœ¼ë¡œ"
            }
        }
        
        key = (task_type, complexity, creativity_level)
        return config_map.get(key, config_map[("analysis", "medium", "low")])
    
    def create_adaptive_prompt(self, base_task: str, context: Optional[str] = None, 
                             examples: Optional[List[Dict]] = None,
                             output_format: str = "ììœ  í˜•ì‹") -> str:
        """ìƒí™©ì— ë§ëŠ” ì ì‘í˜• í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        
        prompt_parts = []
        
        # 1. ì—­í•  ë° ë§¥ë½ ì„¤ì •
        if context:
            prompt_parts.append(f"ë§¥ë½: {context}")
        
        # 2. ì‘ì—… ì •ì˜ (ëª…í™•í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ)
        prompt_parts.append(f"ì‘ì—…: {base_task}")
        
        # 3. ì˜ˆì‹œ ì¶”ê°€ (ê°€ëŠ¥í•œ ê²½ìš°)
        if examples:
            prompt_parts.append("ì˜ˆì‹œ:")
            for i, example in enumerate(examples, 1):
                input_text = example.get('input', example.get('question', ''))
                output_text = example.get('output', example.get('answer', ''))
                prompt_parts.append(f"{i}. ì…ë ¥: {input_text}")
                prompt_parts.append(f"   ì¶œë ¥: {output_text}")
        
        # 4. ì¶œë ¥ í˜•ì‹ ëª…ì‹œ
        if output_format != "ììœ  í˜•ì‹":
            prompt_parts.append(f"ì¶œë ¥ í˜•ì‹: {output_format}")
        
        # 5. ì…ë ¥ placeholder
        prompt_parts.append("ì…ë ¥: {input}")
        prompt_parts.append("ì¶œë ¥:")
        
        return "\n\n".join(prompt_parts)
    
    def test_prompt_variants(self, base_prompt: str, test_inputs: List[str], 
                           variations: List[Dict[str, Any]]) -> Dict[str, Any]:
        """í”„ë¡¬í”„íŠ¸ ë³€í˜•ë“¤ì˜ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        
        results = {}
        
        for i, variation in enumerate(variations):
            variant_name = f"variant_{i+1}"
            variant_results = []
            
            # LLM ì„¤ì • ì ìš©
            llm = VertexAI(
                model_name="gemini-pro",
                temperature=variation.get('temperature', 0.3),
                max_output_tokens=variation.get('max_tokens', 1024),
                top_k=variation.get('top_k', 30),
                top_p=variation.get('top_p', 0.9)
            )
            
            # í”„ë¡¬í”„íŠ¸ ìˆ˜ì •
            modified_prompt = base_prompt
            if 'style_instruction' in variation:
                modified_prompt += f"\n\nìŠ¤íƒ€ì¼: {variation['style_instruction']}"
            
            prompt_template = PromptTemplate(
                input_variables=["input"],
                template=modified_prompt
            )
            
            chain = LLMChain(llm=llm, prompt=prompt_template)
            
            # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
            for test_input in test_inputs:
                start_time = time.time()
                try:
                    response = chain.run(input=test_input)
                    execution_time = time.time() - start_time
                    
                    variant_results.append({
                        "input": test_input,
                        "output": response,
                        "execution_time": execution_time,
                        "success": True
                    })
                except Exception as e:
                    variant_results.append({
                        "input": test_input,
                        "error": str(e),
                        "execution_time": time.time() - start_time,
                        "success": False
                    })
            
            # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
            success_rate = sum(1 for r in variant_results if r['success']) / len(variant_results)
            avg_time = sum(r['execution_time'] for r in variant_results) / len(variant_results)
            
            results[variant_name] = {
                "variation": variation,
                "success_rate": success_rate,
                "avg_execution_time": avg_time,
                "detailed_results": variant_results
            }
        
        return results

# ì‚¬ìš© ì˜ˆì‹œ
optimizer = DynamicPromptOptimizer()

# ìµœì  ì„¤ì • ì¶”ì²œ
optimal_config = optimizer.optimize_for_task("classification", "low", "low")
print(f"ê¶Œì¥ ì„¤ì •: {optimal_config}")

# ì ì‘í˜• í”„ë¡¬í”„íŠ¸ ìƒì„±
adaptive_prompt = optimizer.create_adaptive_prompt(
    base_task="ê³ ê° ë¬¸ì˜ë¥¼ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¥˜",
    context="ì „ìì œí’ˆ ì‡¼í•‘ëª° ê³ ê° ì„œë¹„ìŠ¤",
    examples=[
        {"input": "ë°°ì†¡ì´ ì–¸ì œ ì˜¤ë‚˜ìš”?", "output": "ë°°ì†¡ë¬¸ì˜"},
        {"input": "í™˜ë¶ˆí•˜ê³  ì‹¶ì–´ìš”", "output": "í™˜ë¶ˆë¬¸ì˜"},
        {"input": "ì œí’ˆì´ ê³ ì¥ë‚¬ì–´ìš”", "output": "ê¸°ìˆ ì§€ì›"}
    ],
    output_format="ì¹´í…Œê³ ë¦¬ëª…ë§Œ ë°˜í™˜"
)

# í”„ë¡¬í”„íŠ¸ ë³€í˜• í…ŒìŠ¤íŠ¸
variations = [
    {"temperature": 0.1, "style_instruction": "ì •í™•í•˜ê³  ê°„ê²°í•˜ê²Œ"},
    {"temperature": 0.3, "style_instruction": "ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ë„ë¡"},
    {"temperature": 0.0, "max_tokens": 10}  # ì´ˆì •í™• ëª¨ë“œ
]

test_inputs = [
    "ì–¸ì œ ë°°ì†¡ë˜ë‚˜ìš”?",
    "ìƒ‰ìƒ ë³€ê²½ ê°€ëŠ¥í•œê°€ìš”?",
    "AS ì„¼í„° ìœ„ì¹˜ê°€ ì–´ë””ì¸ê°€ìš”?"
]

test_results = optimizer.test_prompt_variants(
    adaptive_prompt, test_inputs, variations
)

# ìµœê³  ì„±ëŠ¥ ë³€í˜• ì„ íƒ
best_variant = max(test_results.items(), key=lambda x: x[1]['success_rate'])
print(f"ìµœê³  ì„±ëŠ¥ ë³€í˜•: {best_variant[0]} (ì„±ê³µë¥ : {best_variant[1]['success_rate']:.2%})")
```

### ğŸ¯ **Gemini API ìµœì í™” í™œìš©**

#### JSON ë³µêµ¬ ë° ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì‹œìŠ¤í…œ
```python
import json
import jsonschema
from typing import Dict, Any, Optional
import re

class JSONOutputManager:
    def __init__(self):
        self.repair_patterns = [
            (r'}\s*$', '}'),  # ë§ˆì§€ë§‰ ì¤‘ê´„í˜¸ ì •ë¦¬
            (r',\s*}', '}'),  # ë§ˆì§€ë§‰ ì½¤ë§ˆ ì œê±°
            (r',\s*]', ']'),  # ë°°ì—´ ë§ˆì§€ë§‰ ì½¤ë§ˆ ì œê±°
        ]
    
    def repair_json(self, json_string: str) -> Optional[str]:
        """ë¶ˆì™„ì „í•œ JSON ë³µêµ¬"""
        try:
            # 1. ê¸°ë³¸ íŒŒì‹± ì‹œë„
            json.loads(json_string)
            return json_string
        except json.JSONDecodeError:
            pass
        
        # 2. ì¼ë°˜ì ì¸ íŒ¨í„´ ìˆ˜ì •
        cleaned = json_string.strip()
        
        # 3. ì¼ë°˜ì ì¸ ìˆ˜ì • íŒ¨í„´ ì ìš©
        for pattern, replacement in self.repair_patterns:
            cleaned = re.sub(pattern, replacement, cleaned)
        
        # 4. ì¤‘ê´„í˜¸/ëŒ€ê´„í˜¸ ë§¤ì¹­ í™•ì¸ ë° ë³´ì™„
        open_braces = cleaned.count('{')
        close_braces = cleaned.count('}')
        
        if open_braces > close_braces:
            cleaned += '}' * (open_braces - close_braces)
        
        open_brackets = cleaned.count('[')
        close_brackets = cleaned.count(']')
        
        if open_brackets > close_brackets:
            cleaned += ']' * (open_brackets - close_brackets)
        
        # 5. ìµœì¢… ê²€ì¦
        try:
            json.loads(cleaned)
            return cleaned
        except json.JSONDecodeError:
            return None
    
    def validate_against_schema(self, json_ Dict[str, Any], 
                              schema: Dict[str, Any]) -> Dict[str, Any]:
        """ìŠ¤í‚¤ë§ˆ ê¸°ë°˜ JSON ê²€ì¦"""
        try:
            jsonschema.validate(instance=json_data, schema=schema)
            return {"valid": True, "errors": []}
        except jsonschema.ValidationError as e:
            return {
                "valid": False,
                "errors": [str(e)],
                "path": list(e.absolute_path) if e.absolute_path else []
            }
    
    def create_schema_prompt(self, schema: Dict[str, Any], task: str) -> str:
        """ìŠ¤í‚¤ë§ˆ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        schema_str = json.dumps(schema, indent=2, ensure_ascii=False)
        
        return f"""
ì‘ì—…: {task}

ì¶œë ¥ ìŠ¤í‚¤ë§ˆ (ë°˜ë“œì‹œ ì¤€ìˆ˜):
```
{schema_str}
```

ì¤‘ìš” ì§€ì¹¨:
1. ìŠ¤í‚¤ë§ˆì˜ ëª¨ë“  í•„ìˆ˜ í•„ë“œë¥¼ í¬í•¨í•˜ì„¸ìš”
2. ë°ì´í„° íƒ€ì…ì„ ì •í™•íˆ ë§ì¶°ì£¼ì„¸ìš”
3. ì¶”ê°€ í…ìŠ¤íŠ¸ ì—†ì´ JSONë§Œ ë°˜í™˜í•˜ì„¸ìš”
4. ìœ íš¨í•œ JSON í˜•ì‹ì„ ë³´ì¥í•˜ì„¸ìš”

ì…ë ¥: {{input}}

JSON ì¶œë ¥:
"""

# ì‚¬ìš© ì˜ˆì‹œ
json_manager = JSONOutputManager()

# ì œí’ˆ ë¶„ì„ ìŠ¤í‚¤ë§ˆ ì •ì˜
product_schema = {
    "type": "object",
    "properties": {
        "name": {"type": "string"},
        "category": {"type": "string"},
        "sentiment": {"type": "string", "enum": ["POSITIVE", "NEUTRAL", "NEGATIVE"]},
        "price_range": {"type": "string", "enum": ["LOW", "MEDIUM", "HIGH"]},
        "features": {
            "type": "array",
            "items": {"type": "string"}
        },
        "rating": {"type": "number", "minimum": 1, "maximum": 5}
    },
    "required": ["name", "category", "sentiment", "rating"]
}

# ìŠ¤í‚¤ë§ˆ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ ìƒì„±
schema_prompt = json_manager.create_schema_prompt(
    product_schema, 
    "ì œí’ˆ ë¦¬ë·°ë¥¼ ë¶„ì„í•˜ì—¬ êµ¬ì¡°í™”ëœ ì •ë³´ë¥¼ ì¶”ì¶œ"
)

# JSON ë³µêµ¬ í…ŒìŠ¤íŠ¸
broken_json = '{"name": "iPhone", "category": "ìŠ¤ë§ˆíŠ¸í°", "sentiment": "POSITIVE"'
repaired = json_manager.repair_json(broken_json)
print(f"ë³µêµ¬ëœ JSON: {repaired}")
```

#### í”„ë¡¬í”„íŠ¸ ë¬¸ì„œí™” ë° ë²„ì „ ê´€ë¦¬ ì‹œìŠ¤í…œ
```python
import datetime
import pandas as pd
from dataclasses import dataclass
from typing import List, Dict, Any, Optional

@dataclass
class PromptAttempt:
    """í”„ë¡¬í”„íŠ¸ ì‹œë„ ê¸°ë¡"""
    name: str
    version: str
    goal: str
    model: str
    temperature: float
    token_limit: int
    top_k: Optional[int]
    top_p: float
    prompt_text: str
    test_inputs: List[str]
    outputs: List[str]
    success_rate: float
    avg_execution_time: float
    feedback: str
    timestamp: datetime.datetime
    hyperlink: Optional[str] = None

class PromptDocumentationSystem:
    def __init__(self):
        self.attempts: List[PromptAttempt] = []
        self.performance_trends = {}
    
    def log_attempt(self, attempt: PromptAttempt):
        """í”„ë¡¬í”„íŠ¸ ì‹œë„ ë¡œê¹…"""
        self.attempts.append(attempt)
        
        # ì„±ëŠ¥ íŠ¸ë Œë“œ ì—…ë°ì´íŠ¸
        prompt_name = attempt.name
        if prompt_name not in self.performance_trends:
            self.performance_trends[prompt_name] = []
        
        self.performance_trends[prompt_name].append({
            'version': attempt.version,
            'success_rate': attempt.success_rate,
            'timestamp': attempt.timestamp
        })
    
    def get_best_performing_version(self, prompt_name: str) -> Optional[PromptAttempt]:
        """ìµœê³  ì„±ëŠ¥ ë²„ì „ ì¡°íšŒ"""
        relevant_attempts = [a for a in self.attempts if a.name == prompt_name]
        if not relevant_attempts:
            return None
        
        return max(relevant_attempts, key=lambda x: x.success_rate)
    
    def export_to_csv(self, filename: str):
        """CSVë¡œ ë‚´ë³´ë‚´ê¸°"""
        data = []
        for attempt in self.attempts:
            data.append({
                'Name': attempt.name,
                'Version': attempt.version,
                'Goal': attempt.goal,
                'Model': attempt.model,
                'Temperature': attempt.temperature,
                'Token_Limit': attempt.token_limit,
                'Top_K': attempt.top_k,
                'Top_P': attempt.top_p,
                'Success_Rate': attempt.success_rate,
                'Avg_Execution_Time': attempt.avg_execution_time,
                'Feedback': attempt.feedback,
                'Timestamp': attempt.timestamp,
                'Hyperlink': attempt.hyperlink
            })
        
        df = pd.DataFrame(data)
        df.to_csv(filename, index=False, encoding='utf-8-sig')
    
    def generate_performance_report(self, prompt_name: str) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„±"""
        relevant_attempts = [a for a in self.attempts if a.name == prompt_name]
        
        if not relevant_attempts:
            return {"error": "í•´ë‹¹ í”„ë¡¬í”„íŠ¸ì— ëŒ€í•œ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤."}
        
        # ì„±ëŠ¥ í†µê³„
        success_rates = [a.success_rate for a in relevant_attempts]
        execution_times = [a.avg_execution_time for a in relevant_attempts]
        
        return {
            "total_attempts": len(relevant_attempts),
            "avg_success_rate": sum(success_rates) / len(success_rates),
            "best_success_rate": max(success_rates),
            "worst_success_rate": min(success_rates),
            "avg_execution_time": sum(execution_times) / len(execution_times),
            "improvement_trend": self._calculate_trend(relevant_attempts),
            "latest_version": max(relevant_attempts, key=lambda x: x.timestamp).version,
            "best_version": max(relevant_attempts, key=lambda x: x.success_rate).version
        }
    
    def _calculate_trend(self, attempts: List[PromptAttempt]) -> str:
        """ì„±ëŠ¥ íŠ¸ë Œë“œ ê³„ì‚°"""
        if len(attempts) < 2:
            return "ë°ì´í„° ë¶€ì¡±"
        
        sorted_attempts = sorted(attempts, key=lambda x: x.timestamp)
        first_half = sorted_attempts[:len(sorted_attempts)//2]
        second_half = sorted_attempts[len(sorted_attempts)//2:]
        
        first_avg = sum(a.success_rate for a in first_half) / len(first_half)
        second_avg = sum(a.success_rate for a in second_half) / len(second_half)
        
        if second_avg > first_avg + 0.05:
            return "ìƒìŠ¹"
        elif second_avg < first_avg - 0.05:
            return "í•˜ë½"
        else:
            return "ì•ˆì •"

# í”„ë¡¬í”„íŠ¸ A/B í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ
class PromptABTester:
    def __init__(self, doc_system: PromptDocumentationSystem):
        self.doc_system = doc_system
    
    def run_ab_test(self, prompt_a: str, prompt_b: str, test_cases: List[Dict],
                   model_config: Dict, test_name: str) -> Dict[str, Any]:
        """A/B í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        
        results = {"prompt_a": [], "prompt_b": []}
        
        # í”„ë¡¬í”„íŠ¸ A í…ŒìŠ¤íŠ¸
        for case in test_cases:
            # ì‹¤ì œ LLM í˜¸ì¶œ ì‹œë®¬ë ˆì´ì…˜
            success_a = self._simulate_llm_call(prompt_a, case, model_config)
            results["prompt_a"].append(success_a)
        
        # í”„ë¡¬í”„íŠ¸ B í…ŒìŠ¤íŠ¸
        for case in test_cases:
            success_b = self._simulate_llm_call(prompt_b, case, model_config)
            results["prompt_b"].append(success_b)
        
        # í†µê³„ ê³„ì‚°
        success_rate_a = sum(results["prompt_a"]) / len(results["prompt_a"])
        success_rate_b = sum(results["prompt_b"]) / len(results["prompt_b"])
        
        # ê²°ê³¼ ê¸°ë¡
        self._log_ab_results(test_name, prompt_a, prompt_b, 
                           success_rate_a, success_rate_b, model_config)
        
        return {
            "test_name": test_name,
            "prompt_a_success_rate": success_rate_a,
            "prompt_b_success_rate": success_rate_b,
            "winner": "A" if success_rate_a > success_rate_b else "B",
            "improvement": abs(success_rate_a - success_rate_b),
            "statistical_significance": self._calculate_significance(
                results["prompt_a"], results["prompt_b"]
            )
        }
    
    def _simulate_llm_call(self, prompt: str, test_case: Dict, config: Dict) -> bool:
        """LLM í˜¸ì¶œ ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì§„ì§œ LLM í˜¸ì¶œ)"""
        # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ ì‹œë®¬ë ˆì´ì…˜
        import random
        return random.random() > 0.3  # 70% ì„±ê³µë¥  ê°€ì •
    
    def _log_ab_results(self, test_name: str, prompt_a: str, prompt_b: str,
                       success_a: float, success_b: float, config: Dict):
        """A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¡œê¹…"""
        
        timestamp = datetime.datetime.now()
        
        # í”„ë¡¬í”„íŠ¸ A ê¸°ë¡
        attempt_a = PromptAttempt(
            name=f"{test_name}_A",
            version="1.0",
            goal="A/B í…ŒìŠ¤íŠ¸",
            model=config.get("model", "gemini-pro"),
            temperature=config.get("temperature", 0.3),
            token_limit=config.get("token_limit", 1024),
            top_k=config.get("top_k"),
            top_p=config.get("top_p", 0.9),
            prompt_text=prompt_a,
            test_inputs=[],
            outputs=[],
            success_rate=success_a,
            avg_execution_time=0.0,
            feedback=f"A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼: {'ìŠ¹ë¦¬' if success_a > success_b else 'íŒ¨ë°°'}",
            timestamp=timestamp
        )
        
        # í”„ë¡¬í”„íŠ¸ B ê¸°ë¡
        attempt_b = PromptAttempt(
            name=f"{test_name}_B",
            version="1.0",
            goal="A/B í…ŒìŠ¤íŠ¸",
            model=config.get("model", "gemini-pro"),
            temperature=config.get("temperature", 0.3),
            token_limit=config.get("token_limit", 1024),
            top_k=config.get("top_k"),
            top_p=config.get("top_p", 0.9),
            prompt_text=prompt_b,
            test_inputs=[],
            outputs=[],
            success_rate=success_b,
            avg_execution_time=0.0,
            feedback=f"A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼: {'ìŠ¹ë¦¬' if success_b > success_a else 'íŒ¨ë°°'}",
            timestamp=timestamp
        )
        
        self.doc_system.log_attempt(attempt_a)
        self.doc_system.log_attempt(attempt_b)
    
    def _calculate_significance(self, results_a: List[bool], 
                              results_b: List[bool]) -> str:
        """í†µê³„ì  ìœ ì˜ì„± ê³„ì‚° (ê°„ë‹¨í•œ ë²„ì „)"""
        if len(results_a) < 30 or len(results_b) < 30:
            return "í‘œë³¸ í¬ê¸° ë¶€ì¡±"
        
        success_a = sum(results_a) / len(results_a)
        success_b = sum(results_b) / len(results_b)
        
        difference = abs(success_a - success_b)
        
        if difference > 0.1:
            return "í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸"
        elif difference > 0.05:
            return "ë³´í†µ ìˆ˜ì¤€ì˜ ì°¨ì´"
        else:
            return "ìœ ì˜ë¯¸í•œ ì°¨ì´ ì—†ìŒ"

# ì‚¬ìš© ì˜ˆì‹œ
doc_system = PromptDocumentationSystem()
ab_tester = PromptABTester(doc_system)

# í”„ë¡¬í”„íŠ¸ ì‹œë„ ê¸°ë¡
attempt = PromptAttempt(
    name="sentiment_classifier",
    version="1.0",
    goal="ê³ ê° ë¦¬ë·° ê°ì • ë¶„ì„",
    model="gemini-pro",
    temperature=0.1,
    token_limit=50,
    top_k=20,
    top_p=0.9,
    prompt_text="ë‹¤ìŒ ë¦¬ë·°ì˜ ê°ì •ì„ ë¶„ë¥˜í•˜ì„¸ìš”: {review}",
    test_inputs=["ì¢‹ì•„ìš”", "ë‚˜ì˜ë„¤ìš”", "ê·¸ëƒ¥ ê·¸ë˜ìš”"],
    outputs=["POSITIVE", "NEGATIVE", "NEUTRAL"],
    success_rate=0.85,
    avg_execution_time=1.2,
    feedback="ì „ë°˜ì ìœ¼ë¡œ ì¢‹ì€ ì„±ëŠ¥",
    timestamp=datetime.datetime.now()
)

doc_system.log_attempt(attempt)

# A/B í…ŒìŠ¤íŠ¸ ì‹¤í–‰
test_cases = [
    {"input": "ì´ ì œí’ˆ ì •ë§ ì¢‹ì•„ìš”!", "expected": "POSITIVE"},
    {"input": "ë³„ë¡œì—ìš”", "expected": "NEGATIVE"}
]

ab_result = ab_tester.run_ab_test(
    prompt_a="ê°ì •ì„ ë¶„ë¥˜í•˜ì„¸ìš”: {input}",
    prompt_b="ë‹¤ìŒ í…ìŠ¤íŠ¸ì˜ ê°ì •ì„ POSITIVE, NEGATIVE, NEUTRAL ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”: {input}",
    test_cases=test_cases,
    model_config={"model": "gemini-pro", "temperature": 0.1},
    test_name="sentiment_comparison"
)

print(f"A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼: {ab_result}")

# ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„±
report = doc_system.generate_performance_report("sentiment_classifier")
print(f"ì„±ëŠ¥ ë³´ê³ ì„œ: {report}")

# CSV ë‚´ë³´ë‚´ê¸°
doc_system.export_to_csv("prompt_attempts.csv")
```

### ğŸ“Š **ì‘ì—…ë³„ ìµœì í™” ê°€ì´ë“œ**

| ì‘ì—… ìœ í˜• | Temperature | Max Tokens | ê¶Œì¥ ê¸°ë²• | ì£¼ì˜ì‚¬í•­ |
|-----------|-------------|------------|-----------|----------|
| ë¶„ë¥˜ | 0.0-0.1 | 10-50 | Few-shot + ì˜ˆì‹œ ì„ê¸° | í´ë˜ìŠ¤ ê· í˜• |
| ìš”ì•½ | 0.1-0.3 | 200-500 | êµ¬ì¡°í™”ëœ ì§€ì‹œì‚¬í•­ | ê¸¸ì´ ì œí•œ ëª…ì‹œ |
| ìƒì„± | 0.7-0.9 | 1024-2048 | ì—­í•  í”„ë¡¬í”„íŒ… | ì°½ì˜ì„±ê³¼ ì¼ê´€ì„± ê· í˜• |
| ì¶”ì¶œ | 0.0-0.2 | 100-300 | JSON ìŠ¤í‚¤ë§ˆ í™œìš© | ì •í™•í•œ í˜•ì‹ ì§€ì • |
| ë²ˆì—­ | 0.1-0.3 | ì›ë¬¸ì˜ 1.5ë°° | ë§¥ë½ ì œê³µ | ë¬¸í™”ì  ë‰˜ì•™ìŠ¤ ê³ ë ¤ |

### ğŸ”„ **ì§€ì†ì  ê°œì„  í”„ë¡œì„¸ìŠ¤**
```python
class ContinuousImprovementSystem:
    def __init__(self):
        self.feedback_loop = []
        self.performance_baseline = {}
    
    def establish_baseline(self, prompt_name: str, initial_performance: Dict):
        """ì„±ëŠ¥ ê¸°ì¤€ì„  ì„¤ì •"""
        self.performance_baseline[prompt_name] = {
            'initial_success_rate': initial_performance['success_rate'],
            'initial_speed': initial_performance['avg_time'],
            'established_date': datetime.datetime.now()
        }
    
    def track_performance_drift(self, prompt_name: str, 
                              current_performance: Dict) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë³€í™” ì¶”ì """
        if prompt_name not in self.performance_baseline:
            return {"error": "ê¸°ì¤€ì„ ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"}
        
        baseline = self.performance_baseline[prompt_name]
        
        success_drift = current_performance['success_rate'] - baseline['initial_success_rate']
        speed_drift = current_performance['avg_time'] - baseline['initial_speed']
        
        return {
            "success_rate_drift": success_drift,
            "speed_drift": speed_drift,
            "needs_optimization": abs(success_drift) > 0.1 or speed_drift > 2.0,
            "recommendation": self._generate_optimization_recommendation(
                success_drift, speed_drift
            )
        }
    
    def _generate_optimization_recommendation(self, success_drift: float, 
                                           speed_drift: float) -> str:
        """ìµœì í™” ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        if success_drift < -0.1:
            return "ì„±ëŠ¥ ì €í•˜ ê°ì§€: í”„ë¡¬í”„íŠ¸ ì¬ê²€í†  ë° ì˜ˆì‹œ ì—…ë°ì´íŠ¸ í•„ìš”"
        elif speed_drift > 2.0:
            return "ì‘ë‹µ ì†ë„ ì €í•˜: í† í° ê¸¸ì´ ì¤„ì´ê¸° ë˜ëŠ” Temperature ë‚®ì¶”ê¸°"
        elif success_drift > 0.1:
            return "ì„±ëŠ¥ í–¥ìƒ í™•ì¸: í˜„ì¬ ì„¤ì • ìœ ì§€ ê¶Œì¥"
        else:
            return "ì•ˆì •ì  ì„±ëŠ¥: ì •ê¸° ëª¨ë‹ˆí„°ë§ ì§€ì†"

# ì‹¤ë¬´ ì²´í¬ë¦¬ìŠ¤íŠ¸ ìƒì„±ê¸°
class PracticalChecklistGenerator:
    @staticmethod
    def generate_prompt_checklist() -> List[str]:
        """í”„ë¡¬í”„íŠ¸ ì‘ì„± ì²´í¬ë¦¬ìŠ¤íŠ¸"""
        return [
            "âœ… ëª…í™•í•˜ê³  êµ¬ì²´ì ì¸ ì‘ì—… ì •ì˜",
            "âœ… ì ì ˆí•œ ì˜ˆì‹œ ì œê³µ (3-5ê°œ)",
            "âœ… ì›í•˜ëŠ” ì¶œë ¥ í˜•ì‹ ëª…ì‹œ",
            "âœ… ì—­í•  ë˜ëŠ” ë§¥ë½ ì„¤ì •",
            "âœ… ì œì•½ì‚¬í•­ë³´ë‹¤ ì§€ì‹œì‚¬í•­ ìš°ì„ ",
            "âœ… ë³€ìˆ˜ í™œìš©ìœ¼ë¡œ ì¬ì‚¬ìš©ì„± í™•ë³´",
            "âœ… ì˜¨ë„ ì„¤ì • ìµœì í™”",
            "âœ… í† í° ê¸¸ì´ ì ì ˆíˆ ì œí•œ",
            "âœ… í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì¤€ë¹„",
            "âœ… ì„±ëŠ¥ ì¸¡ì • ë°©ë²• ì •ì˜"
        ]
    
    @staticmethod
    def generate_testing_checklist() -> List[str]:
        """í”„ë¡¬í”„íŠ¸ í…ŒìŠ¤íŠ¸ ì²´í¬ë¦¬ìŠ¤íŠ¸"""
        return [
            "âœ… ë‹¤ì–‘í•œ ì…ë ¥ìœ¼ë¡œ í…ŒìŠ¤íŠ¸",
            "âœ… ê²½ê³„ ì‚¬ë¡€(edge case) í™•ì¸",
            "âœ… ì¼ê´€ì„± ê²€ì¦ (ì—¬ëŸ¬ ë²ˆ ì‹¤í–‰)",
            "âœ… ì„±ëŠ¥ ì§€í‘œ ì¸¡ì •",
            "âœ… ì˜¤ë¥˜ ì²˜ë¦¬ í™•ì¸",
            "âœ… ì¶œë ¥ í˜•ì‹ ê²€ì¦",
            "âœ… í¸í–¥ì„± ì²´í¬",
            "âœ… ì•ˆì „ì„± ê²€í† ",
            "âœ… ë¹„ìš© ëŒ€ë¹„ íš¨ê³¼ ë¶„ì„",
            "âœ… ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘"
        ]

# ì‚¬ìš© ì˜ˆì‹œ
improvement_system = ContinuousImprovementSystem()
checklist_gen = PracticalChecklistGenerator()

# ê¸°ì¤€ì„  ì„¤ì •
improvement_system.establish_baseline(
    "customer_inquiry_classifier",
    {"success_rate": 0.85, "avg_time": 1.5}
)

# ì„±ëŠ¥ ë“œë¦¬í”„íŠ¸ í™•ì¸
drift_result = improvement_system.track_performance_drift(
    "customer_inquiry_classifier",
    {"success_rate": 0.78, "avg_time": 2.1}
)

print(f"ì„±ëŠ¥ ë³€í™” ë¶„ì„: {drift_result}")

# ì²´í¬ë¦¬ìŠ¤íŠ¸ ì¶œë ¥
print("\n=== í”„ë¡¬í”„íŠ¸ ì‘ì„± ì²´í¬ë¦¬ìŠ¤íŠ¸ ===")
for item in checklist_gen.generate_prompt_checklist():
    print(item)

print("\n=== í”„ë¡¬í”„íŠ¸ í…ŒìŠ¤íŠ¸ ì²´í¬ë¦¬ìŠ¤íŠ¸ ===")
for item in checklist_gen.generate_testing_checklist():
    print(item)
```

### ğŸ’¡ **ì‹¤ë¬´ íŒ ì •ë¦¬**

#### ğŸš€ **íš¨ìœ¨ì„± í–¥ìƒ**
- **í…œí”Œë¦¿ ì¬ì‚¬ìš©**: ì„±ê³µí•œ í”„ë¡¬í”„íŠ¸ íŒ¨í„´ì„ í…œí”Œë¦¿í™”í•˜ì—¬ ì¬í™œìš©
- **ë°°ì¹˜ ì²˜ë¦¬**: ìœ ì‚¬í•œ ì‘ì—…ë“¤ì„ í•œ ë²ˆì— ì²˜ë¦¬í•˜ì—¬ ë¹„ìš© ì ˆì•½
- **ìºì‹± í™œìš©**: ë°˜ë³µì ì¸ ì¿¼ë¦¬ ê²°ê³¼ë¥¼ ìºì‹±í•˜ì—¬ ì‘ë‹µ ì†ë„ í–¥ìƒ

#### ğŸ¯ **í’ˆì§ˆ ë³´ì¥**
- **ë‹¤ë‹¨ê³„ ê²€ì¦**: ì¶œë ¥ í˜•ì‹ â†’ ë‚´ìš© ì •í™•ì„± â†’ í¸í–¥ì„± ìˆœìœ¼ë¡œ ê²€ì¦
- **ì‚¬ìš©ì í”¼ë“œë°± ë£¨í”„**: ì‹¤ì œ ì‚¬ìš©ì í”¼ë“œë°±ì„ í”„ë¡¬í”„íŠ¸ ê°œì„ ì— ë°˜ì˜
- **ì •ê¸°ì  ì„±ëŠ¥ ë¦¬ë·°**: ëª¨ë¸ ì—…ë°ì´íŠ¸ë‚˜ ë°ì´í„° ë³€í™”ì— ë”°ë¥¸ ì„±ëŠ¥ ë³€í™” ëª¨ë‹ˆí„°ë§

#### ğŸ“ˆ **í™•ì¥ì„± ê³ ë ¤**
- **ëª¨ë“ˆí™”ëœ í”„ë¡¬í”„íŠ¸**: í° ì‘ì—…ì„ ì‘ì€ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ì—¬ ê´€ë¦¬
- **ë‹¤êµ­ì–´ ì§€ì›**: ì–¸ì–´ë³„ íŠ¹ì„±ì„ ê³ ë ¤í•œ í”„ë¡¬í”„íŠ¸ ë³€í˜• ì¤€ë¹„
- **í”Œë«í¼ ë…ë¦½ì„±**: íŠ¹ì • ëª¨ë¸ì— ì¢…ì†ë˜ì§€ ì•ŠëŠ” ì¼ë°˜í™”ëœ í”„ë¡¬í”„íŠ¸ ì„¤ê³„

---

* ì¶œì²˜
  * [1] [Prompt Engineering from Google](https://www.kaggle.com/whitepaper-prompt-engineering)