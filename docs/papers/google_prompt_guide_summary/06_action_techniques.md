# ğŸ“˜ 06. í–‰ë™ ê¸°ë°˜ ê¸°ë²• (Action-based Techniques)

## í•µì‹¬ ìš”ì•½
- **ReAct (Reason & Act)**ëŠ” ìì—°ì–´ ì¶”ë¡ ê³¼ ì™¸ë¶€ ë„êµ¬ ìƒí˜¸ì‘ìš©ì„ ê²°í•©í•˜ì—¬ ë³µì¡í•œ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ì—ì´ì „íŠ¸ ê¸°ë°˜ ì ‘ê·¼ë²•
- **ì‚¬ê³ -í–‰ë™-ê´€ì°° ë£¨í”„**ë¥¼ í†µí•´ ì¸ê°„ì˜ ì‹¤ì œ ë¬¸ì œ í•´ê²° ë°©ì‹ì„ ëª¨ë°©í•˜ë©°, ì™¸ë¶€ ì •ë³´ ê²€ìƒ‰ì´ í•„ìš”í•œ ì‘ì—…ì— íŠ¹íˆ íš¨ê³¼ì 
- **Automatic Prompt Engineering (APE)**ëŠ” í”„ë¡¬í”„íŠ¸ ì‘ì„±ì„ ìë™í™”í•˜ì—¬ ì¸ê°„ì˜ ê°œì… ì—†ì´ë„ ëª¨ë¸ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ê¸°ë²•
- **ë°˜ë³µì  ê°œì„ **ì„ í†µí•´ í›„ë³´ í”„ë¡¬í”„íŠ¸ë“¤ì„ ìƒì„±í•˜ê³  í‰ê°€í•˜ì—¬ ìµœì ì˜ í”„ë¡¬í”„íŠ¸ë¥¼ ì„ íƒí•˜ëŠ” ê³¼ì •

## ì£¼ìš” ê°œë…ê³¼ ì„¤ëª…

### ğŸ¤– **ReAct (Reason & Act)**
- **í•µì‹¬ ê°œë…**: ì¶”ë¡ (Reasoning) + í–‰ë™(Acting)ì„ ë°˜ë³µì ìœ¼ë¡œ ìˆ˜í–‰
- **ì‘ë™ ì›ë¦¬**: 
  1. **ì¶”ë¡ **: ë¬¸ì œ ë¶„ì„ ë° ë‹¤ìŒ í–‰ë™ ê³„íš
  2. **í–‰ë™**: ì™¸ë¶€ ë„êµ¬ ì‚¬ìš© (ê²€ìƒ‰, API í˜¸ì¶œ, ì½”ë“œ ì‹¤í–‰ ë“±)
  3. **ê´€ì°°**: í–‰ë™ ê²°ê³¼ í™•ì¸
  4. **ë°˜ë³µ**: ëª©í‘œ ë‹¬ì„±ê¹Œì§€ ì‚¬ì´í´ ì§€ì†
- **ì¥ì **: ì™¸ë¶€ ì •ë³´ ì ‘ê·¼, ë™ì  ë¬¸ì œ í•´ê²°, í•´ì„ ê°€ëŠ¥ì„± ì œê³µ

### ğŸ› ï¸ **ReActì˜ êµ¬ì„± ìš”ì†Œ**
- **Tools**: ê²€ìƒ‰ ì—”ì§„, ê³„ì‚°ê¸°, ë°ì´í„°ë² ì´ìŠ¤, API ë“±
- **Agent**: ì¶”ë¡ ê³¼ ë„êµ¬ ì„ íƒì„ ë‹´ë‹¹í•˜ëŠ” LLM
- **Memory**: ì´ì „ ìƒí˜¸ì‘ìš© ê¸°ë¡ ì €ì¥
- **Controller**: ì „ì²´ í”„ë¡œì„¸ìŠ¤ ê´€ë¦¬ ë° ì¢…ë£Œ ì¡°ê±´ ì œì–´

### ğŸ”„ **Automatic Prompt Engineering (APE)**
- **ëª©ì **: í”„ë¡¬í”„íŠ¸ ì‘ì„± ê³¼ì • ìë™í™”
- **ë°©ë²•ë¡ **:
  1. ì´ˆê¸° í”„ë¡¬í”„íŠ¸ í›„ë³´ë“¤ ìƒì„±
  2. ê° í›„ë³´ì˜ ì„±ëŠ¥ í‰ê°€
  3. ìµœê³  ì„±ëŠ¥ í›„ë³´ ì„ íƒ
  4. ì„ íƒëœ í”„ë¡¬í”„íŠ¸ ê°œì„  ë° ë°˜ë³µ
- **í‰ê°€ ì§€í‘œ**: BLEU, ROUGE, ì •í™•ë„, F1-score ë“±

## í”„ë¡¬í”„íŠ¸ ì˜ˆì‹œ

### ReAct íŒ¨í„´ - ì •ë³´ ê²€ìƒ‰ ì‘ì—…
```plaintext
# ë¬¸ì œ
Metallica ë°´ë“œ ë©¤ë²„ë“¤ì˜ ìë…€ ìˆ˜ë¥¼ ëª¨ë‘ í•©í•˜ë©´ ëª‡ ëª…ì¸ê°€ìš”?

# ReAct ì¶”ë¡  ê³¼ì •
ìƒê°: MetallicaëŠ” 4ëª…ì˜ ë©¤ë²„ë¡œ êµ¬ì„±ëœ ë°´ë“œì…ë‹ˆë‹¤. ê° ë©¤ë²„ì˜ ìë…€ ìˆ˜ë¥¼ ì•Œì•„ë´ì•¼ í•©ë‹ˆë‹¤.

í–‰ë™: ê²€ìƒ‰
í–‰ë™ ì…ë ¥: James Hetfield ìë…€ ìˆ˜

ê´€ì°°: James HetfieldëŠ” 3ëª…ì˜ ìë…€ê°€ ìˆìŠµë‹ˆë‹¤.

ìƒê°: 1/4 Metallica ë°´ë“œ ë©¤ë²„ê°€ 3ëª…ì˜ ìë…€ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.

í–‰ë™: ê²€ìƒ‰  
í–‰ë™ ì…ë ¥: Lars Ulrich ìë…€ ìˆ˜

ê´€ì°°: Lars UlrichëŠ” 3ëª…ì˜ ìë…€ê°€ ìˆìŠµë‹ˆë‹¤.

ìƒê°: 2/4 Metallica ë°´ë“œ ë©¤ë²„ê°€ ì´ 6ëª…ì˜ ìë…€ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.

í–‰ë™: ê²€ìƒ‰
í–‰ë™ ì…ë ¥: Kirk Hammett ìë…€ ìˆ˜

ê´€ì°°: Kirk HammettëŠ” ë‘ ë²ˆì§¸ ì•„ë‚´ Laniì™€ 1998ë…„ë¶€í„° ê²°í˜¼í–ˆìœ¼ë©°, 
Angel(2006ë…„ìƒ)ê³¼ Vincenzo(2008ë…„ìƒ) ë‘ ì•„ë“¤ì´ ìˆìŠµë‹ˆë‹¤.

ìƒê°: 3/4 Metallica ë°´ë“œ ë©¤ë²„ê°€ ì´ 8ëª…ì˜ ìë…€ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.

í–‰ë™: ê²€ìƒ‰
í–‰ë™ ì…ë ¥: Robert Trujillo ìë…€ ìˆ˜

ê´€ì°°: Robert TrujilloëŠ” 2ëª…ì˜ ìë…€ê°€ ìˆìŠµë‹ˆë‹¤.

ìƒê°: 4/4 Metallica ë°´ë“œ ë©¤ë²„ê°€ ì´ 10ëª…ì˜ ìë…€ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.

ìµœì¢… ë‹µë³€: 10ëª…
```

### APE - í”„ë¡¬í”„íŠ¸ ìƒì„± ìë™í™”
```plaintext
# ë‹¨ê³„ 1: í”„ë¡¬í”„íŠ¸ í›„ë³´ ìƒì„±
ìš°ë¦¬ëŠ” ë°´ë“œ êµ¿ì¦ˆ í‹°ì…”ì¸  ì›¹ìƒµì„ ìš´ì˜í•˜ë©°, ì±—ë´‡ í›ˆë ¨ì„ ìœ„í•´ 
"Metallica í‹°ì…”ì¸  Sì‚¬ì´ì¦ˆ 1ê°œ ì£¼ë¬¸"ê³¼ ê°™ì€ ì˜ë¯¸ì˜ ë‹¤ì–‘í•œ í‘œí˜„ì´ í•„ìš”í•©ë‹ˆë‹¤. 
ë™ì¼í•œ ì˜ë¯¸ë¥¼ ê°€ì§„ 10ê°€ì§€ ë³€í˜•ì„ ìƒì„±í•˜ì„¸ìš”.

# ìƒì„±ëœ í›„ë³´ë“¤:
1. Sì‚¬ì´ì¦ˆ Metallica í‹°ì…”ì¸  êµ¬ë§¤í•˜ê³  ì‹¶ì–´ìš”
2. Metallica í‹°ì…”ì¸  ìŠ¤ëª° ì‚¬ì´ì¦ˆë¡œ ì£¼ë¬¸í•  ìˆ˜ ìˆë‚˜ìš”?
3. ì‘ì€ ì‚¬ì´ì¦ˆ Metallica í‹°ì…”ì¸  í•˜ë‚˜ ì£¼ì„¸ìš”
4. Metallica ì…”ì¸  S ì‚¬ì´ì¦ˆ í•œ ì¥ ë¶€íƒë“œë¦½ë‹ˆë‹¤
5. S ì‚¬ì´ì¦ˆ Metallica í‹°ì…”ì¸  1ê°œ ì›í•©ë‹ˆë‹¤
6. ìŠ¤ëª° ì‚¬ì´ì¦ˆ Metallica í‹°ì…”ì¸  ì£¼ë¬¸í•˜ê² ìŠµë‹ˆë‹¤
7. Metallica ì‘ì€ ì‚¬ì´ì¦ˆ í‹°ì…”ì¸  í•˜ë‚˜ ì°¾ê³  ìˆì–´ìš”
8. Sì‚¬ì´ì¦ˆ Metallica í‹°ì…”ì¸  í•˜ë‚˜ êµ¬í•  ìˆ˜ ìˆë‚˜ìš”?
9. ì‘ì€ ì‚¬ì´ì¦ˆë¡œ Metallica í‹°ì…”ì¸  í•œ ì¥ ì£¼ì„¸ìš”
10. Metallica ìŠ¤ëª° í‹°ì…”ì¸  1ê°œ ì£¼ë¬¸í•˜ê² ìŠµë‹ˆë‹¤

# ë‹¨ê³„ 2: í‰ê°€ ë° ì„ íƒ
# ê° í›„ë³´ë¥¼ BLEU/ROUGE ìŠ¤ì½”ì–´ë¡œ í‰ê°€í•˜ì—¬ ìµœì  í›„ë³´ ì„ íƒ
```

## í™œìš© íŒ

### ğŸš€ **LangChain ReAct ì—ì´ì „íŠ¸ êµ¬í˜„**
```python
from langchain.agents import load_tools, initialize_agent, AgentType
from langchain.llms import VertexAI
from langchain.tools import Tool
import os

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
os.environ["SERPAPI_API_KEY"] = "your-serpapi-key"

class ReActAgent:
    def __init__(self, model_name="gemini-pro"):
        self.llm = VertexAI(
            model_name=model_name,
            temperature=0.1,  # ì •í™•í•œ ì¶”ë¡ ì„ ìœ„í•´ ë‚®ì€ ì˜¨ë„
            max_output_tokens=2048
        )
        
        # ê¸°ë³¸ ë„êµ¬ ë¡œë“œ
        self.tools = load_tools(["serpapi", "llm-math"], llm=self.llm)
        
        # ì»¤ìŠ¤í…€ ë„êµ¬ ì¶”ê°€
        self.tools.append(self._create_korean_search_tool())
        
        # ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
        self.agent = initialize_agent(
            self.tools,
            self.llm,
            agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
            verbose=True,
            max_iterations=5,  # ë¬´í•œ ë£¨í”„ ë°©ì§€
            early_stopping_method="generate"
        )
    
    def _create_korean_search_tool(self):
        """í•œêµ­ì–´ ê²€ìƒ‰ì— íŠ¹í™”ëœ ì»¤ìŠ¤í…€ ë„êµ¬"""
        def korean_search(query: str) -> str:
            # í•œêµ­ì–´ ê²€ìƒ‰ ë¡œì§ êµ¬í˜„
            return f"í•œêµ­ì–´ ê²€ìƒ‰ ê²°ê³¼: {query}ì— ëŒ€í•œ ì •ë³´..."
        
        return Tool(
            name="Korean Search",
            description="í•œêµ­ì–´ ì •ë³´ë¥¼ ê²€ìƒ‰í•  ë•Œ ì‚¬ìš©í•˜ëŠ” ë„êµ¬",
            func=korean_search
        )
    
    def solve_complex_query(self, query: str) -> str:
        """ë³µì¡í•œ ì¿¼ë¦¬ë¥¼ ReAct ë°©ì‹ìœ¼ë¡œ í•´ê²°"""
        try:
            result = self.agent.run(query)
            return result
        except Exception as e:
            return f"ì—ëŸ¬ ë°œìƒ: {str(e)}"

# ì‚¬ìš© ì˜ˆì‹œ
react_agent = ReActAgent()

# ë³µì¡í•œ ì •ë³´ ê²€ìƒ‰ ë° ê³„ì‚°
query = """
2023ë…„ ë…¸ë²¨ ë¬¼ë¦¬í•™ìƒ ìˆ˜ìƒìë“¤ì˜ ì†Œì† ëŒ€í•™êµë¥¼ ì°¾ê³ , 
ê° ëŒ€í•™êµê°€ ìœ„ì¹˜í•œ ë‚˜ë¼ë³„ë¡œ ë¶„ë¥˜í•´ì£¼ì„¸ìš”.
"""

result = react_agent.solve_complex_query(query)
print(result)
```

### ğŸ¯ **ì»¤ìŠ¤í…€ ReAct ì²´ì¸ êµ¬í˜„**
```python
from typing import List, Dict, Any, Optional
import json

class CustomReActChain:
    def __init__(self, llm, tools: List[Tool]):
        self.llm = llm
        self.tools = {tool.name: tool for tool in tools}
        self.memory = []
        self.max_iterations = 10
    
    def _format_tools_description(self) -> str:
        """ë„êµ¬ ì„¤ëª… í¬ë§·"""
        descriptions = []
        for tool_name, tool in self.tools.items():
            descriptions.append(f"- {tool_name}: {tool.description}")
        return "\n".join(descriptions)
    
    def _parse_action(self, text: str) -> Optional[Dict[str, str]]:
        """ì‘ë‹µì—ì„œ í–‰ë™ê³¼ ì…ë ¥ íŒŒì‹±"""
        try:
            lines = text.strip().split('\n')
            action = None
            action_input = None
            
            for line in lines:
                if line.startswith('í–‰ë™:') or line.startswith('Action:'):
                    action = line.split(':', 1)[1].strip()
                elif line.startswith('í–‰ë™ ì…ë ¥:') or line.startswith('Action Input:'):
                    action_input = line.split(':', 1)[1].strip()
            
            if action and action_input:
                return {"action": action, "action_input": action_input}
            return None
        except:
            return None
    
    def _execute_tool(self, action: str, action_input: str) -> str:
        """ë„êµ¬ ì‹¤í–‰"""
        if action in self.tools:
            try:
                result = self.tools[action].run(action_input)
                return str(result)
            except Exception as e:
                return f"ë„êµ¬ ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}"
        else:
            return f"ì•Œ ìˆ˜ ì—†ëŠ” ë„êµ¬: {action}. ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬: {list(self.tools.keys())}"
    
    def solve(self, question: str) -> Dict[str, Any]:
        """ReAct ë°©ì‹ìœ¼ë¡œ ë¬¸ì œ í•´ê²°"""
        
        # ì´ˆê¸° í”„ë¡¬í”„íŠ¸
        initial_prompt = f"""
ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µí•˜ê¸° ìœ„í•´ ë‹¨ê³„ë³„ë¡œ ì¶”ë¡ í•˜ê³  í•„ìš”ì‹œ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.

ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬:
{self._format_tools_description()}

ë„êµ¬ ì‚¬ìš© í˜•ì‹:
ìƒê°: [í˜„ì¬ ìƒí™©ì— ëŒ€í•œ ë¶„ì„]
í–‰ë™: [ë„êµ¬ ì´ë¦„]
í–‰ë™ ì…ë ¥: [ë„êµ¬ì— ì „ë‹¬í•  ì…ë ¥]
ê´€ì°°: [ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ë¥¼ ê¸°ë‹¤ë¦¼]

ì§ˆë¬¸: {question}

ìƒê°:"""

        conversation_history = initial_prompt
        
        for iteration in range(self.max_iterations):
            # LLMì—ê²Œ ì‘ë‹µ ìš”ì²­
            response = self.llm.predict(conversation_history)
            conversation_history += response
            
            # í–‰ë™ íŒŒì‹±
            action_dict = self._parse_action(response)
            
            if action_dict:
                # ë„êµ¬ ì‹¤í–‰
                observation = self._execute_tool(
                    action_dict["action"], 
                    action_dict["action_input"]
                )
                
                # ëŒ€í™” ê¸°ë¡ì— ê´€ì°° ê²°ê³¼ ì¶”ê°€
                conversation_history += f"\nê´€ì°°: {observation}\n\nìƒê°:"
                
                # ë©”ëª¨ë¦¬ì— ì €ì¥
                self.memory.append({
                    "iteration": iteration + 1,
                    "thought": response,
                    "action": action_dict["action"],
                    "action_input": action_dict["action_input"],
                    "observation": observation
                })
            else:
                # ìµœì¢… ë‹µë³€ìœ¼ë¡œ ê°„ì£¼
                final_answer = response.split("ìµœì¢… ë‹µë³€:")[-1].strip() if "ìµœì¢… ë‹µë³€:" in response else response
                
                return {
                    "question": question,
                    "final_answer": final_answer,
                    "reasoning_steps": self.memory,
                    "total_iterations": iteration + 1,
                    "full_conversation": conversation_history
                }
        
        return {
            "question": question,
            "final_answer": "ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤.",
            "reasoning_steps": self.memory,
            "total_iterations": self.max_iterations,
            "full_conversation": conversation_history
        }

# ì‚¬ìš© ì˜ˆì‹œ
from langchain.tools import DuckDuckGoSearchRun, Calculator

# ë„êµ¬ ì •ì˜
search = DuckDuckGoSearchRun()
calculator = Calculator()

tools = [
    Tool(name="Search", description="ì¸í„°ë„· ê²€ìƒ‰", func=search.run),
    Tool(name="Calculator", description="ìˆ˜í•™ ê³„ì‚°", func=calculator.run)
]

# ì»¤ìŠ¤í…€ ReAct ì²´ì¸ ì‹¤í–‰
llm = VertexAI(model_name="gemini-pro", temperature=0.1)
react_chain = CustomReActChain(llm, tools)

result = react_chain.solve("BTS ë©¤ë²„ ìˆ˜ì™€ BLACKPINK ë©¤ë²„ ìˆ˜ë¥¼ ë”í•œ ê°’ì€?")
print(f"ìµœì¢… ë‹µë³€: {result['final_answer']}")
```

### ğŸ”„ **Automatic Prompt Engineering êµ¬í˜„**
```python
from typing import List, Tuple
import numpy as np
from sklearn.metrics import accuracy_score
import itertools

class AutomaticPromptEngineer:
    def __init__(self, llm, evaluation_llm=None):
        self.llm = llm
        self.evaluation_llm = evaluation_llm or llm
        self.prompt_candidates = []
        self.evaluation_results = []
    
    def generate_prompt_candidates(self, task_description: str, num_candidates: int = 10) -> List[str]:
        """í”„ë¡¬í”„íŠ¸ í›„ë³´ë“¤ ìƒì„±"""
        generation_prompt = f"""
ì‘ì—… ì„¤ëª…: {task_description}

ì´ ì‘ì—…ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•œ {num_candidates}ê°€ì§€ ë‹¤ë¥¸ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•˜ì„¸ìš”. 
ê°ê° ë‹¤ë¥¸ ì ‘ê·¼ ë°©ì‹ê³¼ ìŠ¤íƒ€ì¼ì„ ì‚¬ìš©í•˜ì„¸ìš”.

í”„ë¡¬í”„íŠ¸ 1:
í”„ë¡¬í”„íŠ¸ 2:
...
"""
        
        response = self.llm.predict(generation_prompt)
        
        # ì‘ë‹µì—ì„œ í”„ë¡¬í”„íŠ¸ë“¤ ì¶”ì¶œ (ê°„ë‹¨í•œ íŒŒì‹±)
        candidates = []
        lines = response.split('\n')
        current_prompt = ""
        
        for line in lines:
            if line.startswith('í”„ë¡¬í”„íŠ¸ ') and current_prompt:
                candidates.append(current_prompt.strip())
                current_prompt = ""
            elif line.startswith('í”„ë¡¬í”„íŠ¸ '):
                current_prompt = line.split(':', 1)[1].strip() if ':' in line else ""
            else:
                current_prompt += " " + line.strip()
        
        if current_prompt:
            candidates.append(current_prompt.strip())
        
        self.prompt_candidates = candidates[:num_candidates]
        return self.prompt_candidates
    
    def evaluate_prompts(self, test_cases: List[Dict[str, str]], metric: str = "accuracy") -> List[float]:
        """í”„ë¡¬í”„íŠ¸ í›„ë³´ë“¤ í‰ê°€"""
        scores = []
        
        for prompt in self.prompt_candidates:
            if metric == "accuracy":
                score = self._evaluate_accuracy(prompt, test_cases)
            elif metric == "bleu":
                score = self._evaluate_bleu(prompt, test_cases)
            else:
                score = self._evaluate_custom(prompt, test_cases, metric)
            
            scores.append(score)
        
        self.evaluation_results = scores
        return scores
    
    def _evaluate_accuracy(self, prompt: str, test_cases: List[Dict[str, str]]) -> float:
        """ì •í™•ë„ ê¸°ë°˜ í‰ê°€"""
        correct = 0
        total = len(test_cases)
        
        for case in test_cases:
            full_prompt = prompt + "\n\n" + case["input"]
            try:
                response = self.llm.predict(full_prompt)
                predicted = response.strip().upper()
                expected = case["expected"].strip().upper()
                
                if predicted == expected:
                    correct += 1
            except:
                continue
        
        return correct / total if total > 0 else 0.0
    
    def _evaluate_bleu(self, prompt: str, test_cases: List[Dict[str, str]]) -> float:
        """BLEU ìŠ¤ì½”ì–´ ê¸°ë°˜ í‰ê°€"""
        from nltk.translate.bleu_score import sentence_bleu
        
        total_bleu = 0
        count = 0
        
        for case in test_cases:
            full_prompt = prompt + "\n\n" + case["input"]
            try:
                response = self.llm.predict(full_prompt)
                reference = [case["expected"].split()]
                candidate = response.strip().split()
                
                bleu = sentence_bleu(reference, candidate)
                total_bleu += bleu
                count += 1
            except:
                continue
        
        return total_bleu / count if count > 0 else 0.0
    
    def select_best_prompt(self) -> Tuple[str, float]:
        """ìµœê³  ì„±ëŠ¥ í”„ë¡¬í”„íŠ¸ ì„ íƒ"""
        if not self.evaluation_results:
            raise ValueError("ë¨¼ì € í”„ë¡¬í”„íŠ¸ í‰ê°€ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        
        best_idx = np.argmax(self.evaluation_results)
        best_prompt = self.prompt_candidates[best_idx]
        best_score = self.evaluation_results[best_idx]
        
        return best_prompt, best_score
    
    def improve_prompt(self, base_prompt: str, improvement_type: str = "optimize") -> List[str]:
        """í”„ë¡¬í”„íŠ¸ ê°œì„ """
        improvement_prompt = f"""
ê¸°ë³¸ í”„ë¡¬í”„íŠ¸: {base_prompt}

ì´ í”„ë¡¬í”„íŠ¸ë¥¼ ë‹¤ìŒ ë°©í–¥ìœ¼ë¡œ ê°œì„ í•œ 3ê°€ì§€ ë²„ì „ì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”:
1. ë” ëª…í™•í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ
2. ë” ê°„ê²°í•˜ê³  íš¨ìœ¨ì ìœ¼ë¡œ  
3. ë” ì°½ì˜ì ì´ê³  ë‹¤ì–‘í•œ ì ‘ê·¼ìœ¼ë¡œ

ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ 1:
ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ 2:
ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ 3:
"""
        
        response = self.llm.predict(improvement_prompt)
        # íŒŒì‹± ë¡œì§ êµ¬í˜„
        improved_prompts = self._parse_improved_prompts(response)
        
        return improved_prompts
    
    def run_ape_cycle(self, task_description: str, test_cases: List[Dict], iterations: int = 3) -> Dict:
        """ì „ì²´ APE ì‚¬ì´í´ ì‹¤í–‰"""
        best_prompts = []
        
        for iteration in range(iterations):
            print(f"=== APE ë°˜ë³µ {iteration + 1} ===")
            
            # 1ë‹¨ê³„: í”„ë¡¬í”„íŠ¸ í›„ë³´ ìƒì„±
            if iteration == 0:
                candidates = self.generate_prompt_candidates(task_description)
            else:
                # ì´ì „ ìµœê³  í”„ë¡¬í”„íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°œì„ 
                best_prompt, _ = self.select_best_prompt()
                candidates = self.improve_prompt(best_prompt)
                self.prompt_candidates = candidates
            
            # 2ë‹¨ê³„: í‰ê°€
            scores = self.evaluate_prompts(test_cases)
            
            # 3ë‹¨ê³„: ìµœê³  ì„±ëŠ¥ ì„ íƒ
            best_prompt, best_score = self.select_best_prompt()
            best_prompts.append((best_prompt, best_score))
            
            print(f"ë°˜ë³µ {iteration + 1} ìµœê³  ì ìˆ˜: {best_score:.3f}")
        
        # ì „ì²´ ë°˜ë³µì—ì„œ ìµœê³  ì„±ëŠ¥ í”„ë¡¬í”„íŠ¸ ì„ íƒ
        final_best = max(best_prompts, key=lambda x: x[1])
        
        return {
            "best_prompt": final_best[0],
            "best_score": final_best[1],
            "iteration_results": best_prompts,
            "all_candidates": self.prompt_candidates,
            "all_scores": self.evaluation_results
        }

# ì‚¬ìš© ì˜ˆì‹œ
llm = VertexAI(model_name="gemini-pro", temperature=0.7)
ape = AutomaticPromptEngineer(llm)

# í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì •ì˜
test_cases = [
    {"input": "ì´ ì˜í™”ëŠ” ì •ë§ í›Œë¥­í–ˆìŠµë‹ˆë‹¤!", "expected": "POSITIVE"},
    {"input": "ì‹œê°„ ë‚­ë¹„ì˜€ì–´ìš”.", "expected": "NEGATIVE"},
    {"input": "ê·¸ëƒ¥ ê·¸ë¬ì–´ìš”.", "expected": "NEUTRAL"}
]

# APE ì‹¤í–‰
result = ape.run_ape_cycle(
    task_description="ì˜í™” ë¦¬ë·°ë¥¼ POSITIVE, NEGATIVE, NEUTRALë¡œ ë¶„ë¥˜",
    test_cases=test_cases,
    iterations=3
)

print(f"ìµœì¢… ìµœì  í”„ë¡¬í”„íŠ¸: {result['best_prompt']}")
print(f"ì„±ëŠ¥ ì ìˆ˜: {result['best_score']:.3f}")
```

### ğŸ“Š **ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ìµœì í™”**
```python
class ReActPerformanceMonitor:
    def __init__(self):
        self.metrics = {
            "total_queries": 0,
            "successful_queries": 0,
            "avg_iterations": 0,
            "tool_usage": {},
            "error_types": {},
            "response_times": []
        }
    
    def log_query(self, query: str, result: Dict, execution_time: float):
        """ì¿¼ë¦¬ ì‹¤í–‰ ê²°ê³¼ ë¡œê¹…"""
        self.metrics["total_queries"] += 1
        
        if result.get("final_answer") and "ì˜¤ë¥˜" not in result["final_answer"]:
            self.metrics["successful_queries"] += 1
        
        # í‰ê·  ë°˜ë³µ íšŸìˆ˜ ì—…ë°ì´íŠ¸
        iterations = result.get("total_iterations", 0)
        current_avg = self.metrics["avg_iterations"]
        total = self.metrics["total_queries"]
        self.metrics["avg_iterations"] = (current_avg * (total - 1) + iterations) / total
        
        # ë„êµ¬ ì‚¬ìš© í†µê³„
        for step in result.get("reasoning_steps", []):
            tool = step.get("action")
            if tool:
                self.metrics["tool_usage"][tool] = self.metrics["tool_usage"].get(tool, 0) + 1
        
        # ì‘ë‹µ ì‹œê°„ ê¸°ë¡
        self.metrics["response_times"].append(execution_time)
    
    def get_performance_report(self) -> Dict:
        """ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„±"""
        if self.metrics["total_queries"] == 0:
            return {"message": "ì‹¤í–‰ëœ ì¿¼ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤."}
        
        success_rate = self.metrics["successful_queries"] / self.metrics["total_queries"]
        avg_response_time = np.mean(self.metrics["response_times"])
        
        return {
            "ì´ ì¿¼ë¦¬ ìˆ˜": self.metrics["total_queries"],
            "ì„±ê³µë¥ ": f"{success_rate:.2%}",
            "í‰ê·  ë°˜ë³µ íšŸìˆ˜": f"{self.metrics['avg_iterations']:.2f}",
            "í‰ê·  ì‘ë‹µ ì‹œê°„": f"{avg_response_time:.2f}ì´ˆ",
            "ë„êµ¬ ì‚¬ìš© ë¹ˆë„": self.metrics["tool_usage"],
            "ê°€ì¥ ë§ì´ ì‚¬ìš©ëœ ë„êµ¬": max(self.metrics["tool_usage"].items(), key=lambda x: x[1]) if self.metrics["tool_usage"] else None
        }

# ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í†µí•©
class MonitoredReActAgent(ReActAgent):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.monitor = ReActPerformanceMonitor()
    
    def solve_complex_query(self, query: str) -> str:
        import time
        start_time = time.time()
        
        try:
            result = super().solve_complex_query(query)
            execution_time = time.time() - start_time
            
            # ì„±ëŠ¥ ë°ì´í„° ë¡œê¹…
            self.monitor.log_query(query, {"final_answer": result}, execution_time)
            
            return result
        except Exception as e:
            execution_time = time.time() - start_time
            error_result = {"final_answer": f"ì˜¤ë¥˜: {str(e)}"}
            self.monitor.log_query(query, error_result, execution_time)
            raise
    
    def get_performance_summary(self):
        return self.monitor.get_performance_report()

# ì‚¬ìš©ë²•
monitored_agent = MonitoredReActAgent()

# ì—¬ëŸ¬ ì¿¼ë¦¬ ì‹¤í–‰
queries = [
    "ì„œìš¸ì˜ í˜„ì¬ ë‚ ì”¨ëŠ”?",
    "íŒŒì´ì¬ìœ¼ë¡œ í”¼ë³´ë‚˜ì¹˜ ìˆ˜ì—´ êµ¬í˜„í•˜ëŠ” ë°©ë²•ì€?",
    "2024ë…„ AI íŠ¸ë Œë“œëŠ”?"
]

for query in queries:
    try:
        result = monitored_agent.solve_complex_query(query)
        print(f"Q: {query}")
        print(f"A: {result}\n")
    except Exception as e:
        print(f"ì˜¤ë¥˜ ë°œìƒ: {e}\n")

# ì„±ëŠ¥ ìš”ì•½ í™•ì¸
print("=== ì„±ëŠ¥ ìš”ì•½ ===")
print(monitored_agent.get_performance_summary())
```

### ğŸ¯ **ì‘ì—…ë³„ ReAct ìµœì í™” ê°€ì´ë“œ**

| ì‘ì—… ìœ í˜• | ê¶Œì¥ ë„êµ¬ | Temperature | Max Iterations | ì£¼ì˜ì‚¬í•­ |
|-----------|-----------|-------------|----------------|----------|
| ì •ë³´ ê²€ìƒ‰ | Search, Calculator | 0.1-0.3 | 5-7 | ê²€ìƒ‰ ì¿¼ë¦¬ í’ˆì§ˆ |
| ë°ì´í„° ë¶„ì„ | Python REPL, SQL | 0.2-0.4 | 8-10 | ì½”ë“œ ê²€ì¦ í•„ìš” |
| ì°½ì‘ ì§€ì› | Search, Knowledge | 0.5-0.7 | 3-5 | ì°½ì˜ì„±ê³¼ ì •í™•ì„± ê· í˜• |
| ë¬¸ì œ í•´ê²° | Calculator, Search | 0.1-0.2 | 7-12 | ë…¼ë¦¬ì  ë‹¨ê³„ í™•ì¸ |

### ğŸ”§ **ReAct ë””ë²„ê¹… ë° ìµœì í™”**
```python
class ReActDebugger:
    def __init__(self, agent):
        self.agent = agent
        self.debug_logs = []
    
    def debug_solve(self, query: str, verbose: bool = True) -> Dict:
        """ë””ë²„ê¹… ëª¨ë“œë¡œ ReAct ì‹¤í–‰"""
        result = self.agent.solve_complex_query(query)
        
        debug_info = {
            "query": query,
            "result": result,
            "reasoning_analysis": self._analyze_reasoning(),
            "tool_efficiency": self._analyze_tool_usage(),
            "suggestions": self._generate_suggestions()
        }
        
        if verbose:
            self._print_debug_info(debug_info)
        
        return debug_info
    
    def _analyze_reasoning(self) -> Dict:
        """ì¶”ë¡  ê³¼ì • ë¶„ì„"""
        return {
            "logical_flow": "ì–‘í˜¸",
            "redundant_steps": 0,
            "missing_steps": []
        }
    
    def _analyze_tool_usage(self) -> Dict:
        """ë„êµ¬ ì‚¬ìš© íš¨ìœ¨ì„± ë¶„ì„"""
        return {
            "appropriate_tools": True,
            "tool_sequence": "ìµœì ",
            "unused_tools": []
        }
    
    def _generate_suggestions(self) -> List[str]:
        """ê°œì„  ì œì•ˆ ìƒì„±"""
        return [
            "ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ë” êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±",
            "ì¤‘ê°„ ê²°ê³¼ ê²€ì¦ ë‹¨ê³„ ì¶”ê°€",
            "ë„êµ¬ ì„ íƒ ê¸°ì¤€ ëª…í™•í™”"
        ]

# ë””ë²„ê±° ì‚¬ìš©
debugger = ReActDebugger(react_agent)
debug_result = debugger.debug_solve("ë³µì¡í•œ ìˆ˜í•™ ë¬¸ì œë¥¼ í•´ê²°í•´ì£¼ì„¸ìš”")
```

---

* ì¶œì²˜
  * [1] [Prompt Engineering from Google](https://www.kaggle.com/whitepaper-prompt-engineering)