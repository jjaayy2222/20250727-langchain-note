{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42eabd93",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fa8798",
   "metadata": {},
   "source": [
    "* ì¶œì²˜: LangChain ê³µì‹ ë¬¸ì„œ ë˜ëŠ” í•´ë‹¹ êµì¬ëª…\n",
    "* ì›ë³¸ URL: https://smith.langchain.com/hub/teddynote/summary-stuff-documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f188ec8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01aaaf31",
   "metadata": {},
   "source": [
    "### **`HWP` (í•œê¸€)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f4b694",
   "metadata": {},
   "source": [
    "* **`HWP`**\n",
    "\n",
    "  * í•œê¸€(`HWP`) = í•œê¸€ê³¼ì»´í“¨í„°ì—ì„œ ê°œë°œí•œ ì›Œë“œí”„ë¡œì„¸ì„œ = í•œêµ­ì˜ ëŒ€í‘œì ì¸ ë¬¸ì„œ ì‘ì„± í”„ë¡œê·¸ë¨\n",
    "\n",
    "  * íŒŒì¼ í™•ì¥ì = **`.hwp`** ì‚¬ìš© â†’ ê¸°ì—…, í•™êµ, ì •ë¶€ ê¸°ê´€ ë“±ì—ì„œ ë„ë¦¬ í™œìš©\n",
    "\n",
    "  * `LangChain`ì—ëŠ” ì•„ì§ `integration` ì´ ë˜ì§€ ì•Šì•„ ì§ì ‘ êµ¬í˜„í•œ `HWPLoader`ë¥¼ ì‚¬ìš©í•´ì•¼ í•¨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3f7579",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d6b4f2",
   "metadata": {},
   "source": [
    "### 1) Mac ì‚¬ìš©ì ì¶”ì²œ - [`Rust HWP Loader`](https://nomadamas.gitbook.io/ragchain-docs/ragchain-structure/file-loader/rust-hwp-loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b735b890",
   "metadata": {},
   "source": [
    "#### **`ê°œìš”`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795db095",
   "metadata": {},
   "source": [
    "* [**`libhwp`**](https://blog.hanlee.io/2022/hwp-rs)\n",
    "\n",
    "  * `ë¼ì´ë¸ŒëŸ¬ë¦¬ RustHwpLoader`ë¥¼ ì‚¬ìš© â†’ **`HWP` íŒŒì¼ì„ ë¡œë“œí•˜ëŠ” `Python í´ë˜ìŠ¤`**\n",
    "\n",
    "  * **`ëª¨ë“  OS`ì—ì„œ ì‘ë™**\n",
    "\n",
    "<br>\n",
    " \n",
    "* **`Document`**\n",
    "  \n",
    "  * `HWP` íŒŒì¼ì—ì„œ **`ëª¨ë“  ë¬¸ë‹¨`ê³¼ `í‘œ`ë¥¼ ì¶”ì¶œ â†’ `ê°ì²´ ëª©ë¡`ìœ¼ë¡œ ë°˜í™˜** \n",
    "  \n",
    "  * ê° `Document ê°ì²´`ëŠ” `ë¬¸ë‹¨` ë˜ëŠ” `í‘œì˜ ë‚´ìš©`ê³¼ **`ê´€ë ¨ ë©”íƒ€ë°ì´í„°`** ë¥¼ í¬í•¨\n",
    "\n",
    "    * ì²« ë²ˆì§¸ ê°ì²´ëŠ” `Document`ê° í‘œì˜ `í…ìŠ¤íŠ¸`ë¥¼ í¬í•¨í•˜ì—¬ `HWP` íŒŒì¼ì˜ `ëª¨ë“  ë¬¸ë‹¨`ì„ í¬í•¨\n",
    "    * ë‘ ë²ˆì§¸ ê°ì²´ëŠ” `Documents`ê° í‘œì˜ `ë¬¸ë‹¨`\n",
    "\n",
    "  * ì•ˆíƒ€ê¹ê²Œë„ ì´ ë¡œë”ëŠ” **í‘œì˜ í–‰ê³¼ ì—´ì„ êµ¬ë¶„í•˜ì§€ ì•ŠìŒ**\n",
    "\n",
    "  * ê° **`ì†ì„± metadata`**: **`DocumentíŒŒì¼ ê²½ë¡œ`(`ì†ŒìŠ¤` í‚¤ ì•„ë˜)**, **`í˜ì´ì§€ ìœ í˜•`(`í…ìŠ¤íŠ¸` ë˜ëŠ” `í…Œì´ë¸”`)** í¬í•¨\n",
    "\n",
    "  * ë‹¤ë¥¸ `HWP ë¡œë”ê°€ ë” ë›°ì–´ë‚œ ê¸°ëŠ¥ì„ ì œê³µ`í•˜ì§€ë§Œ, **`RustHwpLoaderì™¸ë¶€ HWP ë¡œë” ì„œë²„`** or **`Windows ì „ìš© HWP í”„ë¡œê·¸ë¨ì´ í•„ìš”í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— MacOS ë° Linux ì‚¬ìš©ìì—ê²Œ ì¢‹ì€ ì˜µì…˜`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8966a06",
   "metadata": {},
   "source": [
    "#### **`ìš©ë²•`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0e765f",
   "metadata": {},
   "source": [
    "* **`ì´ˆê¸°í™”`**\n",
    "\n",
    "  * a. **`HWP`** íŒŒì¼ ê²½ë¡œ ì´ˆê¸°í™”\n",
    "  \n",
    "  ```python\n",
    "  \n",
    "    loader = RustHwpLoader(\"/path/to/hwp/file\")\n",
    "\n",
    "  ```\n",
    "\n",
    "  * b. **`libhwp`** ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì€ ê²½ìš°: `pip` ì‚¬ìš©í•´ì„œ `Import Error` ì„¤ì¹˜í•˜ë ¤ëŠ” ë©”ì‹œì§€ì™€ í•¨ê»˜ ì˜¤ë¥˜ ë°œìƒ\n",
    "\n",
    "  ```python\n",
    "\n",
    "    pip install libhwp              # í„°ë¯¸ë„ì— ë¨¼ì € ì„¤ì¹˜\n",
    "    pip install ragchain\n",
    "\n",
    "  ```\n",
    "\n",
    "<br>\n",
    "\n",
    "* **`ë¬¸ì„œ ë¡œë”©`**: `HWP`íŒŒì¼ì—ì„œ ê°ì²´ë¥¼ `RustHWpLoader`ë¡œë“œí•˜ëŠ” ë‘ ê°€ì§€ ë°©ë²•ì„ ì œê³µ\n",
    "\n",
    "  * a. **`load()`**: **`Documents`** â†’ ëª¨ë“  í•­ëª©ì˜ ëª©ë¡ì„ ë°˜í™˜\n",
    "  \n",
    "  ```python\n",
    "\n",
    "    documents = loader.load()\n",
    "  \n",
    "  ```\n",
    "\n",
    "  * b. **`lazy_load()`**: \n",
    "  \n",
    "  ```python\n",
    "\n",
    "    for document in loader.lazy_load():          # process document\n",
    "  \n",
    "  ```\n",
    "    * **`Documents`** â†’ ê°ì²´ë¥¼ ëŠë¦¬ê²Œ ìƒì„±í•˜ëŠ” ìƒì„±ê¸°\n",
    "    * ëª©ë¡ì— ëª¨ë‘ ë¡œë“œí•˜ë©´ **ë§ì€ ë©”ëª¨ë¦¬ë¥¼ ì†Œëª¨í•  ìˆ˜ ìˆëŠ” `ëŒ€ìš©ëŸ‰ HWP íŒŒì¼`ë¡œ ì‘ì—…í•  ë•Œ ìœ ìš©**\n",
    "\n",
    "<br>\n",
    "\n",
    "* ê°ê°ì€ ë¬¸ìì—´ê³¼ ì‚¬ì „ `Document`ì„ í¬í•¨ `load` or `lazy_load` ìƒì„±\n",
    "  * ì—¬ê¸°ì—ëŠ” **`source` í‚¤(íŒŒì¼ ê²½ë¡œ)**, **`page_type` í‚¤(`text` ë˜ëŠ” `table`)ê°€ í¬í•¨**\n",
    "  * `page_content`, `metadata`, `metadata`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a458d18b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cc747c",
   "metadata": {},
   "source": [
    "### 2) **`ê°œë°œí™˜ê²½ ì¶©ëŒ`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5247cec",
   "metadata": {},
   "source": [
    "* ì‚¬ìš©í•˜ëŠ” í™˜ê²½ **`Python` = 3.13.**\n",
    "\n",
    "* ì°¸ê³ í•˜ëŠ” íŒ¨í‚¤ì§€ ë° ì´ë¯¸ ë§Œë“¤ì–´ì§„ ë¡œë”ë“¤(`libhwp`, `pyhwp`, `llama-index-hwp` ë“±)ì´ ëŒ€ë¶€ë¶„ **`Python` = 3.11ê¹Œì§€ ì§€ì›**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0665198e",
   "metadata": {},
   "source": [
    "#### **â€ TeddyNoteì—ì„œ HWPLoaderì˜ í•µì‹¬ ê¸°ëŠ¥ë§Œ ì¶”ì¶œí•´ë³´ê¸°**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d0d956",
   "metadata": {},
   "source": [
    "* `custom_hwp_loader.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e079e1c",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* `custom_hwp_loader.py` ì½”ë“œ\n",
    "\n",
    "    ```python\n",
    "\n",
    "    import os\n",
    "    from typing import Iterator, List\n",
    "    from langchain_core.documents import Document\n",
    "    from langchain_core.document_loaders.base import BaseLoader\n",
    "\n",
    "    class CustomHWPLoader(BaseLoader):\n",
    "        \"\"\"ê°œì„ ëœ HWP ë¡œë” - Python 3.13 í˜¸í™˜\"\"\"\n",
    "        \n",
    "        def __init__(self, file_path: str):\n",
    "            self.file_path = file_path\n",
    "        \n",
    "        def lazy_load(self) -> Iterator[Document]:\n",
    "            \"\"\"HWP íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ Documentë¡œ ë³€í™˜\"\"\"\n",
    "            try:\n",
    "                # ë°©ë²• 1: olefileë¡œ ì‹œë„\n",
    "                text_content = self._extract_with_olefile()\n",
    "                \n",
    "                if not text_content or \"ì¶”ì¶œ ì‹¤íŒ¨\" in text_content:\n",
    "                    # ë°©ë²• 2: zipfileë¡œ ì‹œë„ (HWP 5.0+ëŠ” ZIP ê¸°ë°˜)\n",
    "                    text_content = self._extract_with_zipfile()\n",
    "                \n",
    "                if not text_content or \"ì¶”ì¶œ ì‹¤íŒ¨\" in text_content:\n",
    "                    # ë°©ë²• 3: ë°”ì´ë„ˆë¦¬ ì§ì ‘ ì½ê¸°\n",
    "                    text_content = self._extract_with_binary()\n",
    "                    \n",
    "                metadata = {\n",
    "                    \"source\": self.file_path,\n",
    "                    \"file_type\": \"hwp\",\n",
    "                    \"extraction_method\": \"custom_parser\"\n",
    "                }\n",
    "                \n",
    "                yield Document(\n",
    "                    page_content=text_content,\n",
    "                    metadata=metadata\n",
    "                )\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"ì „ì²´ HWP ë¡œë”© ì˜¤ë¥˜: {e}\")\n",
    "                yield Document(\n",
    "                    page_content=f\"HWP íŒŒì¼ ë¡œë”© ì‹¤íŒ¨: {e}\",\n",
    "                    metadata={\"source\": self.file_path, \"error\": str(e)}\n",
    "                )\n",
    "        \n",
    "        def _extract_with_olefile(self) -> str:\n",
    "            \"\"\"olefileì„ ì‚¬ìš©í•œ ì¶”ì¶œ\"\"\"\n",
    "            try:\n",
    "                import olefile\n",
    "                \n",
    "                if not olefile.isOleFile(self.file_path):\n",
    "                    return \"olefile: HWP í˜•ì‹ì´ ì•„ë‹˜\"\n",
    "                \n",
    "                with olefile.OleFileIO(self.file_path) as ole:\n",
    "                    sections = []\n",
    "                    \n",
    "                    # ëª¨ë“  ìŠ¤íŠ¸ë¦¼ íƒìƒ‰\n",
    "                    for stream_name in ole.listdir():\n",
    "                        stream_path = '/'.join(stream_name)\n",
    "                        \n",
    "                        # BodyText ê´€ë ¨ ìŠ¤íŠ¸ë¦¼ ì°¾ê¸°\n",
    "                        if 'BodyText' in stream_path or 'bodytext' in stream_path.lower():\n",
    "                            try:\n",
    "                                data = ole.open(stream_name).read()\n",
    "                                \n",
    "                                # ì—¬ëŸ¬ ì¸ì½”ë”© ì‹œë„\n",
    "                                for encoding in ['utf-16le', 'utf-16', 'cp949', 'euc-kr', 'utf-8']:\n",
    "                                    try:\n",
    "                                        text = data.decode(encoding, errors='ignore')\n",
    "                                        if text and len(text.strip()) > 10:  # ì˜ë¯¸ìˆëŠ” í…ìŠ¤íŠ¸ì¸ì§€ í™•ì¸\n",
    "                                            sections.append(f\"[{encoding}] {text[:500]}\")\n",
    "                                            break\n",
    "                                    except:\n",
    "                                        continue\n",
    "                                        \n",
    "                            except Exception as e:\n",
    "                                print(f\"ìŠ¤íŠ¸ë¦¼ ì½ê¸° ì˜¤ë¥˜ {stream_path}: {e}\")\n",
    "                                continue\n",
    "                    \n",
    "                    result = '\\n'.join(sections) if sections else \"olefile: í…ìŠ¤íŠ¸ ì„¹ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ\"\n",
    "                    return result\n",
    "                    \n",
    "            except ImportError:\n",
    "                return \"olefile ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ\"\n",
    "            except Exception as e:\n",
    "                return f\"olefile ì¶”ì¶œ ì‹¤íŒ¨: {e}\"\n",
    "        \n",
    "        def _extract_with_zipfile(self) -> str:\n",
    "            \"\"\"zipfileì„ ì‚¬ìš©í•œ ì¶”ì¶œ (HWP 5.0+)\"\"\"\n",
    "            try:\n",
    "                import zipfile\n",
    "                import xml.etree.ElementTree as ET\n",
    "                \n",
    "                with zipfile.ZipFile(self.file_path, 'r') as hwp_zip:\n",
    "                    sections = []\n",
    "                    \n",
    "                    # ZIP ë‚´ë¶€ íŒŒì¼ ëª©ë¡ í™•ì¸\n",
    "                    file_list = hwp_zip.namelist()\n",
    "                    print(f\"ZIP ë‚´ë¶€ íŒŒì¼ë“¤: {file_list}\")\n",
    "                    \n",
    "                    # XML íŒŒì¼ë“¤ í™•ì¸\n",
    "                    for file_name in file_list:\n",
    "                        if file_name.endswith('.xml') or 'content' in file_name.lower():\n",
    "                            try:\n",
    "                                content = hwp_zip.read(file_name)\n",
    "                                \n",
    "                                # XML íŒŒì‹± ì‹œë„\n",
    "                                try:\n",
    "                                    root = ET.fromstring(content)\n",
    "                                    # í…ìŠ¤íŠ¸ ë…¸ë“œ ì¶”ì¶œ\n",
    "                                    for elem in root.iter():\n",
    "                                        if elem.text and elem.text.strip():\n",
    "                                            sections.append(elem.text.strip())\n",
    "                                except:\n",
    "                                    # XMLì´ ì•„ë‹ˆë©´ ì§ì ‘ í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "                                    for encoding in ['utf-8', 'cp949', 'euc-kr']:\n",
    "                                        try:\n",
    "                                            text = content.decode(encoding, errors='ignore')\n",
    "                                            if text and len(text.strip()) > 10:\n",
    "                                                sections.append(f\"[{file_name}] {text[:300]}\")\n",
    "                                                break\n",
    "                                        except:\n",
    "                                            continue\n",
    "                                            \n",
    "                            except Exception as e:\n",
    "                                print(f\"ZIP íŒŒì¼ ì½ê¸° ì˜¤ë¥˜ {file_name}: {e}\")\n",
    "                                continue\n",
    "                    \n",
    "                    return '\\n'.join(sections) if sections else \"zipfile: í…ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ\"\n",
    "                    \n",
    "            except zipfile.BadZipFile:\n",
    "                return \"zipfile: ZIP í˜•ì‹ì´ ì•„ë‹˜\"\n",
    "            except Exception as e:\n",
    "                return f\"zipfile ì¶”ì¶œ ì‹¤íŒ¨: {e}\"\n",
    "        \n",
    "        def _extract_with_binary(self) -> str:\n",
    "            \"\"\"ë°”ì´ë„ˆë¦¬ ì§ì ‘ ì½ê¸°ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ\"\"\"\n",
    "            try:\n",
    "                with open(self.file_path, 'rb') as f:\n",
    "                    data = f.read()\n",
    "                \n",
    "                # í•œê¸€ ë¬¸ìì—´ íŒ¨í„´ ì°¾ê¸°\n",
    "                text_parts = []\n",
    "                \n",
    "                # ì—¬ëŸ¬ ì¸ì½”ë”©ìœ¼ë¡œ ì‹œë„\n",
    "                for encoding in ['utf-16le', 'cp949', 'euc-kr', 'utf-8']:\n",
    "                    try:\n",
    "                        decoded = data.decode(encoding, errors='ignore')\n",
    "                        \n",
    "                        # í•œê¸€ì´ í¬í•¨ëœ ë¬¸ì¥ ì°¾ê¸°\n",
    "                        import re\n",
    "                        korean_sentences = re.findall(r'[ê°€-í£\\s]{10,}', decoded)\n",
    "                        \n",
    "                        if korean_sentences:\n",
    "                            text_parts.extend([f\"[{encoding}] {sent}\" for sent in korean_sentences[:5]])\n",
    "                            \n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                if text_parts:\n",
    "                    return '\\n'.join(text_parts)\n",
    "                else:\n",
    "                    return f\"ë°”ì´ë„ˆë¦¬ ì¶”ì¶œ: í•œê¸€ í…ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ (íŒŒì¼ í¬ê¸°: {len(data)} bytes)\"\n",
    "                    \n",
    "            except Exception as e:\n",
    "                return f\"ë°”ì´ë„ˆë¦¬ ì¶”ì¶œ ì‹¤íŒ¨: {e}\"\n",
    "\n",
    "    if __name__ == \"__main__\":\n",
    "        # ì ˆëŒ€ ê²½ë¡œë¡œ ìˆ˜ì •\n",
    "        import os\n",
    "        \n",
    "        # í˜„ì¬ íŒŒì¼ì˜ ë””ë ‰í† ë¦¬ ê¸°ì¤€ìœ¼ë¡œ ê²½ë¡œ ì„¤ì •\n",
    "        current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "        hwp_file_path = os.path.join(current_dir, \"data\", \"ë””ì§€í„¸ ì •ë¶€í˜ì‹  ì¶”ì§„ê³„íš.hwp\")\n",
    "        \n",
    "        print(f\"íŒŒì¼ ê²½ë¡œ: {hwp_file_path}\")\n",
    "        print(f\"íŒŒì¼ ì¡´ì¬ ì—¬ë¶€: {os.path.exists(hwp_file_path)}\")\n",
    "        \n",
    "        if os.path.exists(hwp_file_path):\n",
    "            # olefile ì„¤ì¹˜ í™•ì¸\n",
    "            try:\n",
    "                import olefile\n",
    "                print(\"âœ… olefile ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© ê°€ëŠ¥\")\n",
    "            except ImportError:\n",
    "                print(\"âš ï¸  olefile ì„¤ì¹˜ í•„ìš”: pip install olefile\")\n",
    "            \n",
    "            loader = CustomHWPLoader(hwp_file_path)\n",
    "            docs = loader.load()\n",
    "            \n",
    "            print(f\"\\nğŸ‰ ì„±ê³µ! ë¡œë“œëœ ë¬¸ì„œ: {len(docs)}\")\n",
    "            print(f\"ğŸ“„ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°:\")\n",
    "            print(\"-\" * 50)\n",
    "            print(docs[0].page_content[:1500])  # ë” ë§ì´ ì¶œë ¥\n",
    "            print(\"-\" * 50)\n",
    "            print(f\"ğŸ“‹ ë©”íƒ€ë°ì´í„°: {docs[0].metadata}\")\n",
    "        else:\n",
    "            print(\"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!\")\n",
    "\n",
    "    ```\n",
    "<br>\n",
    "\n",
    "* ì¶œë ¥ ê²°ê³¼\n",
    "\n",
    "```bash\n",
    "        HWP ë¡œë”© ì˜¤ë¥˜: [Errno 2] No such file or directory: '../../../06_Document_Loader/data/ë””ì§€í„¸ ì •ë¶€í˜ì‹  ì¶”ì§„ê³„íš.hwp'\n",
    "        ì„±ê³µ! ë¡œë“œëœ ë¬¸ì„œ: 1\n",
    "        ë‚´ìš©: \n",
    "        {'source': '../../../06_Document_Loader/data/ë””ì§€í„¸ ì •ë¶€í˜ì‹  ì¶”ì§„ê³„íš.hwp', 'error': \"[Errno 2] No such file or directory: '../../../06_Document_Loader/data/ë””ì§€í„¸ ì •ë¶€í˜ì‹  ì¶”ì§„ê³„íš.hwp'\"}\n",
    "        HWP ë¡œë”© ì˜¤ë¥˜: [Errno 2] No such file or directory: '../../../06_Document_Loader/data/ë””ì§€í„¸ ì •ë¶€í˜ì‹  ì¶”ì§„ê³„íš.hwp'\n",
    "        ì„±ê³µ! ë¡œë“œëœ ë¬¸ì„œ: 1\n",
    "        ë‚´ìš©: \n",
    "        {'source': '../../../06_Document_Loader/data/ë””ì§€í„¸ ì •ë¶€í˜ì‹  ì¶”ì§„ê³„íš.hwp', 'error': \"[Errno 2] No such file or directory: '../../../06_Document_Loader/data/ë””ì§€í„¸ ì •ë¶€í˜ì‹  ì¶”ì§„ê³„íš.hwp'\"}\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "  * ì˜¬ë°”ë¥´ì§€ ì•Šì€ ê²½ë¡œ ì‹¤í–‰\n",
    "\n",
    "    * 1_í„°ë¯¸ë„: ì˜¬ë°”ë¥¸ ê²½ë¡œì—ì„œ ì‹¤í–‰ â†’ ë¬¸ì œ ê·¸ëŒ€ë¡œ\n",
    "\n",
    "    * 2_`Jupyter notebook`ì—ì„œ í…ŒìŠ¤íŠ¸ (ì•„ë˜ ì…€ ì°¸ê³ ) â†’ ë¬¸ì œ ê·¸ëŒ€ë¡œ \n",
    "\n",
    "  * ë¡œì§ ë‹¤ì‹œ ìˆ˜ì • ê²°ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6487ff85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì˜¬ë°”ë¥¸ ê²½ë¡œì—ì„œ í™•ì¸í•˜ê¸° - ë‘ë²ˆì§¸ ë°©ë²•\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# í˜„ì¬ ê²½ë¡œ í™•ì¸\n",
    "print(f\"í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: {os.getcwd()}\")\n",
    "\n",
    "# íŒŒì¼ ê²½ë¡œë“¤ í™•ì¸\n",
    "possible_paths = [\n",
    "    \"./data/ë””ì§€í„¸ ì •ë¶€í˜ì‹  ì¶”ì§„ê³„íš.hwp\",\n",
    "    \"../06_Document_Loader/data/ë””ì§€í„¸ ì •ë¶€í˜ì‹  ì¶”ì§„ê³„íš.hwp\",\n",
    "    \"/Users/jay/Projects/20250727-langchain-note/06_Document_Loader/data/ë””ì§€í„¸ ì •ë¶€í˜ì‹  ì¶”ì§„ê³„íš.hwp\"\n",
    "]\n",
    "\n",
    "for path in possible_paths:\n",
    "    if os.path.exists(path):\n",
    "        print(f\"âœ… íŒŒì¼ ë°œê²¬: {path}\")\n",
    "        hwp_path = path\n",
    "        break\n",
    "else:\n",
    "    print(\"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!\")\n",
    "    \n",
    "# data í´ë” ë‚´ìš© í™•ì¸\n",
    "data_dir = \"./data\"\n",
    "if os.path.exists(data_dir):\n",
    "    print(f\"\\nğŸ“ {data_dir} í´ë” ë‚´ìš©:\")\n",
    "    for file in os.listdir(data_dir):\n",
    "        print(f\"  - {file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860455d1",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* ì…€ ì¶œë ¥\n",
    "\n",
    "  ```markdown\n",
    "  í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: /Users/jay/Projects/20250727-langchain-note/06_Document_Loader\n",
    "  âœ… íŒŒì¼ ë°œê²¬: ./data/ë””ì§€í„¸ ì •ë¶€í˜ì‹  ì¶”ì§„ê³„íš.hwp\n",
    "\n",
    "  ğŸ“ ./data í´ë” ë‚´ìš©:\n",
    "    - á„ƒá…µá„Œá…µá„á…¥á†¯ á„Œá…¥á†¼á„‡á…®á„’á…§á†¨á„‰á…µá†« á„á…®á„Œá…µá†«á„€á…¨á„’á…¬á†¨.hwp\n",
    "    - client.html\n",
    "    - appendix-keywords-CP949.txt\n",
    "    - reference.txt\n",
    "    - á„ƒá…µá„Œá…µá„á…¥á†¯_á„Œá…¥á†¼á„‡á…®á„’á…§á†¨á„‰á…µá†«_á„á…®á„Œá…µá†«á„€á…¨á„’á…¬á†¨.pdf\n",
    "    - appendix-keywords-EUCKR.txt\n",
    "    - SPRI_AI_Brief_2023á„‚á…§á†«12á„‹á…¯á†¯á„’á…©_F.pdf\n",
    "    - audio_utils.py\n",
    "    - chain-of-density.txt\n",
    "    - titanic.csv\n",
    "    - sample-word-document.docx\n",
    "    - titanic.xlsx\n",
    "    - sample-ppt.pptx\n",
    "    - appendix-keywords.txt\n",
    "    - people.json\n",
    "    - appendix-keywords-utf8.txt\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c613471",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914be50a",
   "metadata": {},
   "source": [
    "* **`custom_hwp_loader2.py`** \n",
    "\n",
    "  * ë¡œì§ ìˆ˜ì •í•˜ê¸° \n",
    "\n",
    "  * [**`í•œì»´ì˜¤í”¼ìŠ¤`**](https://tech.hancom.com/python-hwp-parsing-1/)ì—ì„œ ê³µì‹ì ìœ¼ë¡œ ë°œí‘œí•œ `HWP` íŒŒì‹± ë°©ë²• ì ìš©í•˜ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01415062",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* `custom_hwp_loader2.py`ì˜ ì½”ë“œ\n",
    "\n",
    "    ```python\n",
    "    # 06_Document_Loader/custom_hwp_loader.py - í•œì»´ ê³µì‹ ë°©ë²• ì ìš©\n",
    "    import os\n",
    "    import olefile\n",
    "    import zlib\n",
    "    import struct\n",
    "    from typing import Iterator, List\n",
    "    from langchain_core.documents import Document\n",
    "    from langchain_core.document_loaders.base import BaseLoader\n",
    "\n",
    "    class CustomHWPLoader(BaseLoader):\n",
    "        \"\"\"í•œì»´ ê³µì‹ ë°©ë²• ê¸°ë°˜ HWP ë¡œë”\"\"\"\n",
    "        \n",
    "        def __init__(self, file_path: str):\n",
    "            self.file_path = file_path\n",
    "        \n",
    "        def lazy_load(self) -> Iterator[Document]:\n",
    "            \"\"\"HWP íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ Documentë¡œ ë³€í™˜\"\"\"\n",
    "            try:\n",
    "                text_content = self._extract_hwp_text()\n",
    "                \n",
    "                metadata = {\n",
    "                    \"source\": self.file_path,\n",
    "                    \"file_type\": \"hwp\",\n",
    "                    \"extraction_method\": \"hancom_official\"\n",
    "                }\n",
    "                \n",
    "                yield Document(\n",
    "                    page_content=text_content,\n",
    "                    metadata=metadata\n",
    "                )\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"HWP ë¡œë”© ì˜¤ë¥˜: {e}\")\n",
    "                yield Document(\n",
    "                    page_content=f\"HWP íŒŒì¼ ë¡œë”© ì‹¤íŒ¨: {e}\",\n",
    "                    metadata={\"source\": self.file_path, \"error\": str(e)}\n",
    "                )\n",
    "        \n",
    "        def _extract_hwp_text(self) -> str:\n",
    "            \"\"\"í•œì»´ ê³µì‹ ë°©ë²•ìœ¼ë¡œ HWP í…ìŠ¤íŠ¸ ì¶”ì¶œ\"\"\"\n",
    "            try:\n",
    "                f = olefile.OleFileIO(self.file_path)\n",
    "                dirs = f.listdir()\n",
    "                \n",
    "                print(f\"ğŸ“ HWP ë‚´ë¶€ êµ¬ì¡°:\")\n",
    "                for d in dirs:\n",
    "                    print(f\"  - {'/'.join(d) if isinstance(d, list) else d}\")\n",
    "                \n",
    "                # HWP íŒŒì¼ ê²€ì¦\n",
    "                if [\"FileHeader\"] not in dirs or [\"\\\\x05HwpSummaryInformation\"] not in dirs:\n",
    "                    print(\"âš ï¸  HWP íŒŒì¼ í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤\")\n",
    "                    return self._try_prvtext_method(f)\n",
    "                \n",
    "                # ë¬¸ì„œ í¬ë§· ì••ì¶• ì—¬ë¶€ í™•ì¸\n",
    "                header = f.openstream(\"FileHeader\")\n",
    "                header_data = header.read()\n",
    "                is_compressed = (header_data & 1) == 1\n",
    "                print(f\"ğŸ—œï¸  ì••ì¶• ì—¬ë¶€: {is_compressed}\")\n",
    "                \n",
    "                # Body Sections ì°¾ê¸°\n",
    "                nums = []\n",
    "                for d in dirs:\n",
    "                    if isinstance(d, list) and len(d) >= 2 and d == \"BodyText\":\n",
    "                        try:\n",
    "                            section_num = int(d[21][len(\"Section\"):])\n",
    "                            nums.append(section_num)\n",
    "                        except:\n",
    "                            continue\n",
    "                \n",
    "                if not nums:\n",
    "                    print(\"ğŸ“„ BodyText ì„¹ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ, PrvText ë°©ë²• ì‹œë„\")\n",
    "                    return self._try_prvtext_method(f)\n",
    "                    \n",
    "                sections = [f\"BodyText/Section{x}\" for x in sorted(nums)]\n",
    "                print(f\"ğŸ“‘ ë°œê²¬ëœ ì„¹ì…˜: {sections}\")\n",
    "                \n",
    "                # ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "                text = \"\"\n",
    "                for section in sections:\n",
    "                    try:\n",
    "                        print(f\"ğŸ” ì„¹ì…˜ ì²˜ë¦¬ ì¤‘: {section}\")\n",
    "                        bodytext = f.openstream(section)\n",
    "                        data = bodytext.read()\n",
    "                        \n",
    "                        if is_compressed:\n",
    "                            unpacked_data = zlib.decompress(data, -15)\n",
    "                        else:\n",
    "                            unpacked_data = data\n",
    "                        \n",
    "                        # ê° ì„¹ì…˜ ë‚´ í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "                        section_text = self._extract_section_text(unpacked_data)\n",
    "                        text += section_text + \"\\n\"\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"âŒ ì„¹ì…˜ {section} ì²˜ë¦¬ ì˜¤ë¥˜: {e}\")\n",
    "                        continue\n",
    "                \n",
    "                return text if text.strip() else self._try_prvtext_method(f)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ HWP ì¶”ì¶œ ì˜¤ë¥˜: {e}\")\n",
    "                return f\"HWP ì¶”ì¶œ ì‹¤íŒ¨: {e}\"\n",
    "            finally:\n",
    "                if 'f' in locals():\n",
    "                    f.close()\n",
    "        \n",
    "        def _extract_section_text(self, unpacked_data: bytes) -> str:\n",
    "            \"\"\"ì„¹ì…˜ ë°ì´í„°ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ\"\"\"\n",
    "            section_text = \"\"\n",
    "            i = 0\n",
    "            size = len(unpacked_data)\n",
    "            \n",
    "            try:\n",
    "                while i < size:\n",
    "                    if i + 4 > size:\n",
    "                        break\n",
    "                        \n",
    "                    header = struct.unpack_from(\"<I\", unpacked_data, i)\n",
    "                    rec_type = header & 0x3ff\n",
    "                    rec_len = (header >> 20) & 0xfff\n",
    "                    \n",
    "                    # í…ìŠ¤íŠ¸ ë ˆì½”ë“œ íƒ€ì… (67ë²ˆ)\n",
    "                    #if rec_type in :\n",
    "                    if rec_type == 67:\n",
    "                        if i + 4 + rec_len <= size:\n",
    "                            rec_data = unpacked_data[i + 4:i + 4 + rec_len]\n",
    "                            try:\n",
    "                                text = rec_data.decode('utf-16le')\n",
    "                                section_text += text + \"\\n\"\n",
    "                            except:\n",
    "                                try:\n",
    "                                    text = rec_data.decode('utf-16')\n",
    "                                    section_text += text + \"\\n\"\n",
    "                                except:\n",
    "                                    pass\n",
    "                    \n",
    "                    i += 4 + rec_len\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸  ì„¹ì…˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "            \n",
    "            return section_text\n",
    "        \n",
    "        def _try_prvtext_method(self, f) -> str:\n",
    "            \"\"\"PrvText ë°©ë²•ìœ¼ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ë°±ì—… ë°©ë²•)\"\"\"\n",
    "            try:\n",
    "                print(\"ğŸ”„ PrvText ë°©ë²•ìœ¼ë¡œ ì‹œë„ ì¤‘...\")\n",
    "                encoded_text = f.openstream('PrvText').read()\n",
    "                decoded_text = encoded_text.decode('UTF-16le')\n",
    "                print(\"âœ… PrvText ë°©ë²• ì„±ê³µ!\")\n",
    "                return decoded_text\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ PrvText ë°©ë²•ë„ ì‹¤íŒ¨: {e}\")\n",
    "                return f\"ëª¨ë“  ì¶”ì¶œ ë°©ë²• ì‹¤íŒ¨: PrvText ì˜¤ë¥˜ - {e}\"\n",
    "\n",
    "    if __name__ == \"__main__\":\n",
    "        import os\n",
    "        \n",
    "        # í˜„ì¬ íŒŒì¼ì˜ ë””ë ‰í† ë¦¬ ê¸°ì¤€ìœ¼ë¡œ ê²½ë¡œ ì„¤ì •\n",
    "        current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "        hwp_file_path = os.path.join(current_dir, \"data\", \"ë””ì§€í„¸ ì •ë¶€í˜ì‹  ì¶”ì§„ê³„íš.hwp\")\n",
    "        \n",
    "        print(f\"ğŸ“ íŒŒì¼ ê²½ë¡œ: {hwp_file_path}\")\n",
    "        print(f\"âœ… íŒŒì¼ ì¡´ì¬ ì—¬ë¶€: {os.path.exists(hwp_file_path)}\")\n",
    "        \n",
    "        if os.path.exists(hwp_file_path):\n",
    "            print(\"\\nğŸš€ HWP íŒŒì‹± ì‹œì‘...\")\n",
    "            loader = CustomHWPLoader(hwp_file_path)\n",
    "            docs = loader.load()  # ì´ê±´ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•¨\n",
    "            \n",
    "            print(f\"\\nğŸ‰ ì„±ê³µ! ë¡œë“œëœ ë¬¸ì„œ: {len(docs)}\")\n",
    "            \n",
    "            if docs:  # ë¬¸ì„œê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸\n",
    "                first_doc = docs[0]  # ì²« ë²ˆì§¸ ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°\n",
    "                print(f\"ğŸ“„ ë‚´ìš© ê¸¸ì´: {len(first_doc.page_content)} ë¬¸ì\")\n",
    "                print(f\"\\nğŸ“‹ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°:\")\n",
    "                print(\"=\" * 60)\n",
    "                print(first_doc.page_content[:2000])  # 2000ìê¹Œì§€ ì¶œë ¥\n",
    "                print(\"=\" * 60)\n",
    "                print(f\"\\nğŸ“Š ë©”íƒ€ë°ì´í„°: {first_doc.metadata}\")\n",
    "            else:\n",
    "                print(\"âŒ ë¬¸ì„œë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "        else:\n",
    "            print(\"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!\")\n",
    "    ```\n",
    "\n",
    "<br>\n",
    "\n",
    "* í„°ë¯¸ë„ ì‹¤í–‰ ê²°ê³¼\n",
    "\n",
    "```bash\n",
    "    ğŸ“ íŒŒì¼ ê²½ë¡œ: /Users/jay/Projects/20250727-langchain-note/06_Document_Loader/data/ë””ì§€í„¸ ì •ë¶€í˜ì‹  ì¶”ì§„ê³„íš.hwp\n",
    "    âœ… íŒŒì¼ ì¡´ì¬ ì—¬ë¶€: True\n",
    "\n",
    "    ğŸš€ HWP íŒŒì‹± ì‹œì‘...\n",
    "    ğŸ“ HWP ë‚´ë¶€ êµ¬ì¡°:\n",
    "    - HwpSummaryInformation\n",
    "    - BinData/BIN0001.jpg\n",
    "    - BinData/BIN0002.bmp\n",
    "    - BinData/BIN0003.bmp\n",
    "    - BinData/BIN0004.bmp\n",
    "    - BinData/BIN0005.bmp\n",
    "    - BinData/BIN0006.bmp\n",
    "    - BinData/BIN0007.bmp\n",
    "    - BinData/BIN0008.bmp\n",
    "    - BodyText/Section0\n",
    "    - DocInfo\n",
    "    - DocOptions/_LinkDoc\n",
    "    - FileHeader\n",
    "    - PrvImage\n",
    "    - PrvText\n",
    "    - Scripts/DefaultJScript\n",
    "    - Scripts/JScriptVersion\n",
    "    âš ï¸  HWP íŒŒì¼ í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤\n",
    "    ğŸ”„ PrvText ë°©ë²•ìœ¼ë¡œ ì‹œë„ ì¤‘...\n",
    "    âœ… PrvText ë°©ë²• ì„±ê³µ!\n",
    "\n",
    "    ğŸ‰ ì„±ê³µ! ë¡œë“œëœ ë¬¸ì„œ: 1\n",
    "    ğŸ“„ ë‚´ìš© ê¸¸ì´: 1022 ë¬¸ì\n",
    "\n",
    "    ğŸ“‹ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°:\n",
    "    ============================================================\n",
    "\n",
    "    <ë””ì§€í„¸ ì •ë¶€í˜ì‹  ì¶”ì§„ê³„íš>\n",
    "\n",
    "    2019. 10. 29.\n",
    "\n",
    "        <><ê´€ê³„ë¶€ì²˜ í•©ë™>\n",
    "\n",
    "    <><ìˆœ    ì„œ><><â… . ê°œìš”  1 â…¡. ë””ì§€í„¸ ì •ë¶€í˜ì‹  ì¶”ì§„ê³„íš  2   1. ìš°ì„  ì¶”ì§„ê³¼ì œ  2      â‘  ì„ ì œì Â·í†µí•©ì  ëŒ€êµ­ë¯¼ ì„œë¹„ìŠ¤ í˜ì‹       â‘¡ ê³µê³µë¶€ë¬¸ ë§ˆì´ë°ì´í„° í™œì„±í™”      â‘¢ ì‹œë¯¼ì°¸ì—¬ë¥¼ ìœ„í•œ í”Œë«í¼ ê³ ë„í™”      â‘£ í˜„ì¥ì¤‘ì‹¬ í˜‘ì—…ì„ ì§€ì›í•˜ëŠ” ìŠ¤ë§ˆíŠ¸ ì—…ë¬´í™˜ê²½ êµ¬í˜„      â‘¤ í´ë¼ìš°ë“œì™€ ë””ì§€í„¸ì„œë¹„ìŠ¤ ì´ìš© í™œì„±í™”      â‘¥ ê°œë°©í˜• ë°ì´í„°Â·ì„œë¹„ìŠ¤ ìƒíƒœê³„ êµ¬ì¶•   2. ì¤‘ì¥ê¸° ë²”ì •ë¶€ ë””ì§€í„¸ ì „í™˜ ë¡œë“œë§µ ìˆ˜ë¦½  4 â…¢. ì¶”ì§„ì²´ê³„ ë° ì¼ì •  4  [ë¶™ì„] ë””ì§€í„¸ ì •ë¶€í˜ì‹  ìš°ì„  ì¶”ì§„ê³¼ì œ(ìƒì„¸)  8>\n",
    "\n",
    "    <â… . ê°œ ìš”>\n",
    "    â–¡ ì¶”ì§„ ë°°ê²½\n",
    "    â—‹ ìš°ë¦¬ë‚˜ë¼ëŠ” êµ­ê°€ì  ì´ˆê³ ì† ì •ë³´í†µì‹ ë§ íˆ¬ìì™€ ì ê·¹ì ì¸ ê³µê³µì •ë³´í™” ì‚¬ì—… ì¶”ì§„ì— í˜ì…ì–´ ì„¸ê³„ ìµœê³ ìˆ˜ì¤€ì˜ ì „ìì •ë¶€ë¥¼ êµ¬ì¶•â€§ìš´ì˜\n",
    "        * UNì „ìì •ë¶€í‰ê°€ì—ì„œ 2010â€§12â€§14ë…„ 1ìœ„, 16â€§18ë…„ 3ìœ„, UNê³µê³µí–‰ì •ìƒ 13íšŒ ìˆ˜ìƒ\n",
    "    â—‹ ê·¸ëŸ¬ë‚˜, ì¸ê³µì§€ëŠ¥â€§í´ë¼ìš°ë“œ ì¤‘ì‹¬ì˜ ë””ì§€í„¸ ì „í™˜(Digital Transformation) ì‹œëŒ€ê°€ ë„ë˜í•¨ì— ë”°ë¼ ê¸°ì¡´ ì „ìì •ë¶€ì˜ í•œê³„ í‘œì¶œ\n",
    "    - ì¶•ì ëœ í–‰ì •ë°ì´í„°ì—ë„ ë¶ˆêµ¬í•˜ê³  ê¸°ê´€ê°„ ì—°ê³„â€§í™œìš© ë¯¸í¡, ë¶€ì²˜ ë‹¨ìœ„ë¡œ ë‹¨ì ˆëœ ì„œë¹„ìŠ¤, ì‹ ê¸°ìˆ  í™œìš©ì„ ìœ„í•œ ì œë„â€§ê¸°ë°˜ ë¶€ì¡±\n",
    "    - ë””ì§€í„¸ ì „í™˜ì„ ìœ„í•œ ì»¨íŠ¸ë¡¤íƒ€ì›Œê°€ ì—†ê³ , êµ¬ì²´ì  ì „ëµë„ ë¶€ì¬\n",
    "    â—‹ ì´ì—, â€˜19.3ì›”ë¶€í„° ê³µê³µë¶€ë¬¸ ICT í™œìš©í˜„í™© ë° ë¬¸ì œì  ê²€í† ì— ì°©ìˆ˜í•˜ì—¬ ê³µê³µë¶„ì•¼ ë””ì§€í„¸ ì „í™˜ì„ ìœ„í•œ ì¶”ì§„ê³„íš ë§ˆë ¨\n",
    "        * ê´€ê³„ë¶€ì²˜ í˜‘ì˜ 21íšŒ(í–‰ì•ˆ,ê³¼ê¸°ì •í†µ,ê¸°ì¬,ë³µì§€,ê¶Œìµìœ„,êµ­ì •ì› ë“±), ë¯¼ê°„ì „ë¬¸ê°€ ì˜ê²¬ì²­ì·¨ 10íšŒ\n",
    "    â–¡ ë¬¸ì œì  ì§„ë‹¨ ë° í‰ê°€\n",
    "    â—‹ (ì„œë¹„ìŠ¤) êµ­ë¯¼ê³¼ ìµœì¢… ì´ìš©ì ê´€ì ì—ì„œ ì„œë¹„ìŠ¤ í˜ì‹  ë¯¸í¡\n",
    "    - ìê²©ì´ ìˆì–´ë„ ìì‹ ì´ ë°›ì„ ìˆ˜ ìˆëŠ” ê³µê³µì„œë¹„ìŠ¤ë¥¼ íŒŒì•…í•˜ê¸° ì–´ë ¤ì›Œ ì‚¬ê°ì§€ëŒ€ê°€ ë°œìƒí•˜ê³ , ì˜¨ë¼ì¸ ì‹ ì²­ ê°€ëŠ¥í•œ ì„œë¹„ìŠ¤ë„ ì œí•œì \n",
    "    â—‹ (ë°ì´í„°) ê¸°ê´€ë³„ë¡œ ì¶•ì Â·ë³´ìœ í•œ ë°ì´í„°ì˜ ì—°ê³„ì™€ í™œìš© ë¶€ì¡±\n",
    "    - Aê¸°ê´€ì—ì„œ ì„œë¥˜ë¥¼ ë°œê¸‰ë°›ì•„ Bê¸°ê´€ì— ì œì¶œí•˜ëŠ” \n",
    "    ============================================================\n",
    "\n",
    "    ğŸ“Š ë©”íƒ€ë°ì´í„°: {'source': '/Users/jay/Projects/20250727-langchain-note/06_Document_Loader/data/ë””ì§€í„¸ ì •ë¶€í˜ì‹  ì¶”ì§„ê³„íš.hwp', 'file_type': 'hwp', 'extraction_method': 'hancom_official'}\n",
    "\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "  * í„°ë¯¸ë„ ì‹¤í–‰ ê²°ê³¼ í™”ë©´ ìº¡ì³\n",
    "  ![custom_hwp_loader2.py ê²°ê³¼](../06_Document_Loader/Img/custom_hwp_loader2.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "  * í•œì»´ì˜¤í”¼ìŠ¤ì˜ ê³µì‹ ë°©ë²• í™œìš©\n",
    "\n",
    "    * ì™„ì „í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ â­•ï¸\n",
    "    * êµ¬ì¡° ë¶„ì„ â­•ï¸: **`ì´ë¯¸ì§€`** (`BIN0001.jpg`~`BIN0008.bmp`), `BodyText`, `PrvText` ë“± ëª¨ë‘ í™•ì¸\n",
    "    * **`Python 3.13` í˜¸í™˜ ë²„ì „ `HWP` íŒŒì„œ â­•ï¸**\n",
    "    * **`LangChain` í†µí•©** : `Document` ê°ì²´ë¡œ ë³€í™˜ â­•ï¸\n",
    "\n",
    "  * ê° ë‹¨ê³„ë³„ ë””ë²„ê¹… ì ìš©\n",
    "\n",
    "  * í•„ìš”í•œ íŒ¨í‚¤ì§€ë“¤ ì„¤ì¹˜ í•„ìš”(í„°ë¯¸ë„): \n",
    "  \n",
    "```bash\n",
    "        # ì™¸ë¶€ ëª¨ë“ˆ (ì„¤ì¹˜ í•„ìš”)\n",
    "        pip install olefile                         # OLE íŒŒì¼ ì²˜í”¼\n",
    "        pip install langchain-core                  # LangChain ë¬¸ì„œ ê°ì²´\n",
    "```\n",
    "\n",
    "  * **`Python` ë‚´ì¥ ëª¨ë“ˆ**: `zlib`(ì••ì¶•/í•´ì²´), `struct`(ë°”ì´ë„ˆë¦¬ ë°ì´í„° ì²˜ë¦¬), `os`(ìš´ì˜ì²´ì œ ì¸í„°í˜ì´ìŠ¤), `zipfile`(ZIP íŒŒì¼ ì²˜ë¦¬)\n",
    "\n",
    "    * a. `zlib` ì„¤ì¹˜ í™•ì¸ ë°©ë²•\n",
    "```python\n",
    "        python -c \"import zlib; print('âœ… zlib ì‚¬ìš© ê°€ëŠ¥!')\"\n",
    "```\n",
    "\n",
    "  * \n",
    "    * b. `sturct` = ì—­ì‹œ `Python` ë‚´ì¥ ëª¨ë“ˆ (ë³„ë„ ì„¤ì¹˜ ë¶ˆí•„ìš”) \n",
    "```python\n",
    "        # í•„ìš”í•œ ëª¨ë“  ëª¨ë“ˆ í™•ì¸\n",
    "        python -c \"\n",
    "        import zlib\n",
    "        import struct\n",
    "        import olefile\n",
    "        print('âœ… ëª¨ë“  ëª¨ë“ˆ ì‚¬ìš© ê°€ëŠ¥!')\n",
    "        \"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af920d9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f712dc4a",
   "metadata": {},
   "source": [
    "* *next: **`CSV`***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc02980a",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
