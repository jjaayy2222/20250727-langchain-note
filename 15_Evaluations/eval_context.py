# eval_script.py

from dotenv import load_dotenv
load_dotenv()

import os
from myrag5 import PDFRAG
from langchain_google_genai import ChatGoogleGenerativeAI
from langsmith.evaluation import evaluate, LangChainStringEvaluator

print("ğŸš€ Context ê¸°ë°˜ í‰ê°€ ì‹œì‘...\n")

# í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜ í™•ì¸
script_dir = os.path.dirname(os.path.abspath(__file__))
print(f"ğŸ“‚ ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜: {script_dir}\n")

# PDF íŒŒì¼ ì ˆëŒ€ ê²½ë¡œ ìƒì„±
pdf_path = os.path.join(script_dir, "data", "SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf")
print(f"ğŸ“„ PDF ê²½ë¡œ: {pdf_path}")

# íŒŒì¼ ì¡´ì¬ í™•ì¸
if not os.path.exists(pdf_path):
    print(f"âŒ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤!")
    print(f"âŒ ê²½ë¡œ: {pdf_path}\n")
    exit(1)
    
print(f"âœ… PDF í™•ì¸: {pdf_path}\n")
    

# LLM ìƒì„±
llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash-lite",
    temperature=0
)
print("âœ… LLM ìƒì„± ì™„ë£Œ: gemini-2.5-flash-lite\n")

# RAG ìƒì„±
rag = PDFRAG(
    pdf_path,  # ì ˆëŒ€ ê²½ë¡œ ì‚¬ìš©!
    llm,
    chunk_size=300,
    chunk_overlap=50,
)

# Retriever & Chain (k ëŠ˜ë¦¬ê¸°)
retriever = rag.create_retriever(k=10)

# Context í¬í•¨ ì²´ì¸ ìƒì„±
chain_with_context = rag.create_chain_with_context(retriever)

print("\n" + "="*50)
print("ğŸ§ª í…ŒìŠ¤íŠ¸ ì‹¤í–‰...")
print("="*50 + "\n")

# í…ŒìŠ¤íŠ¸
test_question = "ì‚¼ì„±ì „ìê°€ ìì²´ ê°œë°œí•œ ìƒì„±í˜• AIì˜ ì´ë¦„ì€ ë¬´ì—‡ì¸ê°€ìš”?"
test_result = chain_with_context.invoke(test_question)
print(f"ì§ˆë¬¸: {test_question}")
print(f"Context: {test_result['context'][:200]}...")
print(f"ë‹µë³€: {test_result['answer']}\n")

# ========================================
# Context ê¸°ë°˜ í‰ê°€ í•¨ìˆ˜
# ========================================

def context_answer_rag_answer(inputs: dict):
    """
    Contextì™€ Answerë¥¼ í•¨ê»˜ ë°˜í™˜
    
    Returns
    -------
    dict
        - context: ê²€ìƒ‰ëœ Context
        - answer: LLM ë‹µë³€
        - query: ì§ˆë¬¸
    """
    result = chain_with_context.invoke(inputs["question"])
    return {
        "context": result["context"],
        "answer": result["answer"],
        "query": inputs["question"],
    }

# ========================================
# í‰ê°€ì ìƒì„±
# ========================================

print("="*50)
print("ğŸ“Š í‰ê°€ì ìƒì„±...")
print("="*50 + "\n")

# 1. COT_QA Evaluator (Chain-of-Thought)
cot_qa_evaluator = LangChainStringEvaluator(
    "cot_qa",
    config={"llm": llm},
    prepare_data=lambda run, example: {
        "prediction": run.outputs["answer"],      # LLM ë‹µë³€
        "reference": run.outputs["context"],      # Context
        "input": example.inputs["question"],      # ì§ˆë¬¸
    },
)

# 2. Context_QA Evaluator
context_qa_evaluator = LangChainStringEvaluator(
    "context_qa",
    config={"llm": llm},
    prepare_data=lambda run, example: {
        "prediction": run.outputs["answer"],      # LLM ë‹µë³€
        "reference": run.outputs["context"],      # Context
        "input": example.inputs["question"],      # ì§ˆë¬¸
    },
)

print("âœ… í‰ê°€ì ìƒì„± ì™„ë£Œ\n")

# ========================================
# í‰ê°€ ì‹¤í–‰
# ========================================

print("="*50)
print("ğŸ“Š í‰ê°€ ì‹¤í–‰...")
print("="*50 + "\n")

try:
    experiment_results = evaluate(
        context_answer_rag_answer,
        data="RAG_EVAL_DATASET",
        evaluators=[cot_qa_evaluator, context_qa_evaluator],
        experiment_prefix="RAG_EVAL_CONTEXT",
        metadata={
            "variant": "Context ê¸°ë°˜ í‰ê°€ (COT_QA + Context_QA)",
            "k": 7,
            "chunk_size": 300,
        },
        max_concurrency=1,
    )
    
    print("\n" + "="*50)
    print("âœ… í‰ê°€ ì™„ë£Œ!")
    print("="*50)
    print(f"\nê²°ê³¼: {experiment_results}\n")
    
except Exception as e:
    print(f"\nâŒ ì—ëŸ¬ ë°œìƒ: {e}\n")
