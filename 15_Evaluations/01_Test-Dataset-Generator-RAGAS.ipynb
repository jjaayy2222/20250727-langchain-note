{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba52bed0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65001001",
   "metadata": {},
   "source": [
    "* 출처: LangChain 공식 문서 또는 해당 교재명\n",
    "* 원본 URL: https://smith.langchain.com/hub/teddynote/summary-stuff-documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb2ea90",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6662ac",
   "metadata": {},
   "source": [
    "## **`CH15.` `평가`** *(Evaluations)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a0d6f0",
   "metadata": {},
   "source": [
    "* **`LLM`** *(Large Language Model)* 평가: 인공지능 `언어 모델의 성능`, `정확성`, `일관성` 및 기타 중요한 측면을 `측정`하고 `분석`하는 과정\n",
    "\n",
    "* 모델의 개선, 비교, 선택 및 응용 프로그램에 적합한 모델 결정에 필수적인 단계"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80cc512",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587082c6",
   "metadata": {},
   "source": [
    "* **`평가 방법`**\n",
    "\n",
    "  * **`자동화된 메트릭`**: `BLEU`, `ROUGE`, `METEOR`, `SemScore` 등의 지표 사용\n",
    "\n",
    "  * **`인간 평가`**: 전문가 or 크라우드 소싱 통한 직접적 평가 → 수행\n",
    "\n",
    "  * **`작업 기반 평가`**: 특정 작업에서의 성능을 측정함 \n",
    "\n",
    "  * **`LLM-as-judge`**: 다른 `LLM`을 평가자로 사용하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c8d745",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219f5f1d",
   "metadata": {},
   "source": [
    "* **`LangChain에서의 Evaluation`** \n",
    "\n",
    "  * *`LangChain`은 `LLM`의 애플리케이션의 평가를 위한 다양한 도구와 프레임워크를 제공함*\n",
    "\n",
    "    * **`모듈화된 평가 컴포넌트`**: 다양한 평가방법 쉽게 구현 및 조합 가능\n",
    "\n",
    "    * **`Chain 평가`**: 전체 `LLM` 애플리케이션 파이프라인 평가\n",
    "\n",
    "    * **`데이터셋 기반 평가`**: 사용자 정의 데이터셋 사용 → 모델 평가\n",
    "\n",
    "    * **`평가 자표`**: 정확성, 일관성, 관련성 등 다양한 지료 제공"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4512cf84",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5633eef2",
   "metadata": {},
   "source": [
    "* **`LLM-as-judge`**\n",
    "\n",
    "  * `LLM-as-judge`: 다른 `LLM`의 출력을 평가하기 위해 `LLM`을 사용하는 혁신적인 접근 방식\n",
    "\n",
    "    * **`자동화`**: 인간의 개입 없이 `대규모 평가 수행 가능`\n",
    "\n",
    "    * **`일관성`**: 평가 기준 → `일관되게 적용할 수 있음`\n",
    "\n",
    "    * **`유연성`**: 다양한 평가 기준, 상황에 적응 가능\n",
    "\n",
    "    * **`비용 효율성`**: 인간 평가자에 비해 비용이 적게 들 수 있음\n",
    "\n",
    "<br>\n",
    "\n",
    "* \n",
    "  * **`LLM-as-judge`의 작동 방식**\n",
    "\n",
    "    * **`입력 제공`**: 평가할 `LLM`의 출력 • 평가 기준 제공\n",
    "\n",
    "    * **`분석`**: 평가자 `LLM`이 제공된 출력을 분석\n",
    "\n",
    "    * **`평가`**: 정의된 기준에 따라 점수 or 피드백 생성\n",
    "\n",
    "    * **`결과 집계`**: 여러 평가 결과 종합 → 최종 평가 도출"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edad7d33",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae144b7b",
   "metadata": {},
   "source": [
    "* **`장단점`**\n",
    "\n",
    "  * **`장점`**\n",
    "    * 대규모 평가 기능\n",
    "    * 빠른 피드백 루프\n",
    "    * 다양한 평가 기준 적용 가능 \n",
    "\n",
    "  * **`단점`**\n",
    "    * 평가자 `LLM`의 편향 가능성\n",
    "    * 복잡 or 미묘한 평가에 한계가 있을 수 있음\n",
    "    * 평가자 `LLM`의 성능에 의존적"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe64470e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8c3b71",
   "metadata": {},
   "source": [
    "* **`평가의 중요성`**\n",
    "\n",
    "  * **`모델 개선`**: 약점 식별 → 개선 방향 제시함\n",
    "\n",
    "  * **`신뢰성 확보`**: 모델의 성능 • 한계 이해 → 도움을 줌\n",
    "\n",
    "  * **`적합한 모델 선택`**: 특정 직업 or 도메인에 가장 적합한 모델을 선택 가능\n",
    "\n",
    "  * **`윤리적 고려사항`**: 편향, 공정성 등의 윤리적 측면 평가 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c39f53",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb18b49c",
   "metadata": {},
   "source": [
    "### **1. `합성 테스트 데이터셋 생성`** *(RAGAS)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b7758e",
   "metadata": {},
   "source": [
    "* 잦은 에러 발생\n",
    "\n",
    "  * → `Jupyter Notebook`의 `tqdm` 진행바\n",
    "\n",
    "  * → `ipywidgets` + `ContextVar` 충돌\n",
    "\n",
    "  * → `멀티스레딩` 환경 문제\n",
    "\n",
    "* **`➡️` `Python 스크립트`로 실행하기** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee44df4a",
   "metadata": {},
   "source": [
    "##### **[`➀ 01_Test-Dataset-Generator-RAGAS_1.py`](../15_Evaluations/01_Test-Dataset-Generator-RAGAS_1.py)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59d87ba",
   "metadata": {},
   "source": [
    "* 코드\n",
    "\n",
    "<small> <small>\n",
    "\n",
    "```python\n",
    "\n",
    "        # 01_Test-Dataset-Generator-RAGAS_1.py\n",
    "\n",
    "        \"\"\"\n",
    "        llama3.2:3b 교체 및 간단 테스트용\n",
    "        소요 시간: 5-8분\n",
    "        \"\"\"\n",
    "        import os\n",
    "        import warnings\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "        from dotenv import load_dotenv\n",
    "        load_dotenv()\n",
    "\n",
    "        print(\"=\"*60)\n",
    "        print(\"llama3.2:3b 모델 RAGAS 간단 테스트\")\n",
    "        print(\"=\"*60)\n",
    "        print()\n",
    "\n",
    "        # 패키지 임포트\n",
    "        from ragas.testset.generator import TestsetGenerator\n",
    "        from ragas.testset.evolutions import simple\n",
    "        from ragas.llms import LangchainLLMWrapper\n",
    "        from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "        from ragas.testset.extractor import KeyphraseExtractor\n",
    "        from ragas.testset.docstore import InMemoryDocumentStore\n",
    "\n",
    "        from langchain_ollama import ChatOllama\n",
    "        from langchain_huggingface import HuggingFaceEmbeddings\n",
    "        from langchain_community.document_loaders import PDFPlumberLoader\n",
    "        from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "        # ============================================\n",
    "        # 1. LLM 설정\n",
    "        # ============================================\n",
    "        print(\"1. LLM 설정 중...\")\n",
    "\n",
    "        generator_llm = ChatOllama(\n",
    "            model=\"llama3.2:3b\",                    # 자연어 모델로 교체\n",
    "            temperature=0.7,\n",
    "            num_predict=512,\n",
    "        )\n",
    "\n",
    "        # 간단 테스트\n",
    "        try:\n",
    "            test = generator_llm.invoke(\"Say hello\")\n",
    "            print(f\"    ✅ LLM 작동: {test.content[:30]}...\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ 에러: {e}\")\n",
    "            print(\"   💡 확인: ollama list\")\n",
    "            print(\"   💡 다운로드: ollama pull llama3.2:3b\")\n",
    "            exit(1)\n",
    "\n",
    "        print()\n",
    "\n",
    "        # ============================================\n",
    "        # 2. Embeddings\n",
    "        # ============================================\n",
    "        print(\"2. Embeddings 설정 중...\")\n",
    "        embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "            model_kwargs={'device': 'cpu'},\n",
    "        )\n",
    "        print(\"    ✅ 완료\")\n",
    "        print()\n",
    "\n",
    "        # ============================================\n",
    "        # 3. 문서 로드 (3페이지만!)\n",
    "        # ============================================\n",
    "        print(\"3. 문서 로드 중...\")\n",
    "        loader = PDFPlumberLoader(\"../data/SPRI_AI_Brief_2023년12월호_F.pdf\")\n",
    "        docs = loader.load()[3:6]                       # 3페이지만\n",
    "\n",
    "        for doc in docs:\n",
    "            doc.metadata[\"filename\"] = doc.metadata[\"source\"]\n",
    "\n",
    "        print(f\"    ✅ {len(docs)}페이지 로드\")\n",
    "        print()\n",
    "\n",
    "        # ============================================\n",
    "        # 4. DocumentStore\n",
    "        # ============================================\n",
    "        print(\"4. DocumentStore 초기화 중...\")\n",
    "\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=300,             # 사이즈 감소\n",
    "            chunk_overlap=30            # 오버랩 감소\n",
    "        )\n",
    "\n",
    "        langchain_llm = LangchainLLMWrapper(generator_llm)\n",
    "        keyphrase_extractor = KeyphraseExtractor(llm=langchain_llm)\n",
    "        ragas_embeddings = LangchainEmbeddingsWrapper(embeddings)\n",
    "\n",
    "        docstore = InMemoryDocumentStore(\n",
    "            splitter=splitter,\n",
    "            embeddings=ragas_embeddings,\n",
    "            extractor=keyphrase_extractor,\n",
    "        )\n",
    "\n",
    "        print(\"    ✅ 완료\")\n",
    "        print()\n",
    "\n",
    "        # ============================================\n",
    "        # 5. Generator\n",
    "        # ============================================\n",
    "        print(\"5. Generator 생성 중...\")\n",
    "\n",
    "        generator = TestsetGenerator.from_langchain(\n",
    "            generator_llm,\n",
    "            generator_llm,\n",
    "            ragas_embeddings,\n",
    "            docstore=docstore,\n",
    "        )\n",
    "\n",
    "        print(\"    ✅ 완료\")\n",
    "        print()\n",
    "\n",
    "        # ============================================\n",
    "        # 6. 분포\n",
    "        # ============================================\n",
    "        distributions = {\n",
    "            simple: 1.0,  # 100% 간단한 질문\n",
    "        }\n",
    "\n",
    "        # ============================================\n",
    "        # 7. 생성 (1개만!)\n",
    "        # ============================================\n",
    "        print(\"=\"*60)\n",
    "        print(\"🔄 테스트셋 생성 시작\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"예상 시간: 5-8분\")\n",
    "        print(\"멈춰 보여도 정상입니다!\")\n",
    "        print()\n",
    "\n",
    "        try:\n",
    "            testset = generator.generate_with_langchain_docs(\n",
    "                documents=docs,\n",
    "                test_size=1,                        # 1개만!\n",
    "                distributions=distributions,\n",
    "                with_debugging_logs=False,\n",
    "                raise_exceptions=True,\n",
    "            )\n",
    "            \n",
    "            # ============================================\n",
    "            # 8. 결과\n",
    "            # ============================================\n",
    "            test_df = testset.to_pandas()\n",
    "            \n",
    "            if len(test_df) > 0:\n",
    "                print()\n",
    "                print(\"=\"*60)\n",
    "                print(\"✅✅✅ 성공! ✅✅✅\")\n",
    "                print(\"=\"*60)\n",
    "                print()\n",
    "                \n",
    "                for idx, row in test_df.iterrows():\n",
    "                    print(f\"질문: {row['question']}\")\n",
    "                    print(f\"답변: {row['ground_truth'][:80]}...\")\n",
    "                    print()\n",
    "                \n",
    "                test_df.to_csv(\"data/.csv\", index=False)\n",
    "                # test_df.to_csv(\"data/ragas_synthetic_dataset_1.csv\", index=False, encoding='utf-8-sig')\n",
    "                # 혹은 데이터 경로: \"../data/agas_synthetic_dataset_1.csv\"\n",
    "                print(\"✅ 저장: data/jay_success.csv\")\n",
    "\n",
    "                \n",
    "            else:\n",
    "                print(\"❌ 질문 생성 실패\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print()\n",
    "            print(\"❌ 에러:\")\n",
    "            print(f\"    {e}\")\n",
    "            print()\n",
    "            print(\"💡 나에게 에러 메시지 보내줘!\")\n",
    "\n",
    "        print()\n",
    "        print(\"=\"*60)\n",
    "        print(\"✅ 테스트셋 생성 완료\")\n",
    "        print(\"=\"*60)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f86f43",
   "metadata": {},
   "source": [
    "* 결과\n",
    "\n",
    "<small> <small>\n",
    "\n",
    "```bash\n",
    "\n",
    "    ============================================================\n",
    "    llama3.2:3b 모델 RAGAS 간단 테스트\n",
    "    ============================================================\n",
    "\n",
    "    1. LLM 설정 중...\n",
    "        ✅ LLM 작동: Hello! It's nice to meet you. ...\n",
    "\n",
    "    2. Embeddings 설정 중...\n",
    "        ✅ 완료\n",
    "\n",
    "    3. 문서 로드 중...\n",
    "        ✅ 3페이지 로드\n",
    "\n",
    "    4. DocumentStore 초기화 중...\n",
    "        ✅ 완료\n",
    "\n",
    "    5. Generator 생성 중...\n",
    "        ✅ 완료\n",
    "\n",
    "    ============================================================\n",
    "    🔄 테스트셋 생성 시작\n",
    "    ============================================================\n",
    "    예상 시간: 5-8분\n",
    "    멈춰 보여도 정상입니다!\n",
    "\n",
    "    # embedding nodes:  94%|████████████████████████████████████████████████████████████████████████████████▎    | 34/36 [04:09<00:39, 19.87s/it]\n",
    "\n",
    "    # Generating:   0%|                                                                                                    | 0/1 [00:00<?, ?it/s]\n",
    "\n",
    "    ============================================================\n",
    "    ✅✅✅ 성공! ✅✅✅\n",
    "    ============================================================\n",
    "\n",
    "    질문: context: \"n \\uc8fc\\uc694 7\\uac1c\\uad6d(G7)*\\uc740 2023\\ub144 10\\uc6d4 30\\uc77c \\u201’\\ud788\\ub85c\\uc2dc\\ub9c8 AI \\ud504\\ub85c\\uc138\\uc2a4’s\\ud97c \\ud1b5\\ud574 AI \\uae30\\uc5c5 \\ub300\\uc0c1\\uc758 AI \\uad6d\\uc81c\\n\\ud589\\ub3d9\\uac15\\ub839(International Code of Conduct for Advanced AI Systems)\\uc5d0 \\ud569\\uc758\\n\\u2219 G7\\uc740 2023\\ub144 5\\uc6d4 \\uc77c\\ubcf8 \\ud788\\ub85c\\uc2dc\\ub9c8\\uc5d0\\uc11c \\uac1c\\ucd5c\\ub41c \\uc815\\uc0c1\\ud68c\\uc758\\uc5d0\\uc11c \\uc0dd\\uc131 AI\\uc5d0 \\uad00\\ud55c \\uad6d\\uc81c\\uaddc\\ubc94 \\ub9c8\\ub828\\acfc\\n\\uc815\\ubcf4\\uacf5\\uc720\\ub97c \\uc704\\ud574 \\u201’\\ud788\\ub85c\\uc2dc\\ub9c8 AI \\ud504\\ub85c\\uc138\\uc2a4’s\\ud97c \\ucd9c\\ubc94**\\n\\u2219 \\uae30\\uc5c5\\uc758 \\uc790\\ubc1c\\uc801 \\ucc44\\ud0dd\\uc744 \\uc704\\ud574 \\ub9c8\\ub828\\ub41c \\uc774\\ubc88 \\ud589\\ub3d9\\uac15\\ub839\\uc740 \\uae30\\ubc18\\ubaa8\\ub378\\acfc \\uc0dd\\uc131 AI\\ud97c \\ud3ec\\ud568\\ud55c \\ucca8\\ub2e8 AI \\uc2dc\\uc2a4\\ud15c\\uc758\\n\\uc704\\ud5d8 \\uc2dd\\ubcc4\\uacfc \\uc644\\ud654\\uc5d\n",
    "    답변: The answer to given question is not present in context...\n",
    "\n",
    "\n",
    "    ❌ 에러:\n",
    "        Cannot save file into a non-existent directory: 'data'\n",
    "\n",
    "    💡 나에게 에러 메시지 보내줘!\n",
    "\n",
    "    ============================================================\n",
    "    ✅ 테스트셋 생성 완료\n",
    "    ============================================================\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9b6b9b",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* **`../15_Evaluations/data/ragas_synthetic_dataset_1.csv`으로 저장 실패**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ba33eb",
   "metadata": {},
   "source": [
    "##### **[`➁ 01_Test-Dataset-Generator-RAGAS_2.py`](../15_Evaluations/01_Test-Dataset-Generator-RAGAS_2.py)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2f0c90",
   "metadata": {},
   "source": [
    "* 코드\n",
    "\n",
    "<small> <small>\n",
    "\n",
    "```python \n",
    "\n",
    "        # 01_Test-Dataset-Generator-RAGAS_2.py\n",
    "\n",
    "        \"\"\"\n",
    "        RAGAS 테스트셋 생성 - 빠른 테스트용\n",
    "        \"\"\"\n",
    "\n",
    "        import os\n",
    "        import warnings\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "        from dotenv import load_dotenv\n",
    "        load_dotenv()\n",
    "\n",
    "        print(\"=\"*60)\n",
    "        print(\"llama3.2:3b 모델 RAGAS 간단 테스트\")\n",
    "        print(\"=\"*60)\n",
    "        print()\n",
    "\n",
    "        print(\"🔧 RAGAS 초기화 중...\\n\")\n",
    "\n",
    "        from ragas.testset.generator import TestsetGenerator\n",
    "        from ragas.testset.evolutions import simple, reasoning\n",
    "        from ragas.llms import LangchainLLMWrapper\n",
    "        from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "        from ragas.testset.extractor import KeyphraseExtractor\n",
    "        from ragas.testset.docstore import InMemoryDocumentStore\n",
    "\n",
    "        from langchain_ollama import ChatOllama\n",
    "        from langchain_huggingface import HuggingFaceEmbeddings\n",
    "        from langchain_community.document_loaders import PDFPlumberLoader\n",
    "        from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "        # 1. LLM 설정\n",
    "        print(\"1. LLM 설정 중...\")\n",
    "\n",
    "        generator_llm = ChatOllama(\n",
    "            model=\"llama3.2:3b\",                    # 자연어 모델로 교체\n",
    "            temperature=0.7,\n",
    "            num_predict=512,\n",
    "        )\n",
    "\n",
    "        critic_llm = ChatOllama(\n",
    "            model=\"llama3.2:3b\",\n",
    "            temperature=0.1,\n",
    "            num_predict=512,\n",
    "        )\n",
    "\n",
    "        # 간단 테스트\n",
    "        try:\n",
    "            test = generator_llm.invoke(\"Say hello\")\n",
    "            print(f\"    ✅ LLM 작동: {test.content[:30]}...\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ 에러: {e}\")\n",
    "            print(\"   💡 확인: ollama list\")\n",
    "            print(\"   💡 다운로드: ollama pull llama3.2:3b\")\n",
    "            exit(1)\n",
    "\n",
    "\n",
    "        print(\"    ✅ Ollama LLM 설정 완료\")\n",
    "        print()\n",
    "\n",
    "\n",
    "        # 2. Embeddings 설정\n",
    "        print(\"2. Embeddings 설정 중...\")\n",
    "\n",
    "        embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "            model_kwargs={'device': 'cpu'},\n",
    "            encode_kwargs={'normalize_embeddings': True}\n",
    "        )\n",
    "\n",
    "        print(\"    ✅ 완료\")\n",
    "        print()\n",
    "\n",
    "\n",
    "        # 3. 문서 로드\n",
    "        print(\"3. 문서 로드 중...\")\n",
    "        loader = PDFPlumberLoader(\"data/SPRI_AI_Brief_2023년12월호_F.pdf\")\n",
    "        docs = loader.load()\n",
    "        docs = docs[3:8]                            # 5페이지만!\n",
    "\n",
    "        # metadata 설정\n",
    "        for doc in docs:\n",
    "            doc.metadata[\"filename\"] = doc.metadata[\"source\"]\n",
    "\n",
    "        print(f\"    ✅ {len(docs)}페이지 로드\")\n",
    "        print()\n",
    "\n",
    "\n",
    "        # 4. DocumentStore 초기화\n",
    "        print(\"4. DocumentStore 초기화 중...\")\n",
    "\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            #chunk_size=800,            # 조금 줄임\n",
    "            #chunk_size=400,            # 더 작게 조절\n",
    "            #chunk_overlap=50 \n",
    "            chunk_size=300,             # 사이즈 감소\n",
    "            chunk_overlap=30            # 오버랩 감소\n",
    "        )\n",
    "\n",
    "        langchain_llm = LangchainLLMWrapper(generator_llm)\n",
    "        keyphrase_extractor = KeyphraseExtractor(llm=langchain_llm)\n",
    "        ragas_embeddings = LangchainEmbeddingsWrapper(embeddings)\n",
    "\n",
    "        docstore = InMemoryDocumentStore(\n",
    "            splitter=splitter,\n",
    "            embeddings=ragas_embeddings,\n",
    "            extractor=keyphrase_extractor,\n",
    "        )\n",
    "\n",
    "        print(\"    ✅ 완료\")\n",
    "        print()\n",
    "\n",
    "\n",
    "        # 5. TestsetGenerator 생성\n",
    "        print(\"5. Generator 생성 중...\")\n",
    "\n",
    "        generator = TestsetGenerator.from_langchain(\n",
    "            generator_llm,\n",
    "            critic_llm,\n",
    "            ragas_embeddings,\n",
    "            docstore=docstore,\n",
    "        )\n",
    "\n",
    "        print(\"    ✅ 완료\")\n",
    "        print()\n",
    "\n",
    "\n",
    "        # 6. 분포 (간단하게!)\n",
    "        distributions = {\n",
    "            simple: 0.7,      # 70%\n",
    "            reasoning: 0.3,   # 30%\n",
    "        }\n",
    "\n",
    "\n",
    "        # 7. 생성 (2개만!)\n",
    "        print(\"=\"*60)\n",
    "        print(\"🔄 테스트셋 생성 시작\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"🔄 테스트셋 생성 중... (시간이 걸릴 수 있습니다)\")\n",
    "        print(\"    - 분포: 간단(70%), 추론(30%)\\n\")\n",
    "        print(\"🔄 멈춰 보여도 정상입니다! 🔄 \")\n",
    "        print(\"=\"*60)\n",
    "        print()\n",
    "\n",
    "        try:\n",
    "            testset = generator.generate_with_langchain_docs(\n",
    "                documents=docs,\n",
    "                test_size=2,                        # 2개만\n",
    "                distributions=distributions,\n",
    "                with_debugging_logs=False,          # 로그 비활성화\n",
    "                raise_exceptions=False,             # 에러 무시\n",
    "            )\n",
    "            \n",
    "            # 8. 결과 저장\n",
    "            test_df = testset.to_pandas()\n",
    "            test_df.to_csv(\"data/ragas_synthetic_dataset_2.csv\", index=False, encoding='utf-8-sig')\n",
    "            \n",
    "            if len(test_df) > 0:\n",
    "                print()\n",
    "                print(\"=\"*60)\n",
    "                print(\"✅✅✅ 성공! ✅✅✅\")\n",
    "                print(\"=\"*60)\n",
    "                print()\n",
    "                \n",
    "                for idx, row in test_df.iterrows():\n",
    "                    print(f\"질문: {row['question']}\")\n",
    "                    print(f\"답변: {row['ground_truth'][:80]}...\")\n",
    "                    print()\n",
    "\n",
    "                print(\"✅ 저장: data/ragas_synthetic_dataset_2.csv\")\n",
    "\n",
    "            else:\n",
    "                print(\"❌ 질문 생성 실패\")\n",
    "\n",
    "            print()\n",
    "            print(\"=\"*60)  \n",
    "            print(f\"\\n✅ 테스트셋 생성 완료!\")\n",
    "            print(f\"   - 생성된 질문 수: {len(test_df)}\")\n",
    "            print(f\"   - 저장 위치: ../15_Evaluations/data/ragas_synthetic_dataset.csv\\n\")\n",
    "            print(test_df)\n",
    "            \n",
    "            # 9. 결과 미리보기\n",
    "            print(\"📊 생성된 질문 미리보기:\")\n",
    "            print(\"=\"*80)\n",
    "            for idx, row in test_df.iterrows():\n",
    "                print(f\"\\n질문 {idx+1}:\")\n",
    "                print(f\"  {row['question']}\")\n",
    "                print(f\"  Ground Truth: {row['ground_truth'][:100]}...\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ 에러 발생: {e}\")\n",
    "            print(\"\\n💡 해결 방법:\")\n",
    "            print(\"    1. Ollama가 실행 중인지 확인\")\n",
    "            print(\"    2. 모델 다운로드: ollama pull llama3.2:3b\")\n",
    "            print(\"    3. langchain-ollama 버전 확인: pip list | grep langchain-ollama\")\n",
    "            print(\"        → 0.1.3이어야 함!\")\n",
    "\n",
    "        print(\"\\n✅ 정상적으로 테스트셋 생성 완료!\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37be1571",
   "metadata": {},
   "source": [
    "* 결과\n",
    "\n",
    "  * try_1\n",
    "\n",
    "<small> <small>\n",
    "\n",
    "```bash\n",
    "\n",
    "        🔧 RAGAS 초기화 중...\n",
    "\n",
    "        ✅ Ollama LLM 설정 완료\n",
    "        ✅ Embeddings 설정 완료\n",
    "        ✅ 문서 로드 완료: 5페이지\n",
    "\n",
    "        ✅ TestsetGenerator 생성 완료\n",
    "\n",
    "        🔄 시작... (10분 예상)\n",
    "        🔄 테스트셋 생성 중... (시간이 걸릴 수 있습니다)\n",
    "            - 분포: 간단(70%), 추론(30%)\n",
    "        \n",
    "        # 중간 과정\n",
    "        # embedding nodes:  37%|██████████████████████████▌                                             | 17/46 [00:33<01:46,  3.68s/it]\n",
    "        # embedding nodes:  87%|██████████████████████████████████████████████████████████████▌         | 40/46 [06:56<02:35, 25.91s/it]\n",
    "        # embedding nodes:  98%|██████████████████████████████████████████████████████████████████████▍ | 45/46 [09:26<00:29, 29.29s/it]\n",
    "\n",
    "        # Generating:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]\n",
    "        # Generating:  50%|███████████████████████████████████████                                       | 1/2 [06:24<06:24, 384.85s/it]\n",
    "\n",
    "        Generating:  50%|███████████████████████████████████████                                       | 1/2 [06:24<06:24, 384.85s/itFailed to parse output. Returning None.\n",
    "        ↳ 멈춤 → 강제종료    \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8befff4",
   "metadata": {},
   "source": [
    "* \n",
    "  * try_2\n",
    "\n",
    "<small> <small>\n",
    "\n",
    "```bash\n",
    "\n",
    "        ============================================================\n",
    "        llama3.2:3b 모델 RAGAS 간단 테스트\n",
    "        ============================================================\n",
    "\n",
    "        🔧 RAGAS 초기화 중...\n",
    "\n",
    "        1. LLM 설정 중...\n",
    "            ✅ LLM 작동: Hello! It's nice to meet you. ...\n",
    "            ✅ Ollama LLM 설정 완료\n",
    "\n",
    "        2. Embeddings 설정 중...\n",
    "            ✅ 완료\n",
    "\n",
    "        3. 문서 로드 중...\n",
    "            ✅ 5페이지 로드\n",
    "\n",
    "        4. DocumentStore 초기화 중...\n",
    "            ✅ 완료\n",
    "\n",
    "        5. Generator 생성 중...\n",
    "            ✅ 완료\n",
    "\n",
    "        ============================================================\n",
    "        🔄 테스트셋 생성 시작\n",
    "        ============================================================\n",
    "        🔄 테스트셋 생성 중... (시간이 걸릴 수 있습니다)\n",
    "            - 분포: 간단(70%), 추론(30%)\n",
    "\n",
    "        🔄 멈춰 보여도 정상입니다! 🔄 \n",
    "        ============================================================\n",
    "\n",
    "        # 중간 과정\n",
    "        # embedding nodes:  26%|█████████████████████▏                                                            | 15/58 [00:44<03:09,  4.40s/it\n",
    "        # embedding nodes:  74%|████████████████████████████████████████████████████████████▊                     | 43/58 [04:51<02:06,  8.45s/it]\n",
    "        # embedding nodes:  93%|████████████████████████████████████████████████████████████████████████████▎     | 54/58 [09:42<01:50, 27.70s/it]\n",
    "\n",
    "        # Generating:   0%|                                                                                        | 0/2 [00:00<?, ?it/s]\n",
    "        # Generating:  50%|████████████████████████████████████████████                                            | 1/2 [07:46<07:46, 466.04s/it]\n",
    "\n",
    "        Generating: 100%|████████████████████████████████████████████████████████████████████████████████████████| 2/2 [22:33<00:00, 676.72s/it]\n",
    "\n",
    "        ============================================================\n",
    "        ✅✅✅ 성공! ✅✅✅\n",
    "        ============================================================\n",
    "\n",
    "        질문: Here is a question that can be fully answered from the given context:\n",
    "\n",
    "        \"What does 'FTC' stand for in the provided text?\"\n",
    "\n",
    "        This question can be answered by referring to the key phrase \"KEY Contents\\nn \\ubbf8\\uad6d FTC...\" in the context, which explicitly states what \"FTC\" stands for.\n",
    "        답변: FTC...\n",
    "\n",
    "        질문: The goal is to create a rewritten question that conveys the same meaning as \"What type of AI does the Federal Trade Commission possess?\" without directly stating it.\n",
    "\n",
    "        Here's a revised version:\n",
    "\n",
    "        \"What kind of AI system does the FTC use?\"\n",
    "\n",
    "        This rewritten question achieves the same intent as the original but in a more concise and indirect manner, using abbreviation (\"FTC\" instead of \"Federal Trade Commission\") to make the question shorter.\n",
    "        답변: The Federal Trade Commission uses artificial intelligence primarily for regulati...\n",
    "\n",
    "        ✅ 저장: data/ragas_synthetic_dataset_2.csv\n",
    "\n",
    "        ============================================================\n",
    "\n",
    "        ✅ 테스트셋 생성 완료!\n",
    "        - 생성된 질문 수: 2\n",
    "        - 저장 위치: ../15_Evaluations/data/ragas_synthetic_dataset.csv\n",
    "\n",
    "                                                    question  ... episode_done\n",
    "        0  Here is a question that can be fully answered ...  ...         True\n",
    "        1  The goal is to create a rewritten question tha...  ...         True\n",
    "\n",
    "        [2 rows x 6 columns]\n",
    "        📊 생성된 질문 미리보기:\n",
    "        ================================================================================\n",
    "\n",
    "        질문 1:\n",
    "        Here is a question that can be fully answered from the given context:\n",
    "\n",
    "        \"What does 'FTC' stand for in the provided text?\"\n",
    "\n",
    "        This question can be answered by referring to the key phrase \"KEY Contents\\nn \\ubbf8\\uad6d FTC...\" in the context, which explicitly states what \"FTC\" stands for.\n",
    "        Ground Truth: FTC...\n",
    "\n",
    "        질문 2:\n",
    "        The goal is to create a rewritten question that conveys the same meaning as \"What type of AI does the Federal Trade Commission possess?\" without directly stating it.\n",
    "\n",
    "        Here's a revised version:\n",
    "\n",
    "        \"What kind of AI system does the FTC use?\"\n",
    "\n",
    "        This rewritten question achieves the same intent as the original but in a more concise and indirect manner, using abbreviation (\"FTC\" instead of \"Federal Trade Commission\") to make the question shorter.\n",
    "        Ground Truth: The Federal Trade Commission uses artificial intelligence primarily for regulating privacy, trademar...\n",
    "\n",
    "        ✅ 정상적으로 테스트셋 생성 완료!\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84487e3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5238649b",
   "metadata": {},
   "source": [
    "### **2. `주피터 노트북`으로 시도** - *`try_1`*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f936725",
   "metadata": {},
   "source": [
    "#### **1) `합성 테스트 데이터셋 생성`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6ede17",
   "metadata": {},
   "source": [
    "* **왜 `합성 테스트 데이터`(`Synthetic Test Dataset`) 인가?**\n",
    "\n",
    "  * `RAG` *(검색 증강 생성) 증강 파이프라인의 성능을 평가하는 것은 매우 중요*\n",
    "\n",
    "  * 어려움\n",
    "\n",
    "    * 문서에서 `수백 개의 QA` (`질문-문맥-응답`) `샘플`을 `수동`으로 `생성`하는 것 = **`시간과 노동력이 많이 소요`**\n",
    "    * `사람이 만든 질문` = 철저한 평가에 필요한 복잡성 수준에 도달하기 어려움 = 궁극적으로 평가의 품질에 영향을 미칠 수 있음\n",
    "\n",
    "  * **`합성 데이터 생성`을 사용** → 데이터 집계 프로세스에서 `개발자의 시간`을 **`90%`** 까지 **`감소 가능`**\n",
    "\n",
    "  * 참고: [RAGAS](https://docs.ragas.io/en/latest/concepts/testset_generation.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd6b318",
   "metadata": {},
   "source": [
    "* 사전에 `VS Code` 터미널에 설치할 것\n",
    "\n",
    "```bash\n",
    "\n",
    "        pip install -qU ragas\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1958a16",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 구버전으로 설치할 것!\n",
    "\n",
    "```bash\n",
    "\n",
    "        pip install ragas==0.1.21\n",
    "\n",
    "        # 의존성 확인\n",
    "        pip list | grep -E \"ragas|langchain\"\n",
    "\n",
    "        # 예상 출력:\n",
    "        # ragas                     0.1.21\n",
    "        # ...\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a390668a",
   "metadata": {},
   "source": [
    "* 오류 발생 → 커널 및 패키지 충돌\n",
    "\n",
    "* 트러블 슈팅 문서 참고\n",
    "  * [`RAGAS_Synthetic_Dataset_Generation_Version_Conflicts_Troubleshooting`](docs/troubleshooting/RAGAS_Synthetic_Dataset_Generation_Version_Conflicts_Alternative_Solutions_Troubleshooting.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716fbfa2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a342870f",
   "metadata": {},
   "source": [
    "#### **2) `환경설정`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d763fc98",
   "metadata": {},
   "source": [
    "##### **`➀ 기본설정`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b354960e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"TQDM_DISABLE\"] = \"1\"                        # 진행바 비활성화\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(\"✅ 환경 설정 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2178c5",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* ✅ 환경 설정 완료"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a36bcf",
   "metadata": {},
   "source": [
    "##### **`➁ LangSmith 설정`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ef86f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "from langsmith import traceable\n",
    "\n",
    "import os\n",
    "\n",
    "# LangSmith 환경 변수 확인\n",
    "\n",
    "print(\"\\n--- LangSmith 환경 변수 확인 ---\")\n",
    "langchain_tracing_v2 = os.getenv('LANGCHAIN_TRACING_V2')\n",
    "langchain_project = os.getenv('LANGCHAIN_PROJECT')\n",
    "langchain_api_key_status = \"설정됨\" if os.getenv('LANGCHAIN_API_KEY') else \"설정되지 않음\" # API 키 값은 직접 출력하지 않음\n",
    "\n",
    "if langchain_tracing_v2 == \"true\" and os.getenv('LANGCHAIN_API_KEY') and langchain_project:\n",
    "    print(f\"✅ LangSmith 추적 활성화됨 (LANGCHAIN_TRACING_V2='{langchain_tracing_v2}')\")\n",
    "    print(f\"✅ LangSmith 프로젝트: '{langchain_project}'\")\n",
    "    print(f\"✅ LangSmith API Key: {langchain_api_key_status}\")\n",
    "    print(\"  -> 이제 LangSmith 대시보드에서 이 프로젝트를 확인해 보세요.\")\n",
    "else:\n",
    "    print(\"❌ LangSmith 추적이 완전히 활성화되지 않았습니다. 다음을 확인하세요:\")\n",
    "    if langchain_tracing_v2 != \"true\":\n",
    "        print(f\"  - LANGCHAIN_TRACING_V2가 'true'로 설정되어 있지 않습니다 (현재: '{langchain_tracing_v2}').\")\n",
    "    if not os.getenv('LANGCHAIN_API_KEY'):\n",
    "        print(\"  - LANGCHAIN_API_KEY가 설정되어 있지 않습니다.\")\n",
    "    if not langchain_project:\n",
    "        print(\"  - LANGCHAIN_PROJECT가 설정되어 있지 않습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f14d715",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 셀 출력\n",
    "\n",
    "    ```bash\n",
    "    --- LangSmith 환경 변수 확인 ---\n",
    "    ✅ LangSmith 추적 활성화됨 (LANGCHAIN_TRACING_V2='true')\n",
    "    ✅ LangSmith 프로젝트: 'LangChain-prantice'\n",
    "    ✅ LangSmith API Key: 설정됨\n",
    "    -> 이제 LangSmith 대시보드에서 이 프로젝트를 확인해 보세요.\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13582956",
   "metadata": {},
   "source": [
    "##### **`➂ 패키지 임포트`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1c46df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, conditional, multi_context\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from ragas.testset.extractor import KeyphraseExtractor\n",
    "from ragas.testset.docstore import InMemoryDocumentStore\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "print(\"✅ 패키지 임포트 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3aaf569",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* ✅ 패키지 임포트 완료 - (`1.8s`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe5bf03",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb35bf01",
   "metadata": {},
   "source": [
    "#### **3) `LLM` 호출하기**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0395c40",
   "metadata": {},
   "source": [
    "##### **`➀ 데이터셋 생성기`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a95d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 생성기\n",
    "generator_llm = ChatOllama(\n",
    "    model=\"llama3.2:3b\",                    # 자연어 모델로 교체\n",
    "    temperature=0.7,\n",
    "    num_predict=512,\n",
    ")\n",
    "\n",
    "# 테스트 호출\n",
    "test_response = generator_llm.invoke(\"Hello\")\n",
    "print(f\"✅ 데이터셋 생성기 LLM 작동 확인: {test_response.content[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfecc29",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* ✅ 데이터셋 생성기 LLM 작동 확인: How can I assist you today?... - (`4.7s`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f878d27",
   "metadata": {},
   "source": [
    "##### **`➁ 데이터셋 비평기`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c0457d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 비평기\n",
    "critic_llm = ChatOllama(\n",
    "    model=\"llama3.2:3b\",\n",
    "    temperature=0.1,\n",
    "    num_predict=512,\n",
    ")\n",
    "\n",
    "# 테스트 호출\n",
    "test_response2 = critic_llm.invoke(\"안녕?\")\n",
    "print(f\"✅ 데이터셋 생성기 LLM 작동 확인: {test_response2.content[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283a71fb",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* ✅ 데이터셋 생성기 LLM 작동 확인: 안녕하세요! (Hello!) How can I help you today?... - (`2.3s`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a2d393",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486f1503",
   "metadata": {},
   "source": [
    "#### **4) `임베딩 설정하기`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0e7cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 생성\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "print(\"✅ 임베딩 생성 완료\")\n",
    "\n",
    "# 테스트 임베딩\n",
    "test_embed = embeddings.embed_query(\"테스트\")\n",
    "print(f\"✅ 임베딩 작동 확인: 차원={len(test_embed)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a717a3",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* ✅ 임베딩 생성 완료 - (`9.1s`)\n",
    "\n",
    "* ✅ 임베딩 작동 확인: 차원=384"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8473a6b2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20212e1",
   "metadata": {},
   "source": [
    "#### **5) `문서 로드` & `전처리`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01da7831",
   "metadata": {},
   "source": [
    "##### **`➀ 문서 로드하기`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9daeb247",
   "metadata": {},
   "source": [
    "* 실습에 활용할 문서\n",
    "\n",
    "  * 소프트웨어정책연구소 (SPRi) - 2023년 12월호\n",
    "\n",
    "    * 저자: 유재흥(AI정책연구실 책임연구원), 이지수(AI정책연구실 위촉연구원)\n",
    "    * 참고: [링크](https://spri.kr/posts/view/23669)\n",
    "    * 파일명: [SPRI_AI_Brief_2023년12월호_F.pdf](../15_Evaluations/data/SPRI_AI_Brief_2023년12월호_F.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7ea56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PDFPlumberLoader(\"../15_Evaluations/data/SPRI_AI_Brief_2023년12월호_F.pdf\")\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "# 목차, 끝 페이지 제외\n",
    "docs = docs[3:-1]                   # 교재 사이트와 똑같이 설정 \n",
    "\n",
    "print(f\"✅ 문서 로드 완료: {len(docs)}페이지\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39392440",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* ✅ 문서 로드 완료: 19페이지 - (`3.3s`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e1ee8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(docs))               # <class 'list'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6387636",
   "metadata": {},
   "source": [
    "##### **`➁ 메타 데이터 설정하기`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e33ef7",
   "metadata": {},
   "source": [
    "* **`metadata`** 확인하기\n",
    "\n",
    "  * 각 문서 객체에는 `metadata` → 액세스할 수 있는 문서에 대한 `추가 정보`를 `저장`하는 데 사용할 수 있는 `메타데이터 사전`이 `포함`되어 있음\n",
    "\n",
    "  * 메타데이터 사전에 **`filename`** 이라는 `키`가 `포함` 여부 확인하기\n",
    "\n",
    "    * 이 키는 `Test datasets` 생성 프로세스에서 활용될 것\n",
    "\n",
    "    * 메타데이터의 `filename` 속성 = `동일한 문서`에 속한 `청크`를 `식별`하는 데 `사용`됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba6fd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metadata 설정\n",
    "for doc in docs:\n",
    "    doc.metadata[\"filename\"] = doc.metadata[\"source\"]\n",
    "    \n",
    "print(f\"✅ 첫 문서 미리보기: {docs[0].page_content[:50]} ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf2228b",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* ✅ 첫 문서 미리보기: 1. 정책/법제 2. 기업/산업 3. 기술/연구 4. 인력/교육\n",
    "\n",
    "* 미국, 안전하고 신뢰할 수 ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdd56c5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5ec0cb",
   "metadata": {},
   "source": [
    "#### **6) `DocumentStore` 초기화**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d532750",
   "metadata": {},
   "source": [
    "* **`docsstore 구성요소` 및 `docstore 초기화`**\n",
    "\n",
    "  * **`DocumentStore` 초기화**\n",
    "\n",
    "  * **`사용자 정의 LLM`과 `임베딩` 사용**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1395a99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 분할기 설정\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "# LangchainLLMWrapper로 감싸 Ragas와 호환되도록 함\n",
    "langchain_llm = LangchainLLMWrapper(generator_llm)\n",
    "\n",
    "# 주요 구문 추출기 초기화 (위에서 정의한 LLM 사용)\n",
    "keyphrase_extractor = KeyphraseExtractor(llm=langchain_llm)\n",
    "\n",
    "# ragas_embeddings 생성\n",
    "ragas_embeddings = LangchainEmbeddingsWrapper(embeddings)\n",
    "\n",
    "# InMemoryDocumentStore 초기화 (문서를 메모리에 저장하고 관리하는 저장소)\n",
    "docstore = InMemoryDocumentStore(\n",
    "    splitter=splitter,\n",
    "    embeddings=ragas_embeddings,\n",
    "    extractor=keyphrase_extractor,\n",
    ")\n",
    "\n",
    "print(\"✅ DocumentStore 초기화 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b9d562",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* ✅ DocumentStore 초기화 완료"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f906af6e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c4a753",
   "metadata": {},
   "source": [
    "#### **7) `DataSet 생성하기`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f7f3de",
   "metadata": {},
   "source": [
    "##### **`➀ TestSet Generator` 생성하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17203e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = TestsetGenerator.from_langchain(\n",
    "    generator_llm,\n",
    "    critic_llm ,\n",
    "    ragas_embeddings,\n",
    "    docstore=docstore,\n",
    ")\n",
    "\n",
    "print(\"✅ Generator 생성 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b579bba3",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* ✅ Generator 생성 완료"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb88606",
   "metadata": {},
   "source": [
    "##### **`➁ 질문 분포` 설정하기**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b879c42",
   "metadata": {},
   "source": [
    "* **`질문의 유형별 분포`**\n",
    "\n",
    "  * `simple`: 간단한 질물\n",
    "\n",
    "  * `reasoning`: `추론`이 필요한 질문\n",
    "\n",
    "  * `multi_context`: `여러 맥락`을 고려해야 하는 질문\n",
    "\n",
    "  * `conditional`: `조건부 질문`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239ec3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문 유형별 분포 결정\n",
    "\n",
    "distributions = {\n",
    "    simple: 0.4,            # simple: 간단한 질문\n",
    "    reasoning: 0.2,         # reasoning: 추론이 필요한 질문\n",
    "    multi_context: 0.2,     # multi_context: 여러 맥락을 고려해야 하는 질문\n",
    "    conditional: 0.2        # conditional: 조건부 질문\n",
    "    }\n",
    "\n",
    "print(\"✅ 질문 분포 설정 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48df3dd6",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* ✅ 질문 분포 설정 완료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd26fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(distributions))              # <class 'dict'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eebf996",
   "metadata": {},
   "source": [
    "##### **`➂ 데이터셋` 생성하기**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9066cd",
   "metadata": {},
   "source": [
    "* **`테스트셋 생성`**\n",
    "\n",
    "  * `documents`: `문서 데이터`\n",
    "\n",
    "  * `test_size`: `생성`할 `질문의 수`\n",
    "\n",
    "  * `distributions`: 질문 `유형별 분포`\n",
    "\n",
    "  * `with_debugging_logs`: `디버깅 로그 출력 여부`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db2d4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"🔄 테스트셋 생성 시작\")\n",
    "print(\"=\"*60)\n",
    "print(\"🔄 테스트셋 생성 중... (시간이 걸릴 수 있습니다)\")\n",
    "print(\"=\"*60)\n",
    "print(\"🔄 멈춰 보여도 정상입니다! 🔄 \")\n",
    "print(\"=\"*60)\n",
    "print(\"⏰ 예상 시간: 20-30분\")\n",
    "print(\"💡 멈춘 것처럼 보여도 정상입니다!\")\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    testset = generator.generate_with_langchain_docs(\n",
    "        documents=docs,\n",
    "        test_size=3,                            # 더 작게 설정\n",
    "        distributions=distributions,\n",
    "        with_debugging_logs=False,              # 로그 비활성화\n",
    "        raise_exceptions=False,                 # 에러 무시\n",
    "    )\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"\\n✅ 완료! 소요 시간: {elapsed/60:.1f}분\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ 에러: {e}\")\n",
    "    print(\"💡 Ollama가 실행 중인지 확인하세요!\")\n",
    "    print(\"\\n💡 해결 방법:\")\n",
    "    print(\"    1. Ollama가 실행 중인지 확인\")\n",
    "    print(\"    2. 모델 다운로드: ollama pull llama3.2:3b\")\n",
    "    print(\"    3. langchain-ollama 버전 확인: pip list | grep langchain-ollama\")\n",
    "    print(\"        → 0.1.3이어야 함!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b05106",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "```bash\n",
    "\n",
    "    ============================================================\n",
    "    🔄 테스트셋 생성 시작\n",
    "    ============================================================\n",
    "    🔄 테스트셋 생성 중... (시간이 걸릴 수 있습니다)\n",
    "    ============================================================\n",
    "    🔄 멈춰 보여도 정상입니다! 🔄 \n",
    "    ============================================================\n",
    "    ⏰ 예상 시간: 20-30분\n",
    "    💡 멈춘 것처럼 보여도 정상입니다!\n",
    "\n",
    "    Generating: 100%|████████████████████████████████████████| 4/4 [48:31<00:00, 565.54s/it]\n",
    "\n",
    "    Failed to parse output. Returning None.\n",
    "\n",
    "    ✅ 완료! 소요 시간: 74.7분\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf18cc7",
   "metadata": {},
   "source": [
    "##### **`➃ 결과 저장하기`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1b9c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 생성된 테스트셋을 pandas DataFrame으로 변환\n",
    "test_df = testset.to_pandas()\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e426953",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* `test_df`\n",
    "\n",
    "  * ![test_df](../15_Evaluations/assets/test_df.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa10a039",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad91bb27",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* `test_df`\n",
    "\n",
    "  * ![test_df.head](../15_Evaluations/assets/test_df_head.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262a96f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"생성된 질문 수: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f904e560",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 생성된 질문 수: 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972dd4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv(\"../15_Evaluations/data/ragas_synthetic_dataset_3.csv\", index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(\"✅ 저장: data/ragas_synthetic_dataset_3.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a655da43",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* ✅ 저장: data/ragas_synthetic_dataset_3.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fc4725",
   "metadata": {},
   "source": [
    "##### **`➄ 결과 확인하기`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dcaf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 미리보기\n",
    "\n",
    "if len(test_df) > 0:\n",
    "    \n",
    "    print()\n",
    "    print(\"=\"*60)\n",
    "    print(\"✅✅✅ 성공! ✅✅✅\")\n",
    "    print(\"=\"*60)\n",
    "    print()\n",
    "        \n",
    "    for idx, row in test_df.iterrows():\n",
    "        print(f\"\\n질문 {idx+1}: \")\n",
    "        print(f\"    {row['question']}\")\n",
    "        print(f\"\\n답변 {idx+2}: \")\n",
    "        print(f\"    {row['ground_truth'][:100]}...\")\n",
    "        print()\n",
    "        print(\"✅ 저장: data/ragas_synthetic_dataset_3.csv\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ 질문 생성 실패\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de6416e",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* **`생성된 질문보기`**\n",
    "\n",
    "    ```bash\n",
    "\n",
    "    ============================================================\n",
    "    ✅✅✅ 성공! ✅✅✅\n",
    "    ============================================================\n",
    "\n",
    "\n",
    "    질문 1: \n",
    "        Here is a question that can be fully answered from the given context:\n",
    "\n",
    "    \"Who developed the model RAG \\ud3ec\\ud568 \\uc9c8\\ubb38\\uacfc \\ub2f5\\ubcc0\\uc5d0\\uc11c\\ub294 \\ud5c8\\uae45, and what is its relation to LLM?\"\n",
    "\n",
    "    Note that this question can be answered by reading the context provided.\n",
    "\n",
    "    답변 2: \n",
    "        Who developed the model RAG 포함 질문과 답변에서는 \\ud5c8\\uae45, and what is its relation to LLM?...\n",
    "\n",
    "    ✅ 저장: data/ragas_synthetic_dataset_3.csv\n",
    "\n",
    "    질문 2: \n",
    "        Here is a rewritten version of the question:\n",
    "\n",
    "    \"What technology is CCnet 2024 based on?\"\n",
    "\n",
    "    This version still conveys the same meaning as the original question, but in a more concise and indirect way.\n",
    "\n",
    "    답변 3: \n",
    "        nan...\n",
    "\n",
    "    ✅ 저장: data/ragas_synthetic_dataset_3.csv\n",
    "\n",
    "    질문 3: \n",
    "        Here is a rewritten version of the question that conveys the same meaning in a less direct and shorter manner:\n",
    "\n",
    "    \"What specific aspects of UL2 training are targeted by CES 2024's AI focus, and how do they align with the OpenMoE framework?\"\n",
    "\n",
    "    I made the following changes to achieve this:\n",
    "\n",
    "    * Abbreviated \"OpenMoE framework\" to \"OpenMoE\"\n",
    "    * Changed \"specific aspects of UL2 training objective's configuration\" to \"specific aspects of UL2 training\"\n",
    "    * Combined \"within the framework\" into the question itself\n",
    "    * Simplified the wording and sentence structure\n",
    "\n",
    "    답변 4: \n",
    "        The answer to given question is not present in context...\n",
    "\n",
    "    ✅ 저장: data/ragas_synthetic_dataset_3.csv\n",
    "\n",
    "    질문 4: \n",
    "        Here's a rewritten version of the question that conveys the same meaning in a less direct and shorter manner:\n",
    "\n",
    "    \"What triggers EU AI regulation for developers?\"\n",
    "\n",
    "    Alternatively, you could also ask:\n",
    "\n",
    "    \"When must EU AI regulation be applied by developers?\n",
    "\n",
    "    답변 5: \n",
    "        nan...\n",
    "\n",
    "    ✅ 저장: data/ragas_synthetic_dataset_3.csv\n",
    "    \n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a4e926",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc70db0e",
   "metadata": {},
   "source": [
    "### **3. `주피터 노트북`으로 시도** - *`try_2`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33256832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 셀 1: 환경 설정\n",
    "# ============================================\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"TQDM_DISABLE\"] = \"1\"  # 진행바 비활성화\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(\"✅ 환경 설정 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509e2a78",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* ✅ 환경 설정 완료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c499b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 셀 2: 패키지 임포트 & 확인\n",
    "# ============================================\n",
    "\n",
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, conditional\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from ragas.testset.extractor import KeyphraseExtractor\n",
    "from ragas.testset.docstore import InMemoryDocumentStore\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "print(\"✅ 패키지 임포트 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c996c23d",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* ✅ 패키지 임포트 완료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb64efb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 셀 3: LLM 설정 (확인 가능!)\n",
    "# ============================================\n",
    "\n",
    "generator_llm = ChatOllama(\n",
    "    model=\"llama3.2:3b\",                    # 자연어 모델로 교체\n",
    "    temperature=0.7,\n",
    "    num_predict=512,\n",
    ")\n",
    "\n",
    "# 테스트 호출\n",
    "test_response = generator_llm.invoke(\"Hello\")\n",
    "print(f\"✅ 데이터셋 생성기 LLM 작동 확인: {test_response.content[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5644a9",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* ✅ 데이터셋 생성기 LLM 작동 확인: How can I assist you today?... - (`3.9s`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b3da91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 셀 4: 임베딩 설정 (확인 가능!)\n",
    "# ============================================\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "    model_kwargs={'device': 'cpu'},\n",
    ")\n",
    "\n",
    "# 테스트 임베딩\n",
    "test_embed = embeddings.embed_query(\"테스트\")\n",
    "print(f\"✅ 임베딩 작동 확인: 차원={len(test_embed)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514bb27e",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* ✅ 임베딩 작동 확인: 차원=384 - (`3.2s`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed71e0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 셀 5: 문서 로드 (확인 가능!)\n",
    "# ============================================\n",
    "loader = PDFPlumberLoader(\"data/SPRI_AI_Brief_2023년12월호_F.pdf\")\n",
    "docs = loader.load()[3:-1]\n",
    "\n",
    "for doc in docs:\n",
    "    doc.metadata[\"filename\"] = doc.metadata[\"source\"]\n",
    "\n",
    "print(f\"✅ 문서 로드 완료: {len(docs)}페이지\")\n",
    "print(f\"첫 문서 미리보기: {docs[0].page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f592635",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* ✅ 문서 로드 완료: 19페이지 - (`2.8s`)\n",
    "\n",
    "```markdown\n",
    "\n",
    "    첫 문서 미리보기: 1. 정책/법제 2. 기업/산업 3. 기술/연구 4. 인력/교육\n",
    "    미국, 안전하고 신뢰할 수 있는 AI 개발과 사용에 관한 행정명령 발표\n",
    "    KEY Contents\n",
    "    n 미국 바이든 대통...\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c2bfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 셀 6: DocumentStore 초기화 (확인 가능!)\n",
    "# ============================================\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=600,             # 더 작게!\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "langchain_llm = LangchainLLMWrapper(generator_llm)\n",
    "keyphrase_extractor = KeyphraseExtractor(llm=langchain_llm)\n",
    "ragas_embeddings = LangchainEmbeddingsWrapper(embeddings)\n",
    "\n",
    "docstore = InMemoryDocumentStore(\n",
    "    splitter=splitter,\n",
    "    embeddings=ragas_embeddings,\n",
    "    extractor=keyphrase_extractor,\n",
    ")\n",
    "\n",
    "print(\"✅ DocumentStore 초기화 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0c6e6b",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* ✅ DocumentStore 초기화 완료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de539cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 셀 7: Generator 생성 (확인 가능!)\n",
    "# ============================================\n",
    "generator = TestsetGenerator.from_langchain(\n",
    "    generator_llm,\n",
    "    generator_llm,          # critic도 동일하게\n",
    "    ragas_embeddings,\n",
    "    docstore=docstore,\n",
    ")\n",
    "\n",
    "print(\"✅ Generator 생성 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51da5185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 셀 8: 질문 분포 설정 (multi_context = 불안정)\n",
    "# ============================================\n",
    "distributions = {\n",
    "    simple: 0.6,                # 60%\n",
    "    reasoning: 0.2,             # 20%\n",
    "    conditional: 0.2,           # 20%\n",
    "}\n",
    "\n",
    "print(\"✅ 질문 분포 설정 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a066a39",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* ✅ 질문 분포 설정 완료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb59fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 셀 9: 테스트셋 생성 (시간 소요!)\n",
    "# ============================================\n",
    "print(\"🔄 테스트셋 생성 시작...\")\n",
    "print(\"⏰ 예상 시간: 20-30분\")\n",
    "print(\"💡 멈춘 것처럼 보여도 정상입니다!\")\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    testset = generator.generate_with_langchain_docs(\n",
    "        documents=docs,\n",
    "        test_size=3,                        # 더 작게!\n",
    "        distributions=distributions,\n",
    "        with_debugging_logs=False,\n",
    "        raise_exceptions=False,\n",
    "    )\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"\\n✅ 완료! 소요 시간: {elapsed/60:.1f}분\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ 에러: {e}\")\n",
    "    print(\"💡 Ollama가 실행 중인지 확인하세요!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644fcc2b",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "```bash\n",
    "\n",
    "    🔄 테스트셋 생성 시작...\n",
    "    \n",
    "    ⏰ 예상 시간: 20-30분\n",
    "    \n",
    "    💡 멈춘 것처럼 보여도 정상입니다!\n",
    "\n",
    "\n",
    "    # embedding nodes:   100%|████████████████████████████████████████| 108/108 [27:50<00:00, 25.22s/it]\n",
    "    # ↳ 이후 사라짐\n",
    "    \n",
    "    # Generating:   50%|████████████████████                   | 2/4\n",
    "    # Generating:   75%|██████████████████████████             | 3/4\n",
    "    Generating:   100%|████████████████████████████████████████| 4/4 [41:48<00:00, 505.28s/it]\n",
    "\n",
    "    ✅ 완료! 소요 시간: 69.8분 - (`69m 45.4s`)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99df33d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 셀 10: 결과 확인 & 저장\n",
    "# ============================================\n",
    "test_df2 = testset.to_pandas()\n",
    "print(f\"생성된 질문 수: {len(test_df2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bc4e27",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 생성된 질문 수: 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe03ed23",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575288b6",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* `test_df2`\n",
    "  \n",
    "  * ![test_df2](../15_Evaluations/assets/test_df2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75833f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7bc800",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* `test_df2.head()`\n",
    "  \n",
    "  * ![test_df2.head()](../15_Evaluations/assets/test_df2_head.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bea069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV 저장\n",
    "test_df2.to_csv(\"../15_Evaluations/data/ragas_synthetic_dataset_4.csv\", index=False, encoding='utf-8-sig')\n",
    "print(\"\\n✅ 저장 완료: data/ragas_testset4.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17479131",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* ✅ 저장 완료: data/ragas_testset4.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a2ebd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 미리보기\n",
    "if len(test_df2) > 0:\n",
    "    print()\n",
    "    print(\"=\"*60)\n",
    "    print(\"✅✅✅ 성공! ✅✅✅\")\n",
    "    print(\"=\"*60)\n",
    "    print()\n",
    "\n",
    "    for idx, row in test_df2.iterrows():\n",
    "        print(f\"\\n질문 {idx+1}: \")\n",
    "        print(f\"    {row['question']}\")\n",
    "        print(f\"\\n답변 {idx+2}: \")\n",
    "        print(f\"    {row['ground_truth'][:100]}...\")\n",
    "        print()\n",
    "        print(\"✅ 저장: data/ragas_synthetic_dataset_4.csv\")\n",
    "else:\n",
    "    print(\"❌ 질문 생성 실패\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763e8011",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "```bash\n",
    "\n",
    "    ============================================================\n",
    "    ✅✅✅ 성공! ✅✅✅\n",
    "    ============================================================\n",
    "\n",
    "\n",
    "    질문 1: \n",
    "        Here is a question that can be fully answered from the given context:\n",
    "\n",
    "    \"What is Artificial General Intelligence (AGI) and how does it differ from other types of artificial intelligence?\"\n",
    "\n",
    "    This question is formed using the keyphrase \"Artificial General Intelligence\" which appears throughout the provided text.\n",
    "\n",
    "    답변 2: \n",
    "        Artificial General Intelligence (AGI) refers to a type of AI that possesses human-like intelligence ...\n",
    "\n",
    "    ✅ 저장: data/ragas_synthetic_dataset_4.csv\n",
    "\n",
    "    질문 2: \n",
    "        Here's a question that can be fully answered from the given context:\n",
    "\n",
    "    \"Who is the founder of AI?\"\n",
    "\n",
    "    answer: The context does not mention the founder of AI, but it talks about AI in the context of G7 and its applications. However, based on general knowledge, the answer would be \"Many individuals have contributed to the development of AI, but some notable founders include Andrew Ng, Yann LeCun, Geoffrey Hinton, Yoshua Bengio, and others.\"\n",
    "\n",
    "    Note: The provided text does not contain information about the founder(s) of AI.\n",
    "\n",
    "    답변 3: \n",
    "        The answer to given question is not present in context...\n",
    "\n",
    "    ✅ 저장: data/ragas_synthetic_dataset_4.csv\n",
    "\n",
    "    질문 3: \n",
    "        Here is a rewritten question that conveys the same meaning but in a less direct manner, using abbreviation and being shorter:\n",
    "\n",
    "    \"What's the term for AGI?\"\n",
    "\n",
    "    This rewritten question still asks about the concept of Artificial General Intelligence (AGI), but does so in a more concise and indirect way.\n",
    "\n",
    "    답변 4: \n",
    "        nan...\n",
    "\n",
    "    ✅ 저장: data/ragas_synthetic_dataset_4.csv\n",
    "\n",
    "    질문 4: \n",
    "        Here is a rewritten version of the question that conveys the same meaning but in a less direct and shorter manner:\n",
    "\n",
    "    \"What config. details in OpenMoE define AGI's UL2 objective?\"\n",
    "\n",
    "    Alternatively, you could also use:\n",
    "\n",
    "    \"How do OpenMoE settings impact AGI's UL2 training?\"\n",
    "\n",
    "    Or even more concise:\n",
    "\n",
    "    \"Which OpenMoE settings affect AGI's UL2 objective?\n",
    "\n",
    "    답변 5: \n",
    "        nan...\n",
    "\n",
    "    ✅ 저장: data/ragas_synthetic_dataset_4.csv\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009251c4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7626d2",
   "metadata": {},
   "source": [
    "* next: ***`02. SQL`***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f4de16",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc_eval_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
