{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba52bed0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65001001",
   "metadata": {},
   "source": [
    "* ì¶œì²˜: LangChain ê³µì‹ ë¬¸ì„œ ë˜ëŠ” í•´ë‹¹ êµì¬ëª…\n",
    "* ì›ë³¸ URL: https://smith.langchain.com/hub/teddynote/summary-stuff-documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb2ea90",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6662ac",
   "metadata": {},
   "source": [
    "## **`CH15.` `í‰ê°€`** *(Evaluations)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a0d6f0",
   "metadata": {},
   "source": [
    "* **`LLM`** *(Large Language Model)* í‰ê°€: ì¸ê³µì§€ëŠ¥ `ì–¸ì–´ ëª¨ë¸ì˜ ì„±ëŠ¥`, `ì •í™•ì„±`, `ì¼ê´€ì„±` ë° ê¸°íƒ€ ì¤‘ìš”í•œ ì¸¡ë©´ì„ `ì¸¡ì •`í•˜ê³  `ë¶„ì„`í•˜ëŠ” ê³¼ì •\n",
    "\n",
    "* ëª¨ë¸ì˜ ê°œì„ , ë¹„êµ, ì„ íƒ ë° ì‘ìš© í”„ë¡œê·¸ë¨ì— ì í•©í•œ ëª¨ë¸ ê²°ì •ì— í•„ìˆ˜ì ì¸ ë‹¨ê³„"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80cc512",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587082c6",
   "metadata": {},
   "source": [
    "* **`í‰ê°€ ë°©ë²•`**\n",
    "\n",
    "  * **`ìë™í™”ëœ ë©”íŠ¸ë¦­`**: `BLEU`, `ROUGE`, `METEOR`, `SemScore` ë“±ì˜ ì§€í‘œ ì‚¬ìš©\n",
    "\n",
    "  * **`ì¸ê°„ í‰ê°€`**: ì „ë¬¸ê°€ or í¬ë¼ìš°ë“œ ì†Œì‹± í†µí•œ ì§ì ‘ì  í‰ê°€ â†’ ìˆ˜í–‰\n",
    "\n",
    "  * **`ì‘ì—… ê¸°ë°˜ í‰ê°€`**: íŠ¹ì • ì‘ì—…ì—ì„œì˜ ì„±ëŠ¥ì„ ì¸¡ì •í•¨ \n",
    "\n",
    "  * **`LLM-as-judge`**: ë‹¤ë¥¸ `LLM`ì„ í‰ê°€ìë¡œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c8d745",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219f5f1d",
   "metadata": {},
   "source": [
    "* **`LangChainì—ì„œì˜ Evaluation`** \n",
    "\n",
    "  * *`LangChain`ì€ `LLM`ì˜ ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ í‰ê°€ë¥¼ ìœ„í•œ ë‹¤ì–‘í•œ ë„êµ¬ì™€ í”„ë ˆì„ì›Œí¬ë¥¼ ì œê³µí•¨*\n",
    "\n",
    "    * **`ëª¨ë“ˆí™”ëœ í‰ê°€ ì»´í¬ë„ŒíŠ¸`**: ë‹¤ì–‘í•œ í‰ê°€ë°©ë²• ì‰½ê²Œ êµ¬í˜„ ë° ì¡°í•© ê°€ëŠ¥\n",
    "\n",
    "    * **`Chain í‰ê°€`**: ì „ì²´ `LLM` ì• í”Œë¦¬ì¼€ì´ì…˜ íŒŒì´í”„ë¼ì¸ í‰ê°€\n",
    "\n",
    "    * **`ë°ì´í„°ì…‹ ê¸°ë°˜ í‰ê°€`**: ì‚¬ìš©ì ì •ì˜ ë°ì´í„°ì…‹ ì‚¬ìš© â†’ ëª¨ë¸ í‰ê°€\n",
    "\n",
    "    * **`í‰ê°€ ìí‘œ`**: ì •í™•ì„±, ì¼ê´€ì„±, ê´€ë ¨ì„± ë“± ë‹¤ì–‘í•œ ì§€ë£Œ ì œê³µ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4512cf84",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5633eef2",
   "metadata": {},
   "source": [
    "* **`LLM-as-judge`**\n",
    "\n",
    "  * `LLM-as-judge`: ë‹¤ë¥¸ `LLM`ì˜ ì¶œë ¥ì„ í‰ê°€í•˜ê¸° ìœ„í•´ `LLM`ì„ ì‚¬ìš©í•˜ëŠ” í˜ì‹ ì ì¸ ì ‘ê·¼ ë°©ì‹\n",
    "\n",
    "    * **`ìë™í™”`**: ì¸ê°„ì˜ ê°œì… ì—†ì´ `ëŒ€ê·œëª¨ í‰ê°€ ìˆ˜í–‰ ê°€ëŠ¥`\n",
    "\n",
    "    * **`ì¼ê´€ì„±`**: í‰ê°€ ê¸°ì¤€ â†’ `ì¼ê´€ë˜ê²Œ ì ìš©í•  ìˆ˜ ìˆìŒ`\n",
    "\n",
    "    * **`ìœ ì—°ì„±`**: ë‹¤ì–‘í•œ í‰ê°€ ê¸°ì¤€, ìƒí™©ì— ì ì‘ ê°€ëŠ¥\n",
    "\n",
    "    * **`ë¹„ìš© íš¨ìœ¨ì„±`**: ì¸ê°„ í‰ê°€ìì— ë¹„í•´ ë¹„ìš©ì´ ì ê²Œ ë“¤ ìˆ˜ ìˆìŒ\n",
    "\n",
    "<br>\n",
    "\n",
    "* \n",
    "  * **`LLM-as-judge`ì˜ ì‘ë™ ë°©ì‹**\n",
    "\n",
    "    * **`ì…ë ¥ ì œê³µ`**: í‰ê°€í•  `LLM`ì˜ ì¶œë ¥ â€¢ í‰ê°€ ê¸°ì¤€ ì œê³µ\n",
    "\n",
    "    * **`ë¶„ì„`**: í‰ê°€ì `LLM`ì´ ì œê³µëœ ì¶œë ¥ì„ ë¶„ì„\n",
    "\n",
    "    * **`í‰ê°€`**: ì •ì˜ëœ ê¸°ì¤€ì— ë”°ë¼ ì ìˆ˜ or í”¼ë“œë°± ìƒì„±\n",
    "\n",
    "    * **`ê²°ê³¼ ì§‘ê³„`**: ì—¬ëŸ¬ í‰ê°€ ê²°ê³¼ ì¢…í•© â†’ ìµœì¢… í‰ê°€ ë„ì¶œ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edad7d33",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae144b7b",
   "metadata": {},
   "source": [
    "* **`ì¥ë‹¨ì `**\n",
    "\n",
    "  * **`ì¥ì `**\n",
    "    * ëŒ€ê·œëª¨ í‰ê°€ ê¸°ëŠ¥\n",
    "    * ë¹ ë¥¸ í”¼ë“œë°± ë£¨í”„\n",
    "    * ë‹¤ì–‘í•œ í‰ê°€ ê¸°ì¤€ ì ìš© ê°€ëŠ¥ \n",
    "\n",
    "  * **`ë‹¨ì `**\n",
    "    * í‰ê°€ì `LLM`ì˜ í¸í–¥ ê°€ëŠ¥ì„±\n",
    "    * ë³µì¡ or ë¯¸ë¬˜í•œ í‰ê°€ì— í•œê³„ê°€ ìˆì„ ìˆ˜ ìˆìŒ\n",
    "    * í‰ê°€ì `LLM`ì˜ ì„±ëŠ¥ì— ì˜ì¡´ì "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe64470e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8c3b71",
   "metadata": {},
   "source": [
    "* **`í‰ê°€ì˜ ì¤‘ìš”ì„±`**\n",
    "\n",
    "  * **`ëª¨ë¸ ê°œì„ `**: ì•½ì  ì‹ë³„ â†’ ê°œì„  ë°©í–¥ ì œì‹œí•¨\n",
    "\n",
    "  * **`ì‹ ë¢°ì„± í™•ë³´`**: ëª¨ë¸ì˜ ì„±ëŠ¥ â€¢ í•œê³„ ì´í•´ â†’ ë„ì›€ì„ ì¤Œ\n",
    "\n",
    "  * **`ì í•©í•œ ëª¨ë¸ ì„ íƒ`**: íŠ¹ì • ì§ì—… or ë„ë©”ì¸ì— ê°€ì¥ ì í•©í•œ ëª¨ë¸ì„ ì„ íƒ ê°€ëŠ¥\n",
    "\n",
    "  * **`ìœ¤ë¦¬ì  ê³ ë ¤ì‚¬í•­`**: í¸í–¥, ê³µì •ì„± ë“±ì˜ ìœ¤ë¦¬ì  ì¸¡ë©´ í‰ê°€ ê°€ëŠ¥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c39f53",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb18b49c",
   "metadata": {},
   "source": [
    "### **1. `í•©ì„± í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„±`** *(RAGAS)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b7758e",
   "metadata": {},
   "source": [
    "* ì¦ì€ ì—ëŸ¬ ë°œìƒ\n",
    "\n",
    "  * â†’ `Jupyter Notebook`ì˜ `tqdm` ì§„í–‰ë°”\n",
    "\n",
    "  * â†’ `ipywidgets` + `ContextVar` ì¶©ëŒ\n",
    "\n",
    "  * â†’ `ë©€í‹°ìŠ¤ë ˆë”©` í™˜ê²½ ë¬¸ì œ\n",
    "\n",
    "* **`â¡ï¸` `Python ìŠ¤í¬ë¦½íŠ¸`ë¡œ ì‹¤í–‰í•˜ê¸°** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee44df4a",
   "metadata": {},
   "source": [
    "##### **[`â€ 01_Test-Dataset-Generator-RAGAS_1.py`](../15_Evaluations/01_Test-Dataset-Generator-RAGAS_1.py)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59d87ba",
   "metadata": {},
   "source": [
    "* ì½”ë“œ\n",
    "\n",
    "<small> <small>\n",
    "\n",
    "```python\n",
    "\n",
    "        # 01_Test-Dataset-Generator-RAGAS_1.py\n",
    "\n",
    "        \"\"\"\n",
    "        llama3.2:3b êµì²´ ë° ê°„ë‹¨ í…ŒìŠ¤íŠ¸ìš©\n",
    "        ì†Œìš” ì‹œê°„: 5-8ë¶„\n",
    "        \"\"\"\n",
    "        import os\n",
    "        import warnings\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "        from dotenv import load_dotenv\n",
    "        load_dotenv()\n",
    "\n",
    "        print(\"=\"*60)\n",
    "        print(\"llama3.2:3b ëª¨ë¸ RAGAS ê°„ë‹¨ í…ŒìŠ¤íŠ¸\")\n",
    "        print(\"=\"*60)\n",
    "        print()\n",
    "\n",
    "        # íŒ¨í‚¤ì§€ ì„í¬íŠ¸\n",
    "        from ragas.testset.generator import TestsetGenerator\n",
    "        from ragas.testset.evolutions import simple\n",
    "        from ragas.llms import LangchainLLMWrapper\n",
    "        from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "        from ragas.testset.extractor import KeyphraseExtractor\n",
    "        from ragas.testset.docstore import InMemoryDocumentStore\n",
    "\n",
    "        from langchain_ollama import ChatOllama\n",
    "        from langchain_huggingface import HuggingFaceEmbeddings\n",
    "        from langchain_community.document_loaders import PDFPlumberLoader\n",
    "        from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "        # ============================================\n",
    "        # 1. LLM ì„¤ì •\n",
    "        # ============================================\n",
    "        print(\"1. LLM ì„¤ì • ì¤‘...\")\n",
    "\n",
    "        generator_llm = ChatOllama(\n",
    "            model=\"llama3.2:3b\",                    # ìì—°ì–´ ëª¨ë¸ë¡œ êµì²´\n",
    "            temperature=0.7,\n",
    "            num_predict=512,\n",
    "        )\n",
    "\n",
    "        # ê°„ë‹¨ í…ŒìŠ¤íŠ¸\n",
    "        try:\n",
    "            test = generator_llm.invoke(\"Say hello\")\n",
    "            print(f\"    âœ… LLM ì‘ë™: {test.content[:30]}...\")\n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ ì—ëŸ¬: {e}\")\n",
    "            print(\"   ğŸ’¡ í™•ì¸: ollama list\")\n",
    "            print(\"   ğŸ’¡ ë‹¤ìš´ë¡œë“œ: ollama pull llama3.2:3b\")\n",
    "            exit(1)\n",
    "\n",
    "        print()\n",
    "\n",
    "        # ============================================\n",
    "        # 2. Embeddings\n",
    "        # ============================================\n",
    "        print(\"2. Embeddings ì„¤ì • ì¤‘...\")\n",
    "        embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "            model_kwargs={'device': 'cpu'},\n",
    "        )\n",
    "        print(\"    âœ… ì™„ë£Œ\")\n",
    "        print()\n",
    "\n",
    "        # ============================================\n",
    "        # 3. ë¬¸ì„œ ë¡œë“œ (3í˜ì´ì§€ë§Œ!)\n",
    "        # ============================================\n",
    "        print(\"3. ë¬¸ì„œ ë¡œë“œ ì¤‘...\")\n",
    "        loader = PDFPlumberLoader(\"../data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf\")\n",
    "        docs = loader.load()[3:6]                       # 3í˜ì´ì§€ë§Œ\n",
    "\n",
    "        for doc in docs:\n",
    "            doc.metadata[\"filename\"] = doc.metadata[\"source\"]\n",
    "\n",
    "        print(f\"    âœ… {len(docs)}í˜ì´ì§€ ë¡œë“œ\")\n",
    "        print()\n",
    "\n",
    "        # ============================================\n",
    "        # 4. DocumentStore\n",
    "        # ============================================\n",
    "        print(\"4. DocumentStore ì´ˆê¸°í™” ì¤‘...\")\n",
    "\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=300,             # ì‚¬ì´ì¦ˆ ê°ì†Œ\n",
    "            chunk_overlap=30            # ì˜¤ë²„ë© ê°ì†Œ\n",
    "        )\n",
    "\n",
    "        langchain_llm = LangchainLLMWrapper(generator_llm)\n",
    "        keyphrase_extractor = KeyphraseExtractor(llm=langchain_llm)\n",
    "        ragas_embeddings = LangchainEmbeddingsWrapper(embeddings)\n",
    "\n",
    "        docstore = InMemoryDocumentStore(\n",
    "            splitter=splitter,\n",
    "            embeddings=ragas_embeddings,\n",
    "            extractor=keyphrase_extractor,\n",
    "        )\n",
    "\n",
    "        print(\"    âœ… ì™„ë£Œ\")\n",
    "        print()\n",
    "\n",
    "        # ============================================\n",
    "        # 5. Generator\n",
    "        # ============================================\n",
    "        print(\"5. Generator ìƒì„± ì¤‘...\")\n",
    "\n",
    "        generator = TestsetGenerator.from_langchain(\n",
    "            generator_llm,\n",
    "            generator_llm,\n",
    "            ragas_embeddings,\n",
    "            docstore=docstore,\n",
    "        )\n",
    "\n",
    "        print(\"    âœ… ì™„ë£Œ\")\n",
    "        print()\n",
    "\n",
    "        # ============================================\n",
    "        # 6. ë¶„í¬\n",
    "        # ============================================\n",
    "        distributions = {\n",
    "            simple: 1.0,  # 100% ê°„ë‹¨í•œ ì§ˆë¬¸\n",
    "        }\n",
    "\n",
    "        # ============================================\n",
    "        # 7. ìƒì„± (1ê°œë§Œ!)\n",
    "        # ============================================\n",
    "        print(\"=\"*60)\n",
    "        print(\"ğŸ”„ í…ŒìŠ¤íŠ¸ì…‹ ìƒì„± ì‹œì‘\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"ì˜ˆìƒ ì‹œê°„: 5-8ë¶„\")\n",
    "        print(\"ë©ˆì¶° ë³´ì—¬ë„ ì •ìƒì…ë‹ˆë‹¤!\")\n",
    "        print()\n",
    "\n",
    "        try:\n",
    "            testset = generator.generate_with_langchain_docs(\n",
    "                documents=docs,\n",
    "                test_size=1,                        # 1ê°œë§Œ!\n",
    "                distributions=distributions,\n",
    "                with_debugging_logs=False,\n",
    "                raise_exceptions=True,\n",
    "            )\n",
    "            \n",
    "            # ============================================\n",
    "            # 8. ê²°ê³¼\n",
    "            # ============================================\n",
    "            test_df = testset.to_pandas()\n",
    "            \n",
    "            if len(test_df) > 0:\n",
    "                print()\n",
    "                print(\"=\"*60)\n",
    "                print(\"âœ…âœ…âœ… ì„±ê³µ! âœ…âœ…âœ…\")\n",
    "                print(\"=\"*60)\n",
    "                print()\n",
    "                \n",
    "                for idx, row in test_df.iterrows():\n",
    "                    print(f\"ì§ˆë¬¸: {row['question']}\")\n",
    "                    print(f\"ë‹µë³€: {row['ground_truth'][:80]}...\")\n",
    "                    print()\n",
    "                \n",
    "                test_df.to_csv(\"data/.csv\", index=False)\n",
    "                # test_df.to_csv(\"data/ragas_synthetic_dataset_1.csv\", index=False, encoding='utf-8-sig')\n",
    "                # í˜¹ì€ ë°ì´í„° ê²½ë¡œ: \"../data/agas_synthetic_dataset_1.csv\"\n",
    "                print(\"âœ… ì €ì¥: data/jay_success.csv\")\n",
    "\n",
    "                \n",
    "            else:\n",
    "                print(\"âŒ ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print()\n",
    "            print(\"âŒ ì—ëŸ¬:\")\n",
    "            print(f\"    {e}\")\n",
    "            print()\n",
    "            print(\"ğŸ’¡ ë‚˜ì—ê²Œ ì—ëŸ¬ ë©”ì‹œì§€ ë³´ë‚´ì¤˜!\")\n",
    "\n",
    "        print()\n",
    "        print(\"=\"*60)\n",
    "        print(\"âœ… í…ŒìŠ¤íŠ¸ì…‹ ìƒì„± ì™„ë£Œ\")\n",
    "        print(\"=\"*60)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f86f43",
   "metadata": {},
   "source": [
    "* ê²°ê³¼\n",
    "\n",
    "<small> <small>\n",
    "\n",
    "```bash\n",
    "\n",
    "    ============================================================\n",
    "    llama3.2:3b ëª¨ë¸ RAGAS ê°„ë‹¨ í…ŒìŠ¤íŠ¸\n",
    "    ============================================================\n",
    "\n",
    "    1. LLM ì„¤ì • ì¤‘...\n",
    "        âœ… LLM ì‘ë™: Hello! It's nice to meet you. ...\n",
    "\n",
    "    2. Embeddings ì„¤ì • ì¤‘...\n",
    "        âœ… ì™„ë£Œ\n",
    "\n",
    "    3. ë¬¸ì„œ ë¡œë“œ ì¤‘...\n",
    "        âœ… 3í˜ì´ì§€ ë¡œë“œ\n",
    "\n",
    "    4. DocumentStore ì´ˆê¸°í™” ì¤‘...\n",
    "        âœ… ì™„ë£Œ\n",
    "\n",
    "    5. Generator ìƒì„± ì¤‘...\n",
    "        âœ… ì™„ë£Œ\n",
    "\n",
    "    ============================================================\n",
    "    ğŸ”„ í…ŒìŠ¤íŠ¸ì…‹ ìƒì„± ì‹œì‘\n",
    "    ============================================================\n",
    "    ì˜ˆìƒ ì‹œê°„: 5-8ë¶„\n",
    "    ë©ˆì¶° ë³´ì—¬ë„ ì •ìƒì…ë‹ˆë‹¤!\n",
    "\n",
    "    # embedding nodes:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 34/36 [04:09<00:39, 19.87s/it]\n",
    "\n",
    "    # Generating:   0%|                                                                                                    | 0/1 [00:00<?, ?it/s]\n",
    "\n",
    "    ============================================================\n",
    "    âœ…âœ…âœ… ì„±ê³µ! âœ…âœ…âœ…\n",
    "    ============================================================\n",
    "\n",
    "    ì§ˆë¬¸: context: \"n \\uc8fc\\uc694 7\\uac1c\\uad6d(G7)*\\uc740 2023\\ub144 10\\uc6d4 30\\uc77c \\u201â€™\\ud788\\ub85c\\uc2dc\\ub9c8 AI \\ud504\\ub85c\\uc138\\uc2a4â€™s\\ud97c \\ud1b5\\ud574 AI \\uae30\\uc5c5 \\ub300\\uc0c1\\uc758 AI \\uad6d\\uc81c\\n\\ud589\\ub3d9\\uac15\\ub839(International Code of Conduct for Advanced AI Systems)\\uc5d0 \\ud569\\uc758\\n\\u2219 G7\\uc740 2023\\ub144 5\\uc6d4 \\uc77c\\ubcf8 \\ud788\\ub85c\\uc2dc\\ub9c8\\uc5d0\\uc11c \\uac1c\\ucd5c\\ub41c \\uc815\\uc0c1\\ud68c\\uc758\\uc5d0\\uc11c \\uc0dd\\uc131 AI\\uc5d0 \\uad00\\ud55c \\uad6d\\uc81c\\uaddc\\ubc94 \\ub9c8\\ub828\\acfc\\n\\uc815\\ubcf4\\uacf5\\uc720\\ub97c \\uc704\\ud574 \\u201â€™\\ud788\\ub85c\\uc2dc\\ub9c8 AI \\ud504\\ub85c\\uc138\\uc2a4â€™s\\ud97c \\ucd9c\\ubc94**\\n\\u2219 \\uae30\\uc5c5\\uc758 \\uc790\\ubc1c\\uc801 \\ucc44\\ud0dd\\uc744 \\uc704\\ud574 \\ub9c8\\ub828\\ub41c \\uc774\\ubc88 \\ud589\\ub3d9\\uac15\\ub839\\uc740 \\uae30\\ubc18\\ubaa8\\ub378\\acfc \\uc0dd\\uc131 AI\\ud97c \\ud3ec\\ud568\\ud55c \\ucca8\\ub2e8 AI \\uc2dc\\uc2a4\\ud15c\\uc758\\n\\uc704\\ud5d8 \\uc2dd\\ubcc4\\uacfc \\uc644\\ud654\\uc5d\n",
    "    ë‹µë³€: The answer to given question is not present in context...\n",
    "\n",
    "\n",
    "    âŒ ì—ëŸ¬:\n",
    "        Cannot save file into a non-existent directory: 'data'\n",
    "\n",
    "    ğŸ’¡ ë‚˜ì—ê²Œ ì—ëŸ¬ ë©”ì‹œì§€ ë³´ë‚´ì¤˜!\n",
    "\n",
    "    ============================================================\n",
    "    âœ… í…ŒìŠ¤íŠ¸ì…‹ ìƒì„± ì™„ë£Œ\n",
    "    ============================================================\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9b6b9b",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* **`../15_Evaluations/data/ragas_synthetic_dataset_1.csv`ìœ¼ë¡œ ì €ì¥ ì‹¤íŒ¨**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ba33eb",
   "metadata": {},
   "source": [
    "##### **[`â 01_Test-Dataset-Generator-RAGAS_2.py`](../15_Evaluations/01_Test-Dataset-Generator-RAGAS_2.py)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2f0c90",
   "metadata": {},
   "source": [
    "* ì½”ë“œ\n",
    "\n",
    "<small> <small>\n",
    "\n",
    "```python \n",
    "\n",
    "        # 01_Test-Dataset-Generator-RAGAS_2.py\n",
    "\n",
    "        \"\"\"\n",
    "        RAGAS í…ŒìŠ¤íŠ¸ì…‹ ìƒì„± - ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©\n",
    "        \"\"\"\n",
    "\n",
    "        import os\n",
    "        import warnings\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "        from dotenv import load_dotenv\n",
    "        load_dotenv()\n",
    "\n",
    "        print(\"=\"*60)\n",
    "        print(\"llama3.2:3b ëª¨ë¸ RAGAS ê°„ë‹¨ í…ŒìŠ¤íŠ¸\")\n",
    "        print(\"=\"*60)\n",
    "        print()\n",
    "\n",
    "        print(\"ğŸ”§ RAGAS ì´ˆê¸°í™” ì¤‘...\\n\")\n",
    "\n",
    "        from ragas.testset.generator import TestsetGenerator\n",
    "        from ragas.testset.evolutions import simple, reasoning\n",
    "        from ragas.llms import LangchainLLMWrapper\n",
    "        from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "        from ragas.testset.extractor import KeyphraseExtractor\n",
    "        from ragas.testset.docstore import InMemoryDocumentStore\n",
    "\n",
    "        from langchain_ollama import ChatOllama\n",
    "        from langchain_huggingface import HuggingFaceEmbeddings\n",
    "        from langchain_community.document_loaders import PDFPlumberLoader\n",
    "        from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "        # 1. LLM ì„¤ì •\n",
    "        print(\"1. LLM ì„¤ì • ì¤‘...\")\n",
    "\n",
    "        generator_llm = ChatOllama(\n",
    "            model=\"llama3.2:3b\",                    # ìì—°ì–´ ëª¨ë¸ë¡œ êµì²´\n",
    "            temperature=0.7,\n",
    "            num_predict=512,\n",
    "        )\n",
    "\n",
    "        critic_llm = ChatOllama(\n",
    "            model=\"llama3.2:3b\",\n",
    "            temperature=0.1,\n",
    "            num_predict=512,\n",
    "        )\n",
    "\n",
    "        # ê°„ë‹¨ í…ŒìŠ¤íŠ¸\n",
    "        try:\n",
    "            test = generator_llm.invoke(\"Say hello\")\n",
    "            print(f\"    âœ… LLM ì‘ë™: {test.content[:30]}...\")\n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ ì—ëŸ¬: {e}\")\n",
    "            print(\"   ğŸ’¡ í™•ì¸: ollama list\")\n",
    "            print(\"   ğŸ’¡ ë‹¤ìš´ë¡œë“œ: ollama pull llama3.2:3b\")\n",
    "            exit(1)\n",
    "\n",
    "\n",
    "        print(\"    âœ… Ollama LLM ì„¤ì • ì™„ë£Œ\")\n",
    "        print()\n",
    "\n",
    "\n",
    "        # 2. Embeddings ì„¤ì •\n",
    "        print(\"2. Embeddings ì„¤ì • ì¤‘...\")\n",
    "\n",
    "        embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "            model_kwargs={'device': 'cpu'},\n",
    "            encode_kwargs={'normalize_embeddings': True}\n",
    "        )\n",
    "\n",
    "        print(\"    âœ… ì™„ë£Œ\")\n",
    "        print()\n",
    "\n",
    "\n",
    "        # 3. ë¬¸ì„œ ë¡œë“œ\n",
    "        print(\"3. ë¬¸ì„œ ë¡œë“œ ì¤‘...\")\n",
    "        loader = PDFPlumberLoader(\"data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf\")\n",
    "        docs = loader.load()\n",
    "        docs = docs[3:8]                            # 5í˜ì´ì§€ë§Œ!\n",
    "\n",
    "        # metadata ì„¤ì •\n",
    "        for doc in docs:\n",
    "            doc.metadata[\"filename\"] = doc.metadata[\"source\"]\n",
    "\n",
    "        print(f\"    âœ… {len(docs)}í˜ì´ì§€ ë¡œë“œ\")\n",
    "        print()\n",
    "\n",
    "\n",
    "        # 4. DocumentStore ì´ˆê¸°í™”\n",
    "        print(\"4. DocumentStore ì´ˆê¸°í™” ì¤‘...\")\n",
    "\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            #chunk_size=800,            # ì¡°ê¸ˆ ì¤„ì„\n",
    "            #chunk_size=400,            # ë” ì‘ê²Œ ì¡°ì ˆ\n",
    "            #chunk_overlap=50 \n",
    "            chunk_size=300,             # ì‚¬ì´ì¦ˆ ê°ì†Œ\n",
    "            chunk_overlap=30            # ì˜¤ë²„ë© ê°ì†Œ\n",
    "        )\n",
    "\n",
    "        langchain_llm = LangchainLLMWrapper(generator_llm)\n",
    "        keyphrase_extractor = KeyphraseExtractor(llm=langchain_llm)\n",
    "        ragas_embeddings = LangchainEmbeddingsWrapper(embeddings)\n",
    "\n",
    "        docstore = InMemoryDocumentStore(\n",
    "            splitter=splitter,\n",
    "            embeddings=ragas_embeddings,\n",
    "            extractor=keyphrase_extractor,\n",
    "        )\n",
    "\n",
    "        print(\"    âœ… ì™„ë£Œ\")\n",
    "        print()\n",
    "\n",
    "\n",
    "        # 5. TestsetGenerator ìƒì„±\n",
    "        print(\"5. Generator ìƒì„± ì¤‘...\")\n",
    "\n",
    "        generator = TestsetGenerator.from_langchain(\n",
    "            generator_llm,\n",
    "            critic_llm,\n",
    "            ragas_embeddings,\n",
    "            docstore=docstore,\n",
    "        )\n",
    "\n",
    "        print(\"    âœ… ì™„ë£Œ\")\n",
    "        print()\n",
    "\n",
    "\n",
    "        # 6. ë¶„í¬ (ê°„ë‹¨í•˜ê²Œ!)\n",
    "        distributions = {\n",
    "            simple: 0.7,      # 70%\n",
    "            reasoning: 0.3,   # 30%\n",
    "        }\n",
    "\n",
    "\n",
    "        # 7. ìƒì„± (2ê°œë§Œ!)\n",
    "        print(\"=\"*60)\n",
    "        print(\"ğŸ”„ í…ŒìŠ¤íŠ¸ì…‹ ìƒì„± ì‹œì‘\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"ğŸ”„ í…ŒìŠ¤íŠ¸ì…‹ ìƒì„± ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)\")\n",
    "        print(\"    - ë¶„í¬: ê°„ë‹¨(70%), ì¶”ë¡ (30%)\\n\")\n",
    "        print(\"ğŸ”„ ë©ˆì¶° ë³´ì—¬ë„ ì •ìƒì…ë‹ˆë‹¤! ğŸ”„ \")\n",
    "        print(\"=\"*60)\n",
    "        print()\n",
    "\n",
    "        try:\n",
    "            testset = generator.generate_with_langchain_docs(\n",
    "                documents=docs,\n",
    "                test_size=2,                        # 2ê°œë§Œ\n",
    "                distributions=distributions,\n",
    "                with_debugging_logs=False,          # ë¡œê·¸ ë¹„í™œì„±í™”\n",
    "                raise_exceptions=False,             # ì—ëŸ¬ ë¬´ì‹œ\n",
    "            )\n",
    "            \n",
    "            # 8. ê²°ê³¼ ì €ì¥\n",
    "            test_df = testset.to_pandas()\n",
    "            test_df.to_csv(\"data/ragas_synthetic_dataset_2.csv\", index=False, encoding='utf-8-sig')\n",
    "            \n",
    "            if len(test_df) > 0:\n",
    "                print()\n",
    "                print(\"=\"*60)\n",
    "                print(\"âœ…âœ…âœ… ì„±ê³µ! âœ…âœ…âœ…\")\n",
    "                print(\"=\"*60)\n",
    "                print()\n",
    "                \n",
    "                for idx, row in test_df.iterrows():\n",
    "                    print(f\"ì§ˆë¬¸: {row['question']}\")\n",
    "                    print(f\"ë‹µë³€: {row['ground_truth'][:80]}...\")\n",
    "                    print()\n",
    "\n",
    "                print(\"âœ… ì €ì¥: data/ragas_synthetic_dataset_2.csv\")\n",
    "\n",
    "            else:\n",
    "                print(\"âŒ ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨\")\n",
    "\n",
    "            print()\n",
    "            print(\"=\"*60)  \n",
    "            print(f\"\\nâœ… í…ŒìŠ¤íŠ¸ì…‹ ìƒì„± ì™„ë£Œ!\")\n",
    "            print(f\"   - ìƒì„±ëœ ì§ˆë¬¸ ìˆ˜: {len(test_df)}\")\n",
    "            print(f\"   - ì €ì¥ ìœ„ì¹˜: ../15_Evaluations/data/ragas_synthetic_dataset.csv\\n\")\n",
    "            print(test_df)\n",
    "            \n",
    "            # 9. ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°\n",
    "            print(\"ğŸ“Š ìƒì„±ëœ ì§ˆë¬¸ ë¯¸ë¦¬ë³´ê¸°:\")\n",
    "            print(\"=\"*80)\n",
    "            for idx, row in test_df.iterrows():\n",
    "                print(f\"\\nì§ˆë¬¸ {idx+1}:\")\n",
    "                print(f\"  {row['question']}\")\n",
    "                print(f\"  Ground Truth: {row['ground_truth'][:100]}...\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ì—ëŸ¬ ë°œìƒ: {e}\")\n",
    "            print(\"\\nğŸ’¡ í•´ê²° ë°©ë²•:\")\n",
    "            print(\"    1. Ollamaê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸\")\n",
    "            print(\"    2. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ: ollama pull llama3.2:3b\")\n",
    "            print(\"    3. langchain-ollama ë²„ì „ í™•ì¸: pip list | grep langchain-ollama\")\n",
    "            print(\"        â†’ 0.1.3ì´ì–´ì•¼ í•¨!\")\n",
    "\n",
    "        print(\"\\nâœ… ì •ìƒì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ì…‹ ìƒì„± ì™„ë£Œ!\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37be1571",
   "metadata": {},
   "source": [
    "* ê²°ê³¼\n",
    "\n",
    "  * try_1\n",
    "\n",
    "<small> <small>\n",
    "\n",
    "```bash\n",
    "\n",
    "        ğŸ”§ RAGAS ì´ˆê¸°í™” ì¤‘...\n",
    "\n",
    "        âœ… Ollama LLM ì„¤ì • ì™„ë£Œ\n",
    "        âœ… Embeddings ì„¤ì • ì™„ë£Œ\n",
    "        âœ… ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ: 5í˜ì´ì§€\n",
    "\n",
    "        âœ… TestsetGenerator ìƒì„± ì™„ë£Œ\n",
    "\n",
    "        ğŸ”„ ì‹œì‘... (10ë¶„ ì˜ˆìƒ)\n",
    "        ğŸ”„ í…ŒìŠ¤íŠ¸ì…‹ ìƒì„± ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)\n",
    "            - ë¶„í¬: ê°„ë‹¨(70%), ì¶”ë¡ (30%)\n",
    "        \n",
    "        # ì¤‘ê°„ ê³¼ì •\n",
    "        # embedding nodes:  37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                             | 17/46 [00:33<01:46,  3.68s/it]\n",
    "        # embedding nodes:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ         | 40/46 [06:56<02:35, 25.91s/it]\n",
    "        # embedding nodes:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/46 [09:26<00:29, 29.29s/it]\n",
    "\n",
    "        # Generating:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]\n",
    "        # Generating:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                       | 1/2 [06:24<06:24, 384.85s/it]\n",
    "\n",
    "        Generating:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                       | 1/2 [06:24<06:24, 384.85s/itFailed to parse output. Returning None.\n",
    "        â†³ ë©ˆì¶¤ â†’ ê°•ì œì¢…ë£Œ    \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8befff4",
   "metadata": {},
   "source": [
    "* \n",
    "  * try_2\n",
    "\n",
    "<small> <small>\n",
    "\n",
    "```bash\n",
    "\n",
    "        ============================================================\n",
    "        llama3.2:3b ëª¨ë¸ RAGAS ê°„ë‹¨ í…ŒìŠ¤íŠ¸\n",
    "        ============================================================\n",
    "\n",
    "        ğŸ”§ RAGAS ì´ˆê¸°í™” ì¤‘...\n",
    "\n",
    "        1. LLM ì„¤ì • ì¤‘...\n",
    "            âœ… LLM ì‘ë™: Hello! It's nice to meet you. ...\n",
    "            âœ… Ollama LLM ì„¤ì • ì™„ë£Œ\n",
    "\n",
    "        2. Embeddings ì„¤ì • ì¤‘...\n",
    "            âœ… ì™„ë£Œ\n",
    "\n",
    "        3. ë¬¸ì„œ ë¡œë“œ ì¤‘...\n",
    "            âœ… 5í˜ì´ì§€ ë¡œë“œ\n",
    "\n",
    "        4. DocumentStore ì´ˆê¸°í™” ì¤‘...\n",
    "            âœ… ì™„ë£Œ\n",
    "\n",
    "        5. Generator ìƒì„± ì¤‘...\n",
    "            âœ… ì™„ë£Œ\n",
    "\n",
    "        ============================================================\n",
    "        ğŸ”„ í…ŒìŠ¤íŠ¸ì…‹ ìƒì„± ì‹œì‘\n",
    "        ============================================================\n",
    "        ğŸ”„ í…ŒìŠ¤íŠ¸ì…‹ ìƒì„± ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)\n",
    "            - ë¶„í¬: ê°„ë‹¨(70%), ì¶”ë¡ (30%)\n",
    "\n",
    "        ğŸ”„ ë©ˆì¶° ë³´ì—¬ë„ ì •ìƒì…ë‹ˆë‹¤! ğŸ”„ \n",
    "        ============================================================\n",
    "\n",
    "        # ì¤‘ê°„ ê³¼ì •\n",
    "        # embedding nodes:  26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                            | 15/58 [00:44<03:09,  4.40s/it\n",
    "        # embedding nodes:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                     | 43/58 [04:51<02:06,  8.45s/it]\n",
    "        # embedding nodes:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 54/58 [09:42<01:50, 27.70s/it]\n",
    "\n",
    "        # Generating:   0%|                                                                                        | 0/2 [00:00<?, ?it/s]\n",
    "        # Generating:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                            | 1/2 [07:46<07:46, 466.04s/it]\n",
    "\n",
    "        Generating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [22:33<00:00, 676.72s/it]\n",
    "\n",
    "        ============================================================\n",
    "        âœ…âœ…âœ… ì„±ê³µ! âœ…âœ…âœ…\n",
    "        ============================================================\n",
    "\n",
    "        ì§ˆë¬¸: Here is a question that can be fully answered from the given context:\n",
    "\n",
    "        \"What does 'FTC' stand for in the provided text?\"\n",
    "\n",
    "        This question can be answered by referring to the key phrase \"KEY Contents\\nn \\ubbf8\\uad6d FTC...\" in the context, which explicitly states what \"FTC\" stands for.\n",
    "        ë‹µë³€: FTC...\n",
    "\n",
    "        ì§ˆë¬¸: The goal is to create a rewritten question that conveys the same meaning as \"What type of AI does the Federal Trade Commission possess?\" without directly stating it.\n",
    "\n",
    "        Here's a revised version:\n",
    "\n",
    "        \"What kind of AI system does the FTC use?\"\n",
    "\n",
    "        This rewritten question achieves the same intent as the original but in a more concise and indirect manner, using abbreviation (\"FTC\" instead of \"Federal Trade Commission\") to make the question shorter.\n",
    "        ë‹µë³€: The Federal Trade Commission uses artificial intelligence primarily for regulati...\n",
    "\n",
    "        âœ… ì €ì¥: data/ragas_synthetic_dataset_2.csv\n",
    "\n",
    "        ============================================================\n",
    "\n",
    "        âœ… í…ŒìŠ¤íŠ¸ì…‹ ìƒì„± ì™„ë£Œ!\n",
    "        - ìƒì„±ëœ ì§ˆë¬¸ ìˆ˜: 2\n",
    "        - ì €ì¥ ìœ„ì¹˜: ../15_Evaluations/data/ragas_synthetic_dataset.csv\n",
    "\n",
    "                                                    question  ... episode_done\n",
    "        0  Here is a question that can be fully answered ...  ...         True\n",
    "        1  The goal is to create a rewritten question tha...  ...         True\n",
    "\n",
    "        [2 rows x 6 columns]\n",
    "        ğŸ“Š ìƒì„±ëœ ì§ˆë¬¸ ë¯¸ë¦¬ë³´ê¸°:\n",
    "        ================================================================================\n",
    "\n",
    "        ì§ˆë¬¸ 1:\n",
    "        Here is a question that can be fully answered from the given context:\n",
    "\n",
    "        \"What does 'FTC' stand for in the provided text?\"\n",
    "\n",
    "        This question can be answered by referring to the key phrase \"KEY Contents\\nn \\ubbf8\\uad6d FTC...\" in the context, which explicitly states what \"FTC\" stands for.\n",
    "        Ground Truth: FTC...\n",
    "\n",
    "        ì§ˆë¬¸ 2:\n",
    "        The goal is to create a rewritten question that conveys the same meaning as \"What type of AI does the Federal Trade Commission possess?\" without directly stating it.\n",
    "\n",
    "        Here's a revised version:\n",
    "\n",
    "        \"What kind of AI system does the FTC use?\"\n",
    "\n",
    "        This rewritten question achieves the same intent as the original but in a more concise and indirect manner, using abbreviation (\"FTC\" instead of \"Federal Trade Commission\") to make the question shorter.\n",
    "        Ground Truth: The Federal Trade Commission uses artificial intelligence primarily for regulating privacy, trademar...\n",
    "\n",
    "        âœ… ì •ìƒì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ì…‹ ìƒì„± ì™„ë£Œ!\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84487e3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5238649b",
   "metadata": {},
   "source": [
    "### **2. `ì£¼í”¼í„° ë…¸íŠ¸ë¶`ìœ¼ë¡œ ì‹œë„** - *`try_1`*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f936725",
   "metadata": {},
   "source": [
    "#### **1) `í•©ì„± í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„±`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6ede17",
   "metadata": {},
   "source": [
    "* **ì™œ `í•©ì„± í…ŒìŠ¤íŠ¸ ë°ì´í„°`(`Synthetic Test Dataset`) ì¸ê°€?**\n",
    "\n",
    "  * `RAG` *(ê²€ìƒ‰ ì¦ê°• ìƒì„±) ì¦ê°• íŒŒì´í”„ë¼ì¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” ê²ƒì€ ë§¤ìš° ì¤‘ìš”*\n",
    "\n",
    "  * ì–´ë ¤ì›€\n",
    "\n",
    "    * ë¬¸ì„œì—ì„œ `ìˆ˜ë°± ê°œì˜ QA` (`ì§ˆë¬¸-ë¬¸ë§¥-ì‘ë‹µ`) `ìƒ˜í”Œ`ì„ `ìˆ˜ë™`ìœ¼ë¡œ `ìƒì„±`í•˜ëŠ” ê²ƒ = **`ì‹œê°„ê³¼ ë…¸ë™ë ¥ì´ ë§ì´ ì†Œìš”`**\n",
    "    * `ì‚¬ëŒì´ ë§Œë“  ì§ˆë¬¸` = ì² ì €í•œ í‰ê°€ì— í•„ìš”í•œ ë³µì¡ì„± ìˆ˜ì¤€ì— ë„ë‹¬í•˜ê¸° ì–´ë ¤ì›€ = ê¶ê·¹ì ìœ¼ë¡œ í‰ê°€ì˜ í’ˆì§ˆì— ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆìŒ\n",
    "\n",
    "  * **`í•©ì„± ë°ì´í„° ìƒì„±`ì„ ì‚¬ìš©** â†’ ë°ì´í„° ì§‘ê³„ í”„ë¡œì„¸ìŠ¤ì—ì„œ `ê°œë°œìì˜ ì‹œê°„`ì„ **`90%`** ê¹Œì§€ **`ê°ì†Œ ê°€ëŠ¥`**\n",
    "\n",
    "  * ì°¸ê³ : [RAGAS](https://docs.ragas.io/en/latest/concepts/testset_generation.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd6b318",
   "metadata": {},
   "source": [
    "* ì‚¬ì „ì— `VS Code` í„°ë¯¸ë„ì— ì„¤ì¹˜í•  ê²ƒ\n",
    "\n",
    "```bash\n",
    "\n",
    "        pip install -qU ragas\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1958a16",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* êµ¬ë²„ì „ìœ¼ë¡œ ì„¤ì¹˜í•  ê²ƒ!\n",
    "\n",
    "```bash\n",
    "\n",
    "        pip install ragas==0.1.21\n",
    "\n",
    "        # ì˜ì¡´ì„± í™•ì¸\n",
    "        pip list | grep -E \"ragas|langchain\"\n",
    "\n",
    "        # ì˜ˆìƒ ì¶œë ¥:\n",
    "        # ragas                     0.1.21\n",
    "        # ...\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a390668a",
   "metadata": {},
   "source": [
    "* ì˜¤ë¥˜ ë°œìƒ â†’ ì»¤ë„ ë° íŒ¨í‚¤ì§€ ì¶©ëŒ\n",
    "\n",
    "* íŠ¸ëŸ¬ë¸” ìŠˆíŒ… ë¬¸ì„œ ì°¸ê³ \n",
    "  * [`RAGAS_Synthetic_Dataset_Generation_Version_Conflicts_Troubleshooting`](docs/troubleshooting/RAGAS_Synthetic_Dataset_Generation_Version_Conflicts_Alternative_Solutions_Troubleshooting.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716fbfa2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a342870f",
   "metadata": {},
   "source": [
    "#### **2) `í™˜ê²½ì„¤ì •`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d763fc98",
   "metadata": {},
   "source": [
    "##### **`â€ ê¸°ë³¸ì„¤ì •`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b354960e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"TQDM_DISABLE\"] = \"1\"                        # ì§„í–‰ë°” ë¹„í™œì„±í™”\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(\"âœ… í™˜ê²½ ì„¤ì • ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2178c5",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* âœ… í™˜ê²½ ì„¤ì • ì™„ë£Œ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a36bcf",
   "metadata": {},
   "source": [
    "##### **`â LangSmith ì„¤ì •`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ef86f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "from langsmith import traceable\n",
    "\n",
    "import os\n",
    "\n",
    "# LangSmith í™˜ê²½ ë³€ìˆ˜ í™•ì¸\n",
    "\n",
    "print(\"\\n--- LangSmith í™˜ê²½ ë³€ìˆ˜ í™•ì¸ ---\")\n",
    "langchain_tracing_v2 = os.getenv('LANGCHAIN_TRACING_V2')\n",
    "langchain_project = os.getenv('LANGCHAIN_PROJECT')\n",
    "langchain_api_key_status = \"ì„¤ì •ë¨\" if os.getenv('LANGCHAIN_API_KEY') else \"ì„¤ì •ë˜ì§€ ì•ŠìŒ\" # API í‚¤ ê°’ì€ ì§ì ‘ ì¶œë ¥í•˜ì§€ ì•ŠìŒ\n",
    "\n",
    "if langchain_tracing_v2 == \"true\" and os.getenv('LANGCHAIN_API_KEY') and langchain_project:\n",
    "    print(f\"âœ… LangSmith ì¶”ì  í™œì„±í™”ë¨ (LANGCHAIN_TRACING_V2='{langchain_tracing_v2}')\")\n",
    "    print(f\"âœ… LangSmith í”„ë¡œì íŠ¸: '{langchain_project}'\")\n",
    "    print(f\"âœ… LangSmith API Key: {langchain_api_key_status}\")\n",
    "    print(\"  -> ì´ì œ LangSmith ëŒ€ì‹œë³´ë“œì—ì„œ ì´ í”„ë¡œì íŠ¸ë¥¼ í™•ì¸í•´ ë³´ì„¸ìš”.\")\n",
    "else:\n",
    "    print(\"âŒ LangSmith ì¶”ì ì´ ì™„ì „íˆ í™œì„±í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‹¤ìŒì„ í™•ì¸í•˜ì„¸ìš”:\")\n",
    "    if langchain_tracing_v2 != \"true\":\n",
    "        print(f\"  - LANGCHAIN_TRACING_V2ê°€ 'true'ë¡œ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤ (í˜„ì¬: '{langchain_tracing_v2}').\")\n",
    "    if not os.getenv('LANGCHAIN_API_KEY'):\n",
    "        print(\"  - LANGCHAIN_API_KEYê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
    "    if not langchain_project:\n",
    "        print(\"  - LANGCHAIN_PROJECTê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f14d715",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* ì…€ ì¶œë ¥\n",
    "\n",
    "    ```bash\n",
    "    --- LangSmith í™˜ê²½ ë³€ìˆ˜ í™•ì¸ ---\n",
    "    âœ… LangSmith ì¶”ì  í™œì„±í™”ë¨ (LANGCHAIN_TRACING_V2='true')\n",
    "    âœ… LangSmith í”„ë¡œì íŠ¸: 'LangChain-prantice'\n",
    "    âœ… LangSmith API Key: ì„¤ì •ë¨\n",
    "    -> ì´ì œ LangSmith ëŒ€ì‹œë³´ë“œì—ì„œ ì´ í”„ë¡œì íŠ¸ë¥¼ í™•ì¸í•´ ë³´ì„¸ìš”.\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13582956",
   "metadata": {},
   "source": [
    "##### **`â‚ íŒ¨í‚¤ì§€ ì„í¬íŠ¸`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1c46df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, conditional, multi_context\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from ragas.testset.extractor import KeyphraseExtractor\n",
    "from ragas.testset.docstore import InMemoryDocumentStore\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "print(\"âœ… íŒ¨í‚¤ì§€ ì„í¬íŠ¸ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3aaf569",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* âœ… íŒ¨í‚¤ì§€ ì„í¬íŠ¸ ì™„ë£Œ - (`1.8s`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe5bf03",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb35bf01",
   "metadata": {},
   "source": [
    "#### **3) `LLM` í˜¸ì¶œí•˜ê¸°**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0395c40",
   "metadata": {},
   "source": [
    "##### **`â€ ë°ì´í„°ì…‹ ìƒì„±ê¸°`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a95d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„°ì…‹ ìƒì„±ê¸°\n",
    "generator_llm = ChatOllama(\n",
    "    model=\"llama3.2:3b\",                    # ìì—°ì–´ ëª¨ë¸ë¡œ êµì²´\n",
    "    temperature=0.7,\n",
    "    num_predict=512,\n",
    ")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ í˜¸ì¶œ\n",
    "test_response = generator_llm.invoke(\"Hello\")\n",
    "print(f\"âœ… ë°ì´í„°ì…‹ ìƒì„±ê¸° LLM ì‘ë™ í™•ì¸: {test_response.content[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfecc29",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* âœ… ë°ì´í„°ì…‹ ìƒì„±ê¸° LLM ì‘ë™ í™•ì¸: How can I assist you today?... - (`4.7s`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f878d27",
   "metadata": {},
   "source": [
    "##### **`â ë°ì´í„°ì…‹ ë¹„í‰ê¸°`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c0457d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„°ì…‹ ë¹„í‰ê¸°\n",
    "critic_llm = ChatOllama(\n",
    "    model=\"llama3.2:3b\",\n",
    "    temperature=0.1,\n",
    "    num_predict=512,\n",
    ")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ í˜¸ì¶œ\n",
    "test_response2 = critic_llm.invoke(\"ì•ˆë…•?\")\n",
    "print(f\"âœ… ë°ì´í„°ì…‹ ìƒì„±ê¸° LLM ì‘ë™ í™•ì¸: {test_response2.content[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283a71fb",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* âœ… ë°ì´í„°ì…‹ ìƒì„±ê¸° LLM ì‘ë™ í™•ì¸: ì•ˆë…•í•˜ì„¸ìš”! (Hello!) How can I help you today?... - (`2.3s`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a2d393",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486f1503",
   "metadata": {},
   "source": [
    "#### **4) `ì„ë² ë”© ì„¤ì •í•˜ê¸°`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0e7cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì„ë² ë”© ìƒì„±\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "print(\"âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ\")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì„ë² ë”©\n",
    "test_embed = embeddings.embed_query(\"í…ŒìŠ¤íŠ¸\")\n",
    "print(f\"âœ… ì„ë² ë”© ì‘ë™ í™•ì¸: ì°¨ì›={len(test_embed)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a717a3",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ - (`9.1s`)\n",
    "\n",
    "* âœ… ì„ë² ë”© ì‘ë™ í™•ì¸: ì°¨ì›=384"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8473a6b2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20212e1",
   "metadata": {},
   "source": [
    "#### **5) `ë¬¸ì„œ ë¡œë“œ` & `ì „ì²˜ë¦¬`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01da7831",
   "metadata": {},
   "source": [
    "##### **`â€ ë¬¸ì„œ ë¡œë“œí•˜ê¸°`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9daeb247",
   "metadata": {},
   "source": [
    "* ì‹¤ìŠµì— í™œìš©í•  ë¬¸ì„œ\n",
    "\n",
    "  * ì†Œí”„íŠ¸ì›¨ì–´ì •ì±…ì—°êµ¬ì†Œ (SPRi) - 2023ë…„ 12ì›”í˜¸\n",
    "\n",
    "    * ì €ì: ìœ ì¬í¥(AIì •ì±…ì—°êµ¬ì‹¤ ì±…ì„ì—°êµ¬ì›), ì´ì§€ìˆ˜(AIì •ì±…ì—°êµ¬ì‹¤ ìœ„ì´‰ì—°êµ¬ì›)\n",
    "    * ì°¸ê³ : [ë§í¬](https://spri.kr/posts/view/23669)\n",
    "    * íŒŒì¼ëª…: [SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf](../15_Evaluations/data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7ea56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PDFPlumberLoader(\"../15_Evaluations/data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf\")\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "# ëª©ì°¨, ë í˜ì´ì§€ ì œì™¸\n",
    "docs = docs[3:-1]                   # êµì¬ ì‚¬ì´íŠ¸ì™€ ë˜‘ê°™ì´ ì„¤ì • \n",
    "\n",
    "print(f\"âœ… ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ: {len(docs)}í˜ì´ì§€\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39392440",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* âœ… ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ: 19í˜ì´ì§€ - (`3.3s`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e1ee8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(docs))               # <class 'list'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6387636",
   "metadata": {},
   "source": [
    "##### **`â ë©”íƒ€ ë°ì´í„° ì„¤ì •í•˜ê¸°`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e33ef7",
   "metadata": {},
   "source": [
    "* **`metadata`** í™•ì¸í•˜ê¸°\n",
    "\n",
    "  * ê° ë¬¸ì„œ ê°ì²´ì—ëŠ” `metadata` â†’ ì•¡ì„¸ìŠ¤í•  ìˆ˜ ìˆëŠ” ë¬¸ì„œì— ëŒ€í•œ `ì¶”ê°€ ì •ë³´`ë¥¼ `ì €ì¥`í•˜ëŠ” ë° ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” `ë©”íƒ€ë°ì´í„° ì‚¬ì „`ì´ `í¬í•¨`ë˜ì–´ ìˆìŒ\n",
    "\n",
    "  * ë©”íƒ€ë°ì´í„° ì‚¬ì „ì— **`filename`** ì´ë¼ëŠ” `í‚¤`ê°€ `í¬í•¨` ì—¬ë¶€ í™•ì¸í•˜ê¸°\n",
    "\n",
    "    * ì´ í‚¤ëŠ” `Test datasets` ìƒì„± í”„ë¡œì„¸ìŠ¤ì—ì„œ í™œìš©ë  ê²ƒ\n",
    "\n",
    "    * ë©”íƒ€ë°ì´í„°ì˜ `filename` ì†ì„± = `ë™ì¼í•œ ë¬¸ì„œ`ì— ì†í•œ `ì²­í¬`ë¥¼ `ì‹ë³„`í•˜ëŠ” ë° `ì‚¬ìš©`ë¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba6fd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metadata ì„¤ì •\n",
    "for doc in docs:\n",
    "    doc.metadata[\"filename\"] = doc.metadata[\"source\"]\n",
    "    \n",
    "print(f\"âœ… ì²« ë¬¸ì„œ ë¯¸ë¦¬ë³´ê¸°: {docs[0].page_content[:50]} ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf2228b",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* âœ… ì²« ë¬¸ì„œ ë¯¸ë¦¬ë³´ê¸°: 1. ì •ì±…/ë²•ì œ 2. ê¸°ì—…/ì‚°ì—… 3. ê¸°ìˆ /ì—°êµ¬ 4. ì¸ë ¥/êµìœ¡\n",
    "\n",
    "* ë¯¸êµ­, ì•ˆì „í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdd56c5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5ec0cb",
   "metadata": {},
   "source": [
    "#### **6) `DocumentStore` ì´ˆê¸°í™”**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d532750",
   "metadata": {},
   "source": [
    "* **`docsstore êµ¬ì„±ìš”ì†Œ` ë° `docstore ì´ˆê¸°í™”`**\n",
    "\n",
    "  * **`DocumentStore` ì´ˆê¸°í™”**\n",
    "\n",
    "  * **`ì‚¬ìš©ì ì •ì˜ LLM`ê³¼ `ì„ë² ë”©` ì‚¬ìš©**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1395a99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì„¤ì •\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "# LangchainLLMWrapperë¡œ ê°ì‹¸ Ragasì™€ í˜¸í™˜ë˜ë„ë¡ í•¨\n",
    "langchain_llm = LangchainLLMWrapper(generator_llm)\n",
    "\n",
    "# ì£¼ìš” êµ¬ë¬¸ ì¶”ì¶œê¸° ì´ˆê¸°í™” (ìœ„ì—ì„œ ì •ì˜í•œ LLM ì‚¬ìš©)\n",
    "keyphrase_extractor = KeyphraseExtractor(llm=langchain_llm)\n",
    "\n",
    "# ragas_embeddings ìƒì„±\n",
    "ragas_embeddings = LangchainEmbeddingsWrapper(embeddings)\n",
    "\n",
    "# InMemoryDocumentStore ì´ˆê¸°í™” (ë¬¸ì„œë¥¼ ë©”ëª¨ë¦¬ì— ì €ì¥í•˜ê³  ê´€ë¦¬í•˜ëŠ” ì €ì¥ì†Œ)\n",
    "docstore = InMemoryDocumentStore(\n",
    "    splitter=splitter,\n",
    "    embeddings=ragas_embeddings,\n",
    "    extractor=keyphrase_extractor,\n",
    ")\n",
    "\n",
    "print(\"âœ… DocumentStore ì´ˆê¸°í™” ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b9d562",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* âœ… DocumentStore ì´ˆê¸°í™” ì™„ë£Œ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f906af6e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c4a753",
   "metadata": {},
   "source": [
    "#### **7) `DataSet ìƒì„±í•˜ê¸°`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f7f3de",
   "metadata": {},
   "source": [
    "##### **`â€ TestSet Generator` ìƒì„±í•˜ê¸°**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17203e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = TestsetGenerator.from_langchain(\n",
    "    generator_llm,\n",
    "    critic_llm ,\n",
    "    ragas_embeddings,\n",
    "    docstore=docstore,\n",
    ")\n",
    "\n",
    "print(\"âœ… Generator ìƒì„± ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b579bba3",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* âœ… Generator ìƒì„± ì™„ë£Œ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb88606",
   "metadata": {},
   "source": [
    "##### **`â ì§ˆë¬¸ ë¶„í¬` ì„¤ì •í•˜ê¸°**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b879c42",
   "metadata": {},
   "source": [
    "* **`ì§ˆë¬¸ì˜ ìœ í˜•ë³„ ë¶„í¬`**\n",
    "\n",
    "  * `simple`: ê°„ë‹¨í•œ ì§ˆë¬¼\n",
    "\n",
    "  * `reasoning`: `ì¶”ë¡ `ì´ í•„ìš”í•œ ì§ˆë¬¸\n",
    "\n",
    "  * `multi_context`: `ì—¬ëŸ¬ ë§¥ë½`ì„ ê³ ë ¤í•´ì•¼ í•˜ëŠ” ì§ˆë¬¸\n",
    "\n",
    "  * `conditional`: `ì¡°ê±´ë¶€ ì§ˆë¬¸`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239ec3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì§ˆë¬¸ ìœ í˜•ë³„ ë¶„í¬ ê²°ì •\n",
    "\n",
    "distributions = {\n",
    "    simple: 0.4,            # simple: ê°„ë‹¨í•œ ì§ˆë¬¸\n",
    "    reasoning: 0.2,         # reasoning: ì¶”ë¡ ì´ í•„ìš”í•œ ì§ˆë¬¸\n",
    "    multi_context: 0.2,     # multi_context: ì—¬ëŸ¬ ë§¥ë½ì„ ê³ ë ¤í•´ì•¼ í•˜ëŠ” ì§ˆë¬¸\n",
    "    conditional: 0.2        # conditional: ì¡°ê±´ë¶€ ì§ˆë¬¸\n",
    "    }\n",
    "\n",
    "print(\"âœ… ì§ˆë¬¸ ë¶„í¬ ì„¤ì • ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48df3dd6",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* âœ… ì§ˆë¬¸ ë¶„í¬ ì„¤ì • ì™„ë£Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd26fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(distributions))              # <class 'dict'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eebf996",
   "metadata": {},
   "source": [
    "##### **`â‚ ë°ì´í„°ì…‹` ìƒì„±í•˜ê¸°**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9066cd",
   "metadata": {},
   "source": [
    "* **`í…ŒìŠ¤íŠ¸ì…‹ ìƒì„±`**\n",
    "\n",
    "  * `documents`: `ë¬¸ì„œ ë°ì´í„°`\n",
    "\n",
    "  * `test_size`: `ìƒì„±`í•  `ì§ˆë¬¸ì˜ ìˆ˜`\n",
    "\n",
    "  * `distributions`: ì§ˆë¬¸ `ìœ í˜•ë³„ ë¶„í¬`\n",
    "\n",
    "  * `with_debugging_logs`: `ë””ë²„ê¹… ë¡œê·¸ ì¶œë ¥ ì—¬ë¶€`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db2d4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ğŸ”„ í…ŒìŠ¤íŠ¸ì…‹ ìƒì„± ì‹œì‘\")\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ”„ í…ŒìŠ¤íŠ¸ì…‹ ìƒì„± ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)\")\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ”„ ë©ˆì¶° ë³´ì—¬ë„ ì •ìƒì…ë‹ˆë‹¤! ğŸ”„ \")\n",
    "print(\"=\"*60)\n",
    "print(\"â° ì˜ˆìƒ ì‹œê°„: 20-30ë¶„\")\n",
    "print(\"ğŸ’¡ ë©ˆì¶˜ ê²ƒì²˜ëŸ¼ ë³´ì—¬ë„ ì •ìƒì…ë‹ˆë‹¤!\")\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    testset = generator.generate_with_langchain_docs(\n",
    "        documents=docs,\n",
    "        test_size=3,                            # ë” ì‘ê²Œ ì„¤ì •\n",
    "        distributions=distributions,\n",
    "        with_debugging_logs=False,              # ë¡œê·¸ ë¹„í™œì„±í™”\n",
    "        raise_exceptions=False,                 # ì—ëŸ¬ ë¬´ì‹œ\n",
    "    )\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"\\nâœ… ì™„ë£Œ! ì†Œìš” ì‹œê°„: {elapsed/60:.1f}ë¶„\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ì—ëŸ¬: {e}\")\n",
    "    print(\"ğŸ’¡ Ollamaê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”!\")\n",
    "    print(\"\\nğŸ’¡ í•´ê²° ë°©ë²•:\")\n",
    "    print(\"    1. Ollamaê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸\")\n",
    "    print(\"    2. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ: ollama pull llama3.2:3b\")\n",
    "    print(\"    3. langchain-ollama ë²„ì „ í™•ì¸: pip list | grep langchain-ollama\")\n",
    "    print(\"        â†’ 0.1.3ì´ì–´ì•¼ í•¨!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b05106",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "```bash\n",
    "\n",
    "    ============================================================\n",
    "    ğŸ”„ í…ŒìŠ¤íŠ¸ì…‹ ìƒì„± ì‹œì‘\n",
    "    ============================================================\n",
    "    ğŸ”„ í…ŒìŠ¤íŠ¸ì…‹ ìƒì„± ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)\n",
    "    ============================================================\n",
    "    ğŸ”„ ë©ˆì¶° ë³´ì—¬ë„ ì •ìƒì…ë‹ˆë‹¤! ğŸ”„ \n",
    "    ============================================================\n",
    "    â° ì˜ˆìƒ ì‹œê°„: 20-30ë¶„\n",
    "    ğŸ’¡ ë©ˆì¶˜ ê²ƒì²˜ëŸ¼ ë³´ì—¬ë„ ì •ìƒì…ë‹ˆë‹¤!\n",
    "\n",
    "    Generating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [48:31<00:00, 565.54s/it]\n",
    "\n",
    "    Failed to parse output. Returning None.\n",
    "\n",
    "    âœ… ì™„ë£Œ! ì†Œìš” ì‹œê°„: 74.7ë¶„\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf18cc7",
   "metadata": {},
   "source": [
    "##### **`âƒ ê²°ê³¼ ì €ì¥í•˜ê¸°`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1b9c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìƒì„±ëœ í…ŒìŠ¤íŠ¸ì…‹ì„ pandas DataFrameìœ¼ë¡œ ë³€í™˜\n",
    "test_df = testset.to_pandas()\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e426953",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* `test_df`\n",
    "\n",
    "  * ![test_df](../15_Evaluations/assets/test_df.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa10a039",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad91bb27",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* `test_df`\n",
    "\n",
    "  * ![test_df.head](../15_Evaluations/assets/test_df_head.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262a96f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ìƒì„±ëœ ì§ˆë¬¸ ìˆ˜: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f904e560",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* ìƒì„±ëœ ì§ˆë¬¸ ìˆ˜: 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972dd4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv(\"../15_Evaluations/data/ragas_synthetic_dataset_3.csv\", index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(\"âœ… ì €ì¥: data/ragas_synthetic_dataset_3.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a655da43",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* âœ… ì €ì¥: data/ragas_synthetic_dataset_3.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fc4725",
   "metadata": {},
   "source": [
    "##### **`â„ ê²°ê³¼ í™•ì¸í•˜ê¸°`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dcaf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¯¸ë¦¬ë³´ê¸°\n",
    "\n",
    "if len(test_df) > 0:\n",
    "    \n",
    "    print()\n",
    "    print(\"=\"*60)\n",
    "    print(\"âœ…âœ…âœ… ì„±ê³µ! âœ…âœ…âœ…\")\n",
    "    print(\"=\"*60)\n",
    "    print()\n",
    "        \n",
    "    for idx, row in test_df.iterrows():\n",
    "        print(f\"\\nì§ˆë¬¸ {idx+1}: \")\n",
    "        print(f\"    {row['question']}\")\n",
    "        print(f\"\\në‹µë³€ {idx+2}: \")\n",
    "        print(f\"    {row['ground_truth'][:100]}...\")\n",
    "        print()\n",
    "        print(\"âœ… ì €ì¥: data/ragas_synthetic_dataset_3.csv\")\n",
    "\n",
    "else:\n",
    "    print(\"âŒ ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de6416e",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* **`ìƒì„±ëœ ì§ˆë¬¸ë³´ê¸°`**\n",
    "\n",
    "    ```bash\n",
    "\n",
    "    ============================================================\n",
    "    âœ…âœ…âœ… ì„±ê³µ! âœ…âœ…âœ…\n",
    "    ============================================================\n",
    "\n",
    "\n",
    "    ì§ˆë¬¸ 1: \n",
    "        Here is a question that can be fully answered from the given context:\n",
    "\n",
    "    \"Who developed the model RAG \\ud3ec\\ud568 \\uc9c8\\ubb38\\uacfc \\ub2f5\\ubcc0\\uc5d0\\uc11c\\ub294 \\ud5c8\\uae45, and what is its relation to LLM?\"\n",
    "\n",
    "    Note that this question can be answered by reading the context provided.\n",
    "\n",
    "    ë‹µë³€ 2: \n",
    "        Who developed the model RAG í¬í•¨ ì§ˆë¬¸ê³¼ ë‹µë³€ì—ì„œëŠ” \\ud5c8\\uae45, and what is its relation to LLM?...\n",
    "\n",
    "    âœ… ì €ì¥: data/ragas_synthetic_dataset_3.csv\n",
    "\n",
    "    ì§ˆë¬¸ 2: \n",
    "        Here is a rewritten version of the question:\n",
    "\n",
    "    \"What technology is CCnet 2024 based on?\"\n",
    "\n",
    "    This version still conveys the same meaning as the original question, but in a more concise and indirect way.\n",
    "\n",
    "    ë‹µë³€ 3: \n",
    "        nan...\n",
    "\n",
    "    âœ… ì €ì¥: data/ragas_synthetic_dataset_3.csv\n",
    "\n",
    "    ì§ˆë¬¸ 3: \n",
    "        Here is a rewritten version of the question that conveys the same meaning in a less direct and shorter manner:\n",
    "\n",
    "    \"What specific aspects of UL2 training are targeted by CES 2024's AI focus, and how do they align with the OpenMoE framework?\"\n",
    "\n",
    "    I made the following changes to achieve this:\n",
    "\n",
    "    * Abbreviated \"OpenMoE framework\" to \"OpenMoE\"\n",
    "    * Changed \"specific aspects of UL2 training objective's configuration\" to \"specific aspects of UL2 training\"\n",
    "    * Combined \"within the framework\" into the question itself\n",
    "    * Simplified the wording and sentence structure\n",
    "\n",
    "    ë‹µë³€ 4: \n",
    "        The answer to given question is not present in context...\n",
    "\n",
    "    âœ… ì €ì¥: data/ragas_synthetic_dataset_3.csv\n",
    "\n",
    "    ì§ˆë¬¸ 4: \n",
    "        Here's a rewritten version of the question that conveys the same meaning in a less direct and shorter manner:\n",
    "\n",
    "    \"What triggers EU AI regulation for developers?\"\n",
    "\n",
    "    Alternatively, you could also ask:\n",
    "\n",
    "    \"When must EU AI regulation be applied by developers?\n",
    "\n",
    "    ë‹µë³€ 5: \n",
    "        nan...\n",
    "\n",
    "    âœ… ì €ì¥: data/ragas_synthetic_dataset_3.csv\n",
    "    \n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a4e926",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc70db0e",
   "metadata": {},
   "source": [
    "### **3. `ì£¼í”¼í„° ë…¸íŠ¸ë¶`ìœ¼ë¡œ ì‹œë„** - *`try_2`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33256832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ì…€ 1: í™˜ê²½ ì„¤ì •\n",
    "# ============================================\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"TQDM_DISABLE\"] = \"1\"  # ì§„í–‰ë°” ë¹„í™œì„±í™”\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(\"âœ… í™˜ê²½ ì„¤ì • ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509e2a78",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* âœ… í™˜ê²½ ì„¤ì • ì™„ë£Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c499b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ì…€ 2: íŒ¨í‚¤ì§€ ì„í¬íŠ¸ & í™•ì¸\n",
    "# ============================================\n",
    "\n",
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, conditional\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from ragas.testset.extractor import KeyphraseExtractor\n",
    "from ragas.testset.docstore import InMemoryDocumentStore\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "print(\"âœ… íŒ¨í‚¤ì§€ ì„í¬íŠ¸ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c996c23d",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* âœ… íŒ¨í‚¤ì§€ ì„í¬íŠ¸ ì™„ë£Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb64efb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ì…€ 3: LLM ì„¤ì • (í™•ì¸ ê°€ëŠ¥!)\n",
    "# ============================================\n",
    "\n",
    "generator_llm = ChatOllama(\n",
    "    model=\"llama3.2:3b\",                    # ìì—°ì–´ ëª¨ë¸ë¡œ êµì²´\n",
    "    temperature=0.7,\n",
    "    num_predict=512,\n",
    ")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ í˜¸ì¶œ\n",
    "test_response = generator_llm.invoke(\"Hello\")\n",
    "print(f\"âœ… ë°ì´í„°ì…‹ ìƒì„±ê¸° LLM ì‘ë™ í™•ì¸: {test_response.content[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5644a9",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* âœ… ë°ì´í„°ì…‹ ìƒì„±ê¸° LLM ì‘ë™ í™•ì¸: How can I assist you today?... - (`3.9s`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b3da91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ì…€ 4: ì„ë² ë”© ì„¤ì • (í™•ì¸ ê°€ëŠ¥!)\n",
    "# ============================================\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "    model_kwargs={'device': 'cpu'},\n",
    ")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì„ë² ë”©\n",
    "test_embed = embeddings.embed_query(\"í…ŒìŠ¤íŠ¸\")\n",
    "print(f\"âœ… ì„ë² ë”© ì‘ë™ í™•ì¸: ì°¨ì›={len(test_embed)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514bb27e",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* âœ… ì„ë² ë”© ì‘ë™ í™•ì¸: ì°¨ì›=384 - (`3.2s`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed71e0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ì…€ 5: ë¬¸ì„œ ë¡œë“œ (í™•ì¸ ê°€ëŠ¥!)\n",
    "# ============================================\n",
    "loader = PDFPlumberLoader(\"data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf\")\n",
    "docs = loader.load()[3:-1]\n",
    "\n",
    "for doc in docs:\n",
    "    doc.metadata[\"filename\"] = doc.metadata[\"source\"]\n",
    "\n",
    "print(f\"âœ… ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ: {len(docs)}í˜ì´ì§€\")\n",
    "print(f\"ì²« ë¬¸ì„œ ë¯¸ë¦¬ë³´ê¸°: {docs[0].page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f592635",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* âœ… ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ: 19í˜ì´ì§€ - (`2.8s`)\n",
    "\n",
    "```markdown\n",
    "\n",
    "    ì²« ë¬¸ì„œ ë¯¸ë¦¬ë³´ê¸°: 1. ì •ì±…/ë²•ì œ 2. ê¸°ì—…/ì‚°ì—… 3. ê¸°ìˆ /ì—°êµ¬ 4. ì¸ë ¥/êµìœ¡\n",
    "    ë¯¸êµ­, ì•ˆì „í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” AI ê°œë°œê³¼ ì‚¬ìš©ì— ê´€í•œ í–‰ì •ëª…ë ¹ ë°œí‘œ\n",
    "    KEY Contents\n",
    "    n ë¯¸êµ­ ë°”ì´ë“  ëŒ€í†µ...\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c2bfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ì…€ 6: DocumentStore ì´ˆê¸°í™” (í™•ì¸ ê°€ëŠ¥!)\n",
    "# ============================================\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=600,             # ë” ì‘ê²Œ!\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "langchain_llm = LangchainLLMWrapper(generator_llm)\n",
    "keyphrase_extractor = KeyphraseExtractor(llm=langchain_llm)\n",
    "ragas_embeddings = LangchainEmbeddingsWrapper(embeddings)\n",
    "\n",
    "docstore = InMemoryDocumentStore(\n",
    "    splitter=splitter,\n",
    "    embeddings=ragas_embeddings,\n",
    "    extractor=keyphrase_extractor,\n",
    ")\n",
    "\n",
    "print(\"âœ… DocumentStore ì´ˆê¸°í™” ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0c6e6b",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* âœ… DocumentStore ì´ˆê¸°í™” ì™„ë£Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de539cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ì…€ 7: Generator ìƒì„± (í™•ì¸ ê°€ëŠ¥!)\n",
    "# ============================================\n",
    "generator = TestsetGenerator.from_langchain(\n",
    "    generator_llm,\n",
    "    generator_llm,          # criticë„ ë™ì¼í•˜ê²Œ\n",
    "    ragas_embeddings,\n",
    "    docstore=docstore,\n",
    ")\n",
    "\n",
    "print(\"âœ… Generator ìƒì„± ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51da5185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ì…€ 8: ì§ˆë¬¸ ë¶„í¬ ì„¤ì • (multi_context = ë¶ˆì•ˆì •)\n",
    "# ============================================\n",
    "distributions = {\n",
    "    simple: 0.6,                # 60%\n",
    "    reasoning: 0.2,             # 20%\n",
    "    conditional: 0.2,           # 20%\n",
    "}\n",
    "\n",
    "print(\"âœ… ì§ˆë¬¸ ë¶„í¬ ì„¤ì • ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a066a39",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* âœ… ì§ˆë¬¸ ë¶„í¬ ì„¤ì • ì™„ë£Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb59fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ì…€ 9: í…ŒìŠ¤íŠ¸ì…‹ ìƒì„± (ì‹œê°„ ì†Œìš”!)\n",
    "# ============================================\n",
    "print(\"ğŸ”„ í…ŒìŠ¤íŠ¸ì…‹ ìƒì„± ì‹œì‘...\")\n",
    "print(\"â° ì˜ˆìƒ ì‹œê°„: 20-30ë¶„\")\n",
    "print(\"ğŸ’¡ ë©ˆì¶˜ ê²ƒì²˜ëŸ¼ ë³´ì—¬ë„ ì •ìƒì…ë‹ˆë‹¤!\")\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    testset = generator.generate_with_langchain_docs(\n",
    "        documents=docs,\n",
    "        test_size=3,                        # ë” ì‘ê²Œ!\n",
    "        distributions=distributions,\n",
    "        with_debugging_logs=False,\n",
    "        raise_exceptions=False,\n",
    "    )\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"\\nâœ… ì™„ë£Œ! ì†Œìš” ì‹œê°„: {elapsed/60:.1f}ë¶„\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ì—ëŸ¬: {e}\")\n",
    "    print(\"ğŸ’¡ Ollamaê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644fcc2b",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "```bash\n",
    "\n",
    "    ğŸ”„ í…ŒìŠ¤íŠ¸ì…‹ ìƒì„± ì‹œì‘...\n",
    "    \n",
    "    â° ì˜ˆìƒ ì‹œê°„: 20-30ë¶„\n",
    "    \n",
    "    ğŸ’¡ ë©ˆì¶˜ ê²ƒì²˜ëŸ¼ ë³´ì—¬ë„ ì •ìƒì…ë‹ˆë‹¤!\n",
    "\n",
    "\n",
    "    # embedding nodes:   100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 108/108 [27:50<00:00, 25.22s/it]\n",
    "    # â†³ ì´í›„ ì‚¬ë¼ì§\n",
    "    \n",
    "    # Generating:   50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                   | 2/4\n",
    "    # Generating:   75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ             | 3/4\n",
    "    Generating:   100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [41:48<00:00, 505.28s/it]\n",
    "\n",
    "    âœ… ì™„ë£Œ! ì†Œìš” ì‹œê°„: 69.8ë¶„ - (`69m 45.4s`)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99df33d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ì…€ 10: ê²°ê³¼ í™•ì¸ & ì €ì¥\n",
    "# ============================================\n",
    "test_df2 = testset.to_pandas()\n",
    "print(f\"ìƒì„±ëœ ì§ˆë¬¸ ìˆ˜: {len(test_df2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bc4e27",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* ìƒì„±ëœ ì§ˆë¬¸ ìˆ˜: 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe03ed23",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575288b6",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* `test_df2`\n",
    "  \n",
    "  * ![test_df2](../15_Evaluations/assets/test_df2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75833f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7bc800",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* `test_df2.head()`\n",
    "  \n",
    "  * ![test_df2.head()](../15_Evaluations/assets/test_df2_head.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bea069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV ì €ì¥\n",
    "test_df2.to_csv(\"../15_Evaluations/data/ragas_synthetic_dataset_4.csv\", index=False, encoding='utf-8-sig')\n",
    "print(\"\\nâœ… ì €ì¥ ì™„ë£Œ: data/ragas_testset4.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17479131",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* âœ… ì €ì¥ ì™„ë£Œ: data/ragas_testset4.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a2ebd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¯¸ë¦¬ë³´ê¸°\n",
    "if len(test_df2) > 0:\n",
    "    print()\n",
    "    print(\"=\"*60)\n",
    "    print(\"âœ…âœ…âœ… ì„±ê³µ! âœ…âœ…âœ…\")\n",
    "    print(\"=\"*60)\n",
    "    print()\n",
    "\n",
    "    for idx, row in test_df2.iterrows():\n",
    "        print(f\"\\nì§ˆë¬¸ {idx+1}: \")\n",
    "        print(f\"    {row['question']}\")\n",
    "        print(f\"\\në‹µë³€ {idx+2}: \")\n",
    "        print(f\"    {row['ground_truth'][:100]}...\")\n",
    "        print()\n",
    "        print(\"âœ… ì €ì¥: data/ragas_synthetic_dataset_4.csv\")\n",
    "else:\n",
    "    print(\"âŒ ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763e8011",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "```bash\n",
    "\n",
    "    ============================================================\n",
    "    âœ…âœ…âœ… ì„±ê³µ! âœ…âœ…âœ…\n",
    "    ============================================================\n",
    "\n",
    "\n",
    "    ì§ˆë¬¸ 1: \n",
    "        Here is a question that can be fully answered from the given context:\n",
    "\n",
    "    \"What is Artificial General Intelligence (AGI) and how does it differ from other types of artificial intelligence?\"\n",
    "\n",
    "    This question is formed using the keyphrase \"Artificial General Intelligence\" which appears throughout the provided text.\n",
    "\n",
    "    ë‹µë³€ 2: \n",
    "        Artificial General Intelligence (AGI) refers to a type of AI that possesses human-like intelligence ...\n",
    "\n",
    "    âœ… ì €ì¥: data/ragas_synthetic_dataset_4.csv\n",
    "\n",
    "    ì§ˆë¬¸ 2: \n",
    "        Here's a question that can be fully answered from the given context:\n",
    "\n",
    "    \"Who is the founder of AI?\"\n",
    "\n",
    "    answer: The context does not mention the founder of AI, but it talks about AI in the context of G7 and its applications. However, based on general knowledge, the answer would be \"Many individuals have contributed to the development of AI, but some notable founders include Andrew Ng, Yann LeCun, Geoffrey Hinton, Yoshua Bengio, and others.\"\n",
    "\n",
    "    Note: The provided text does not contain information about the founder(s) of AI.\n",
    "\n",
    "    ë‹µë³€ 3: \n",
    "        The answer to given question is not present in context...\n",
    "\n",
    "    âœ… ì €ì¥: data/ragas_synthetic_dataset_4.csv\n",
    "\n",
    "    ì§ˆë¬¸ 3: \n",
    "        Here is a rewritten question that conveys the same meaning but in a less direct manner, using abbreviation and being shorter:\n",
    "\n",
    "    \"What's the term for AGI?\"\n",
    "\n",
    "    This rewritten question still asks about the concept of Artificial General Intelligence (AGI), but does so in a more concise and indirect way.\n",
    "\n",
    "    ë‹µë³€ 4: \n",
    "        nan...\n",
    "\n",
    "    âœ… ì €ì¥: data/ragas_synthetic_dataset_4.csv\n",
    "\n",
    "    ì§ˆë¬¸ 4: \n",
    "        Here is a rewritten version of the question that conveys the same meaning but in a less direct and shorter manner:\n",
    "\n",
    "    \"What config. details in OpenMoE define AGI's UL2 objective?\"\n",
    "\n",
    "    Alternatively, you could also use:\n",
    "\n",
    "    \"How do OpenMoE settings impact AGI's UL2 training?\"\n",
    "\n",
    "    Or even more concise:\n",
    "\n",
    "    \"Which OpenMoE settings affect AGI's UL2 objective?\n",
    "\n",
    "    ë‹µë³€ 5: \n",
    "        nan...\n",
    "\n",
    "    âœ… ì €ì¥: data/ragas_synthetic_dataset_4.csv\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009251c4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7626d2",
   "metadata": {},
   "source": [
    "* next: ***`02. SQL`***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f4de16",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc_eval_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
