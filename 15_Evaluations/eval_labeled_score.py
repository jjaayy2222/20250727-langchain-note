# ========================================
# eval_labeled_score.py (ë³µë¶™!)
# ì ìˆ˜ ê¸°ë°˜ í‰ê°€
# ========================================

from dotenv import load_dotenv
load_dotenv()

import os
from myrag5 import PDFRAG
from langchain_ollama import ChatOllama             # Ollama ì‚¬ìš©
from langsmith.evaluation import evaluate, LangChainStringEvaluator
import warnings
warnings.filterwarnings("ignore")
os.environ["TOKENIZERS_PARALLELISM"] = "false"           # Tokenizer Parallelism ì¶œë ¥ ê²½ê³  ë©”ì‹œì§€ ì–µì œ

print("ğŸš€ Labeled Score í‰ê°€ ì‹œì‘...\n")


# ========================================
# íŒŒì¼ ê²½ë¡œ & LLM ì„¤ì •
# ========================================

script_dir = os.path.dirname(os.path.abspath(__file__))
pdf_path = os.path.join(script_dir, "data", "SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf")

if not os.path.exists(pdf_path):
    print(f"âŒ íŒŒì¼ ì—†ìŒ: {pdf_path}")
    exit(1)

print(f"âœ… PDF í™•ì¸: {pdf_path}\n")

# LLM ìƒì„±

llm = ChatOllama(
    model="qwen2.5-coder:7b-instruct",
    temperature=0
)

print("âœ… ë¡œì»¬ LLM ìƒì„± ì™„ë£Œ: Qwen2.5-Coder-7B \n")


# ========================================
# RAG ì‹œìŠ¤í…œ ìƒì„±
# ========================================

rag = PDFRAG(pdf_path, llm, chunk_size=300, chunk_overlap=50)
retriever = rag.create_retriever(k=10, search_type="similarity")

# Context í¬í•¨ ì²´ì¸
chain_with_context = rag.create_chain_with_context(retriever)

print("\n" + "="*50)
print("ğŸ§ª í…ŒìŠ¤íŠ¸ ì‹¤í–‰...")
print("="*50 + "\n")

# í…ŒìŠ¤íŠ¸
test_question = "ì‚¼ì„±ì „ìê°€ ìì²´ ê°œë°œí•œ ìƒì„±í˜• AIì˜ ì´ë¦„ì€ ë¬´ì—‡ì¸ê°€ìš”?"

try:
    test_result = chain_with_context.invoke(test_question)
    print(f"ì§ˆë¬¸: {test_question}")
    print(f"Context: {test_result['context'][:200]}...")
    print(f"ë‹µë³€: {test_result['answer']}\n")
except Exception as e:
    print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}\n")
    import traceback
    traceback.print_exc()
    exit(1)


# ========================================
# Context + Answer ë°˜í™˜ í•¨ìˆ˜
# ========================================

def context_answer_rag_answer(inputs: dict):
    """
    Contextì™€ Answerë¥¼ í•¨ê»˜ ë°˜í™˜
    """
    result = chain_with_context.invoke(inputs["question"])
    return {
        "context": result["context"],
        "answer": result["answer"],
        "query": inputs["question"],
    }


# ========================================
# Labeled Score í‰ê°€ì ìƒì„±
# ========================================

print("="*50)
print("ğŸ“Š Labeled Score í‰ê°€ì ìƒì„±...")
print("="*50 + "\n")


# í‰ê°€ìš© LLM
eval_llm = ChatOllama(
    model="qwen2.5-coder:7b-instruct",
    temperature=0
)


# 1. Accuracy Score (1-10 ì ìˆ˜)
accuracy_score_evaluator = LangChainStringEvaluator(
    "labeled_score_string",
    config={
        "criteria": {
            "accuracy": (
                "How accurate is this prediction compared to the reference "
                "on a scale of 1-10? "
                "Rate factual correctness only."
            )
        },
        "normalize_by": 10,                      # 0-1 ì‚¬ì´ë¡œ ì •ê·œí™”
        "llm": llm,
    },
    prepare_data=lambda run, example: {
        "prediction": run.outputs["answer"],
        "reference": example.outputs["answer"],  # Ground Truth
        "input": example.inputs["question"],
    },
)

# 2. Completeness Score (1-10 ì ìˆ˜)
completeness_score_evaluator = LangChainStringEvaluator(
    "labeled_score_string",
    config={
        "criteria": {
            "completeness": (
                "How complete is this prediction compared to the reference "
                "on a scale of 1-10? "
                "Does it cover all important information?"
            )
        },
        "normalize_by": 10,
        "llm": llm,
    },
    prepare_data=lambda run, example: {
        "prediction": run.outputs["answer"],
        "reference": example.outputs["answer"],  # Ground Truth
        "input": example.inputs["question"],
    },
)

# 3. Context Relevance Score (1-10 ì ìˆ˜)
context_relevance_score_evaluator = LangChainStringEvaluator(
    "labeled_score_string",
    config={
        "criteria": {
            "context_relevance": (
                "How well does this prediction use the provided context "
                "on a scale of 1-10? "
                "Rate how accurately it references the context."
            )
        },
        "normalize_by": 10,
        "llm": eval_llm,                      # í‰ê°€ìš© llmìœ¼ë¡œ ìˆ˜ì •
    },
    prepare_data=lambda run, example: {
        "prediction": run.outputs["answer"],
        "reference": run.outputs["context"],  # Context
        "input": example.inputs["question"],
    },
)

print("âœ… í‰ê°€ì ìƒì„± ì™„ë£Œ\n")


# ========================================
# í‰ê°€ ì‹¤í–‰
# ========================================

print("="*50)
print("ğŸ“Š í‰ê°€ ì‹¤í–‰...")
print("="*50 + "\n")

try:
    experiment_results = evaluate(
        context_answer_rag_answer,
        data="RAG_EVAL_DATASET",
        evaluators=[
            accuracy_score_evaluator,
            completeness_score_evaluator,
            context_relevance_score_evaluator,
        ],
        experiment_prefix="RAG_EVAL_LABELED_SCORE_LOCAL",
        metadata={
            "variant": "Labeled Score (Accuracy + Completeness + Context Relevance) - Local",
            "k": 10,
            "chunk_size": 300,
            "llm": "qwen2.5:14b-instruct",
            "eval_llm": "qwen2.5:14b-instruct",
            "scoring": "1-10 scale, normalized to 0-1",
        },
        max_concurrency=1,
    )
    
    print("\n" + "="*50)
    print("âœ… í‰ê°€ ì™„ë£Œ!")
    print("="*50)
    print(f"\nê²°ê³¼: {experiment_results}\n")

except Exception as e:
    print(f"\nâŒ ì—ëŸ¬ ë°œìƒ: {e}\n")
    import traceback
    traceback.print_exc()