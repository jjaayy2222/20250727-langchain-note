{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "574f6502",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf366bf9",
   "metadata": {},
   "source": [
    "* 출처: LangChain 공식 문서 또는 해당 교재명\n",
    "* 원본 URL: https://smith.langchain.com/hub/teddynote/summary-stuff-documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31f7552",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af770941",
   "metadata": {},
   "source": [
    "### **5. `05. LLM-as-Judge`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb2944b",
   "metadata": {},
   "source": [
    "#### **2) `Question-Answer Evaluator`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300b2621",
   "metadata": {},
   "source": [
    "* 가장 기본 기능을 가진 `Evaluator` = `Query` - `Answer` 평가하기\n",
    "\n",
    "* 사용자 입력 = `input` → `LLM`이 생성한 답변 → `prediction`으로 정답 답변은 `reference`로 정의됨\n",
    "\n",
    "  * `Prompt`변수 \n",
    "    * `query`: 질문\n",
    "    * `result`: `LLM` 답변\n",
    "    * `answer`: 정답 답변"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213e7ff9",
   "metadata": {},
   "source": [
    "* **새로운 가상환경 생성 - `lc_eval_env`**\n",
    "\n",
    "  * `Python-3.12`\n",
    "  * `Pydantic`: `ver 1.10.18`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa96e875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pydantic 버전: 1.10.18\n"
     ]
    }
   ],
   "source": [
    "import pydantic\n",
    "print(f\"Pydantic 버전: {pydantic.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0a0018",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* **`Pydantic 버전: 1.10.18`** - (`0.1s`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f802a885",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb9c7b9",
   "metadata": {},
   "source": [
    "* **`환경 설정`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3acf7ae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API 키를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API 키 정보 로드\n",
    "load_dotenv()                           # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9d33436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- LangSmith 환경 변수 확인 ---\n",
      "✅ LangSmith 추적 활성화됨 (LANGCHAIN_TRACING_V2='true')\n",
      "✅ LangSmith 프로젝트: 'LangChain-prantice'\n",
      "✅ LangSmith API Key: 설정됨\n",
      "  -> 이제 LangSmith 대시보드에서 이 프로젝트를 확인해 보세요.\n"
     ]
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "from langsmith import traceable\n",
    "\n",
    "import os\n",
    "\n",
    "# LangSmith 환경 변수 확인\n",
    "\n",
    "print(\"\\n--- LangSmith 환경 변수 확인 ---\")\n",
    "langchain_tracing_v2 = os.getenv('LANGCHAIN_TRACING_V2')\n",
    "langchain_project = os.getenv('LANGCHAIN_PROJECT')\n",
    "langchain_api_key_status = \"설정됨\" if os.getenv('LANGCHAIN_API_KEY') else \"설정되지 않음\" # API 키 값은 직접 출력하지 않음\n",
    "\n",
    "if langchain_tracing_v2 == \"true\" and os.getenv('LANGCHAIN_API_KEY') and langchain_project:\n",
    "    print(f\"✅ LangSmith 추적 활성화됨 (LANGCHAIN_TRACING_V2='{langchain_tracing_v2}')\")\n",
    "    print(f\"✅ LangSmith 프로젝트: '{langchain_project}'\")\n",
    "    print(f\"✅ LangSmith API Key: {langchain_api_key_status}\")\n",
    "    print(\"  -> 이제 LangSmith 대시보드에서 이 프로젝트를 확인해 보세요.\")\n",
    "else:\n",
    "    print(\"❌ LangSmith 추적이 완전히 활성화되지 않았습니다. 다음을 확인하세요:\")\n",
    "    if langchain_tracing_v2 != \"true\":\n",
    "        print(f\"  - LANGCHAIN_TRACING_V2가 'true'로 설정되어 있지 않습니다 (현재: '{langchain_tracing_v2}').\")\n",
    "    if not os.getenv('LANGCHAIN_API_KEY'):\n",
    "        print(\"  - LANGCHAIN_API_KEY가 설정되어 있지 않습니다.\")\n",
    "    if not langchain_project:\n",
    "        print(\"  - LANGCHAIN_PROJECT가 설정되어 있지 않습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3cee3b",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 셀 출력\n",
    "\n",
    "    ```bash\n",
    "    --- LangSmith 환경 변수 확인 ---\n",
    "    ✅ LangSmith 추적 활성화됨 (LANGCHAIN_TRACING_V2='true')\n",
    "    ✅ LangSmith 프로젝트: 'LangChain-prantice'\n",
    "    ✅ LangSmith API Key: 설정됨\n",
    "    -> 이제 LangSmith 대시보드에서 이 프로젝트를 확인해 보세요.\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "36c37fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from myrag import PDFRAG                        # local 임베딩 버전으로 수정한 myrag.py 불러오기\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# API 키 확인\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "GOOGLE_API_KEY=os.getenv(\"GOOGLE_API_KEY2\")\n",
    "\n",
    "if not os.getenv(\"GOOGLE_API_KEY2\"):\n",
    "    os.environ[\"GOOGLE_API_KEY2\"] = input(\"Enter your GOOGLE_API_KEY2: \")\n",
    "\n",
    "if \"GOOGLE_API_KEY2\" not in os.environ:\n",
    "    print(\"❌ 경고: GOOGLE_API_KEY2 환경 변수가 설정되지 않았습니다. 반드시 설정해야 gemini LLM이 작동합니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5d0a2b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ gemini-2.5-flash-lite 성공!\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm2 = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\", temperature=0, api_key=os.getenv(\"GOOGLE_API_KEY2\"))\n",
    "\n",
    "print(\"✅ gemini-2.5-flash-lite 성공!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95081584",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* **`✅ 성공!`** \n",
    "\n",
    "  * *`API 할당량 부족으로 두번쨰 계정으로 다시 시도`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1c51b692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 문서 로드 완료: 23개 페이지\n",
      "✅ 문서 분할 완료: 43개 청크\n",
      "✅ 임베딩 모델 로드: all-MiniLM-L6-v2\n",
      "✅ 벡터스토어 생성 완료\n",
      "✅ PDFRAG 생성 성공!\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm2 = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\", temperature=0, api_key=os.getenv(\"GOOGLE_API_KEY2\"))\n",
    "\n",
    "rag = PDFRAG(\n",
    "    \"../15_Evaluations/data/SPRI_AI_Brief_2023년12월호_F.pdf\",\n",
    "    llm2,\n",
    ")\n",
    "\n",
    "print(\"✅ PDFRAG 생성 성공!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8dd84b",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* **`PDFRAG 생성`** - (`4.9s`)\n",
    "\n",
    "    ```markdown\n",
    "\n",
    "    ✅ 문서 로드 완료: 23개 페이지\n",
    "    ✅ 문서 분할 완료: 43개 청크\n",
    "    ✅ 임베딩 모델 로드: all-MiniLM-L6-v2\n",
    "    ✅ 벡터스토어 생성 완료\n",
    "    ✅ PDFRAG 생성 성공!\n",
    "\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2d43340c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 검색기 생성 완료 (k=4)\n"
     ]
    }
   ],
   "source": [
    "# 검색기(retriever) 생성\n",
    "retriever = rag.create_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344ce808",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* **`✅ 검색기 생성 완료 (k=4)`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ac9d8a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ RAG 체인 생성 완료\n"
     ]
    }
   ],
   "source": [
    "# 체인(chain) 생성\n",
    "chain = rag.create_chain(retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04637af",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* **`✅ RAG 체인 생성 완료`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "869c8197",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1760508400.597600 9355863 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "삼성전자가 자체 개발한 생성형 AI의 이름은 '삼성 가우스'입니다.\n"
     ]
    }
   ],
   "source": [
    "# 질문\n",
    "answer = chain.invoke(\"삼성전자가 자체 개발한 생성형 AI의 이름은 무엇인가요?\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f68a2e",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* **`삼성전자가 자체 개발한 생성형 AI의 이름은 '삼성 가우스'입니다.`** - (`2.0s`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b47c2bcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': \"삼성전자가 자체 개발한 생성형 AI의 이름은 '삼성 가우스'입니다.\"}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 질문에 대한 답변하는 함수를 생성\n",
    "def ask_question(inputs: dict):\n",
    "    return {\"answer\": chain.invoke(inputs[\"question\"])}\n",
    "\n",
    "# 사용자 질문 예시\n",
    "llm_answer = ask_question(\n",
    "    {\"question\": \"삼성전자가 자체 개발한 생성형 AI의 이름은 무엇인가요?\"}\n",
    ")\n",
    "\n",
    "llm_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0efe342",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* **`ask_question()`** - (`1.2s`)\n",
    "\n",
    "    ```python\n",
    "\n",
    "        {'answer': \"삼성전자가 자체 개발한 생성형 AI의 이름은 '삼성 가우스'입니다.\"}\n",
    "\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "747018bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluator prompt 출력을 위한 함수\n",
    "def print_evaluator_prompt(evaluator):\n",
    "    return evaluator.evaluator.prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bf06b760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a teacher grading a quiz.\n",
      "You are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\n",
      "\n",
      "Example Format:\n",
      "QUESTION: question here\n",
      "STUDENT ANSWER: student's answer here\n",
      "TRUE ANSWER: true answer here\n",
      "GRADE: CORRECT or INCORRECT here\n",
      "\n",
      "Grade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \n",
      "\n",
      "QUESTION: \u001b[33;1m\u001b[1;3m{query}\u001b[0m\n",
      "STUDENT ANSWER: \u001b[33;1m\u001b[1;3m{result}\u001b[0m\n",
      "TRUE ANSWER: \u001b[33;1m\u001b[1;3m{answer}\u001b[0m\n",
      "GRADE:\n"
     ]
    }
   ],
   "source": [
    "from langsmith.evaluation import evaluate, LangChainStringEvaluator\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# qa 평가자 생성\n",
    "qa_evalulator = LangChainStringEvaluator(\n",
    "    \"qa\",\n",
    "    config={\n",
    "        \"llm\":ChatGoogleGenerativeAI(\n",
    "            model=\"gemini-2.5-flash-lite\", \n",
    "            temperature=0, \n",
    "            google_api_key=os.getenv(\"GOOGLE_API_KEY2\"))\n",
    "        }\n",
    ")\n",
    "\n",
    "# 프롬프트 출력\n",
    "print_evaluator_prompt(qa_evalulator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafb79c3",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* **`qa_evaluator`** - (`0.1s`)\n",
    "\n",
    "    ```markdown\n",
    "\n",
    "    You are a teacher grading a quiz.\n",
    "    You are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\n",
    "\n",
    "    Example Format:\n",
    "    QUESTION: question here\n",
    "    STUDENT ANSWER: student's answer here\n",
    "    TRUE ANSWER: true answer here\n",
    "    GRADE: CORRECT or INCORRECT here\n",
    "\n",
    "    Grade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \n",
    "\n",
    "    QUESTION: *{query}*\n",
    "    STUDENT ANSWER: *{result}*\n",
    "    TRUE ANSWER: *{answer}*\n",
    "    GRADE:\n",
    "\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54472586",
   "metadata": {},
   "source": [
    "* **`평가 진행` → 출력한 `URL` 이동 → `결과 확인하기`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "448b0dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'RAG_EVAL-7ff2d7c9' at:\n",
      "https://smith.langchain.com/o/2c3342d3-1170-4ffa-86fd-f621199e0b9c/datasets/420dd308-2ebd-44c9-8ce8-9aff3886dc8e/compare?selectedSessions=6d370b7d-2aa7-4b8e-9d38-78c005eea377\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]E0000 00:00:1760508510.914989 9404755 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
      "E0000 00:00:1760508510.921383 9404756 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
      "5it [00:04,  1.02it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"RAG_EVAL_DATASET\"\n",
    "\n",
    "# 평가 실행\n",
    "experiment_results = evaluate(\n",
    "    ask_question,\n",
    "    data=dataset_name,\n",
    "    evaluators=[qa_evalulator],\n",
    "    experiment_prefix=\"RAG_EVAL\",\n",
    "    # 실험 메타데이터 지정\n",
    "    metadata={\n",
    "        \"variant\": \"QA Evaluator 를 활용한 평가\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992242c1",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 결과 확인하기\n",
    "\n",
    "  * `RAG_EVAL` / *`COMPILED`* - (*`45s`*)\n",
    "  * ![`RAG_EVAL_COMPILED`](../15_Evaluations/assets/RAG_EVAL_1.png)\n",
    "\n",
    "  * \n",
    "\n",
    "  * `RAG_EVAL` / *`CORRECT=1`, `INCORRECT=4`* - (*`7.8s`*)\n",
    "  * ![`RAG_EVAL_MORE_INCORRECT`](../15_Evaluations/assets/RAG_EVAL_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa2a070",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1775b968",
   "metadata": {},
   "source": [
    "* **`RAG`** 시스템 개선하기\n",
    "\n",
    "  * **a. `Retriever` 개선하기**\n",
    "  * **b. `Chunk Size` 조정하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96641657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 문서 로드 완료: 23개 페이지\n",
      "✅ 문서 분할 완료: 72개 청크\n",
      "✅ 임베딩 모델 로드: all-MiniLM-L6-v2\n",
      "✅ 벡터스토어 생성 완료\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# Chunk Size 조정\n",
    "# ========================================\n",
    "from myrag2 import PDFRAG\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv() \n",
    "\n",
    "# Chunk Size 조정\n",
    "rag2 = PDFRAG(\n",
    "    \"../15_Evaluations/data/SPRI_AI_Brief_2023년12월호_F.pdf\",\n",
    "    ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-2.0-flash-lite\", \n",
    "        temperature=0, \n",
    "        google_api_key=os.getenv(\"GOOGLE_API_KEY2\")),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b022410",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* **`rag2 생성`** - (`12.2s`)\n",
    "\n",
    "    ```mardkwon\n",
    "\n",
    "    ✅ 문서 로드 완료: 23개 페이지\n",
    "    ✅ 문서 분할 완료: 72개 청크\n",
    "    ✅ 임베딩 모델 로드: all-MiniLM-L6-v2\n",
    "    ✅ 벡터스토어 생성 완료\n",
    "\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f0b0126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 검색기 생성 완료 \n",
      "✅ RAG 체인 생성 완료\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# Top-K 증가\n",
    "# ========================================\n",
    "\n",
    "# 개선\n",
    "retriever2 = rag2.create_retriever()        # myrag2.py 불러오기\n",
    "\n",
    "# 체인 재생성\n",
    "chain2 = rag2.create_chain(retriever2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad59bb6c",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* **`✅ 검색기 생성 완료`**\n",
    "\n",
    "* **`✅ RAG 체인 생성 완료`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70e94e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1760512905.700200 9492174 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I am sorry, but the provided context does not contain the answer to your question.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 질문에 대한 답변 생성\n",
    "chain2.invoke(\"삼성전자가 자체 개발한 생성형 AI의 이름은 무엇인가요?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24623124",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* **`'삼성전자가 자체 개발한 생성형 AI의 이름은 제공된 문서에서 찾을 수 없습니다.'`** - (`1.4s`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d2b1ec0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': '삼성전자가 자체 개발한 생성형 AI의 이름은 제공된 문서에서 찾을 수 없습니다.'}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 질문에 대한 답변하는 함수를 생성\n",
    "def ask_question(inputs: dict):\n",
    "    return {\"answer\": chain2.invoke(inputs[\"question\"])}\n",
    "\n",
    "# 사용자 질문 예시\n",
    "llm_answer = ask_question(\n",
    "    {\"question\": \"삼성전자가 자체 개발한 생성형 AI의 이름은 무엇인가요?\"}\n",
    ")\n",
    "\n",
    "llm_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d3d51e",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* **`ask_question()`** - (`1.3s`)\n",
    "\n",
    "    ```python\n",
    "\n",
    "    {'answer': '삼성전자가 자체 개발한 생성형 AI의 이름은 제공된 문서에서 찾을 수 없습니다.'}\n",
    "\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7798a392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluator prompt 출력을 위한 함수\n",
    "def print_evaluator_prompt(evaluator):\n",
    "    return evaluator.evaluator.prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a31c0c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a teacher grading a quiz.\n",
      "You are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\n",
      "\n",
      "Example Format:\n",
      "QUESTION: question here\n",
      "STUDENT ANSWER: student's answer here\n",
      "TRUE ANSWER: true answer here\n",
      "GRADE: CORRECT or INCORRECT here\n",
      "\n",
      "Grade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \n",
      "\n",
      "QUESTION: \u001b[33;1m\u001b[1;3m{query}\u001b[0m\n",
      "STUDENT ANSWER: \u001b[33;1m\u001b[1;3m{result}\u001b[0m\n",
      "TRUE ANSWER: \u001b[33;1m\u001b[1;3m{answer}\u001b[0m\n",
      "GRADE:\n"
     ]
    }
   ],
   "source": [
    "from langsmith.evaluation import evaluate, LangChainStringEvaluator\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "\n",
    "# qa 평가자 생성\n",
    "qa_evalulator = LangChainStringEvaluator(\n",
    "    \"qa\",\n",
    "    config={\n",
    "        \"llm\":ChatGoogleGenerativeAI(\n",
    "            model=\"gemini-2.5-flash-lite\", \n",
    "            temperature=0, \n",
    "            google_api_key=os.getenv(\"GOOGLE_API_KEY2\"))\n",
    "        }\n",
    ")\n",
    "\n",
    "\n",
    "# 프롬프트 출력\n",
    "print_evaluator_prompt(qa_evalulator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ec610bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'RAG_EVAL_2-00c422bd' at:\n",
      "https://smith.langchain.com/o/2c3342d3-1170-4ffa-86fd-f621199e0b9c/datasets/420dd308-2ebd-44c9-8ce8-9aff3886dc8e/compare?selectedSessions=cfb85525-ad18-4c76-9e47-4b2c1beb95fa\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]E0000 00:00:1760510512.220081 9442399 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
      "5it [00:04,  1.12it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"RAG_EVAL_DATASET\"\n",
    "\n",
    "# 평가 실행\n",
    "experiment_results = evaluate(\n",
    "    ask_question,\n",
    "    data=dataset_name,\n",
    "    evaluators=[qa_evalulator],\n",
    "    experiment_prefix=\"RAG_EVAL_2\",\n",
    "    # 실험 메타데이터 지정\n",
    "    metadata={\n",
    "        \"variant\": \"myrag2.py 반영\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2154ce",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* **개선 시도: `RAG_EVAL_2`**\n",
    "\n",
    "  ```bash\n",
    "\n",
    "  View the evaluation results for experiment: 'RAG_EVAL_2-00c422bd' at:\n",
    "  https://smith.langchain.com/o/2c3342d3-1170-4ffa-86fd-f621199e0b9c/datasets/420dd308-2ebd-44c9-8ce8-9aff3886dc8e/compare?selectedSessions=cfb85525-ad18-4c76-9e47-4b2c1beb95fa\n",
    "\n",
    "\n",
    "  0it [00:00, ?it/s]\n",
    "\n",
    "  5it [00:04,  1.12it/s]\n",
    "\n",
    "  ```\n",
    "\n",
    "* 결과 확인하기\n",
    "\n",
    "<br>\n",
    "\n",
    "* \n",
    "  * `RAG_EVAL` / *`CORRECT=2`, `INCORRECT=3`* - (*`7.3s`*)\n",
    "  * ![`RAG_EVAL_MORE_INCORRECT`](../15_Evaluations/assets/RAG_EVAL_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9620d711",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc52808b",
   "metadata": {},
   "source": [
    "* **`RAG`** 시스템 개선하기_2\n",
    "\n",
    "  * **c. `chunk_size` 조정**\n",
    "  * **d. 잦은 커널 충돌 → `python` 파일로 실행하기**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a7f716",
   "metadata": {},
   "source": [
    "* c.\n",
    "\n",
    "  * `chunck_size` 조절 test\n",
    "    * **`chunk_size` 지정 ❌** or **`chunk_size` = `4` → 검색 결과 없음**\n",
    "      * *`'죄송합니다. 제공된 문서에서 삼성전자가 자체 개발한 생성형 AI의 이름에 대한 정보를 찾을 수 없습니다.'`*\n",
    "\n",
    "    * **`chunk_size` = `7` → 검색 결과 ⭕️**\n",
    "      * *`\"삼성전자가 자체 개발한 생성형 AI의 이름은 '삼성 가우스'입니다.\"`*\n",
    "\n",
    "  * [**`myrag4.py`**](../15_Evaluations/myrag4.py)로 정리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8482815d",
   "metadata": {},
   "source": [
    "* d.\n",
    "  * **`잦은 커널 충돌로 인한 문제` → `python` 파일로 실행 → 터미널에서 직접 확인하기**\n",
    "\n",
    "  * [**`eval_script.py`**](../15_Evaluations/eval_script.py) 실행 → 터미널에서 결과 바로 확인\n",
    "\n",
    "  * ![`RAG_EVAL_4`](../15_Evaluations/assets/RAG_EVAL_4.png)\n",
    "\n",
    "<small>\n",
    "\n",
    "```bash\n",
    "\n",
    "      (가상환경)/15_Evaluations/eval_script.py\n",
    "\n",
    "      🚀 RAG 시스템 초기화 시작...\n",
    "\n",
    "      📂 스크립트 위치: ... /15_Evaluations\n",
    "\n",
    "      📄 PDF 경로: ... /data/SPRI_AI_Brief_2023년12월호_F.pdf\n",
    "\n",
    "      ✅ 파일 확인 완료!\n",
    "\n",
    "      ✅ LLM 생성 완료: gemini-2.5-flash-lite\n",
    "\n",
    "      ✅ PDF 로드 완료: 23 페이지\n",
    "\n",
    "      ✅ 청크 분할 완료: 119 청크 (크기=300, 오버랩=50)\n",
    "\n",
    "\n",
    "      ✅ LLM 생성 완료: gemini-2.5-flash-lite\n",
    "\n",
    "      ✅ 청크 분할 완료: 119 청크 (크기=300, 오버랩=50)\n",
    "      pytorch_model.bin: 100%|████████████████████████████████████████████████████████████████████| 2.27G/2.27G [00:03<00:00, 695MB/s]\n",
    "      tokenizer_config.json: 100%|███████████████████████████████████████████████████████████████████| 444/444 [00:00<00:00, 1.92MB/s]\n",
    "      sentencepiece.bpe.model: 100%|██████████████████████████████████████████████████████████████| 5.07M/5.07M [00:06<00:00, 781kB/s]\n",
    "      tokenizer.json: 100%|██████████████████████████████████████████████████████████████████████| 17.1M/17.1M [00:06<00:00, 2.70MB/s]\n",
    "      special_tokens_map.json: 100%|█████████████████████████████████████████████████████████████████| 964/964 [00:00<00:00, 3.61MB/s]\n",
    "      config.json: 100%|██████████████████████████████████████████████████████████████████████████████| 191/191 [00:00<00:00, 628kB/s]\n",
    "\n",
    "      ✅ 임베딩 모델 로드 완료: BAAI/bge-m3                                                                 | 0.00/191 [00:00<?, ?B/s]\n",
    "      model.safetensors:   0%|                                                                  | 2.19M/2.27G [00:24<1:26:03, 439kB/s]✅ 벡터스토어 생성 완료: FAISS\n",
    "\n",
    "      ✅ 검색기 생성 완료 (k=7, search_type=similarity)\n",
    "\n",
    "      ✅ RAG 체인 생성 완료\n",
    "\n",
    "      ==================================================\n",
    "      🧪 테스트 실행...\n",
    "      ==================================================\n",
    "\n",
    "      View the evaluation results for experiment: 'RAG_EVAL_K7-3a35bd45' at:\n",
    "      https://smith.langchain.com/o/2c3342d3-1170-4ffa-86fd-f621199e0b9c/datasets/420dd308-2ebd-44c9-8ce8-9aff3886dc8e/compare?selectedSessions=cdb9ea10-e3c1-47bc-bfe1-48f651364c28\n",
    "\n",
    "\n",
    "      5it [00:14,  2.90s/it]\n",
    "      5it [00:14,  2.18s/it]\n",
    "\n",
    "      ==================================================\n",
    "      ✅ 평가 완료!\n",
    "      ==================================================\n",
    "\n",
    "      결과: <ExperimentResults RAG_EVAL_K7-3a35bd45>\n",
    "\n",
    "      model.safetensors: 100%|███████████████████████████████████████████████████████████████████| 2.27G/2.27G [02:14<00:00, 16.9MB/s]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f5e95d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06ace2f",
   "metadata": {},
   "source": [
    "* **`현재 상황 분석`**\n",
    "\n",
    "  | 항목 | 개수 | 정확도 |\n",
    "  |------|------|--------|\n",
    "  | **CORRECT** | 3/5 | 60% |\n",
    "  | **INCORRECT** | 2/5 | 40% |\n",
    "\n",
    "* 진전\n",
    "  * 이전: 1/5 (20%)\n",
    "  * 현재: 3/5 (60%)\n",
    "  * **향상: +200%!**\n",
    "\n",
    "* **`RAG` 평가**\n",
    "\n",
    "  * 어려움\n",
    "\n",
    "    | 이유 | 설명 |\n",
    "    |------|------|\n",
    "    | **초기 정확도** | 20-40%는 정상 |\n",
    "    | **반복 개선** | 60% → 80% → 90%로 점진적 향상 |\n",
    "    | **완벽은 없음** | 100%는 거의 불가능 |\n",
    "\n",
    "<br>\n",
    "\n",
    "* \n",
    "\n",
    "  * 현재 단계\n",
    "\n",
    "    ```markdown\n",
    "        일반적인 RAG 평가 발전:\n",
    "\n",
    "        1단계: 20-40% (초기)\n",
    "        2단계: 50-60% (개선 후)  ←  여기!\n",
    "        3단계: 70-80% (최적화)\n",
    "        4단계: 80-90% (완성)\n",
    "    ```\n",
    "\n",
    "<br>\n",
    "\n",
    "* **`향상 방법`**\n",
    "\n",
    "  * **a. `k값 증가`**\n",
    "\n",
    "  * **b. `chunk_size` 조정하기**\n",
    "\n",
    "  * **c. `프롬프트 개선하기`**\n",
    "\n",
    "  * **d. `다른 임베딩 모델 시도하기`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77e6d4d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270678d7",
   "metadata": {},
   "source": [
    "#### **3) `Context 에 기반한 답변 Evaluator`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7afa6d",
   "metadata": {},
   "source": [
    "* **`LangChainStringEvaluator(\"context_qa\")`**: `LLM` 체인에 정확성을 판단하는 데 참조 **`\"context\"`** 를 사용 → 지시\n",
    "\n",
    "* **`LangChainStringEvaluator(\"cot_qa\")`**: \n",
    "  * `\"cot_qa\"` = `\"context_qa\"` 평가자와 유사\n",
    "  * `but` 최종 판결을 결정하기 전에 `LLM` 의 `'추론'`을 사용하도록 지시한다는 점에서 차이 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4d8e4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c9ae99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132b8f52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2756a0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f957c44e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b77f17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e2f3fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dc1748",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9fbfd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c62f0b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaa9472",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9447887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'참고 자료에 해당 정보가 없습니다.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# query_1 \n",
    "# 질문에 대한 답변 생성\n",
    "chain.invoke(\"삼성전자가 자체 개발한 생성형 AI의 이름은 무엇인가요?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec9e8e6",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* **`chain.invoke()`** - (`0.6s`)\n",
    "\n",
    "    ```markdown\n",
    "\n",
    "    '주어진 문서에서 삼성전자가 자체 개발한 생성형 AI의 이름에 대한 정보는 찾을 수 없습니다.'\n",
    "\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c794d03",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d3fd2b",
   "metadata": {},
   "source": [
    "* **`디버깅하기`**\n",
    "\n",
    "  * `RAG 체인`의 핵심 구성 요소 = **`문서 로드`, `분할`, `임베딩`, `검색`(Retrieval) 단계**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "201dbc16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1760439003.197106 7875803 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> BAAI/bge-small-en-v1.5 모델을 사용하여 임베딩을 생성 중...\n",
      ">>> 벡터 데이터베이스 생성이 완료되었습니다.\n",
      "검색된 문서 개수: 3\n",
      "영국을 AI 안전 연구의 글로벌 허브로 확립하는 것을 목표로 함\n",
      "∙영국 정부는 향후 10년간 연구소에 공공자금을 투자해 연구를 지원할 계획으로, 연구소는 △첨단 AI 시스템 \n",
      "평가 개발과 시행 △AI 안전 연구 촉진 △정보 교류 활성화를 핵심 기능으로 함\n",
      "n (첨단 AI 시스템 평가 개발과 시행) 시스템의 안전 관련 속성을 중심으로 안전과 보안 기능을 이해\n",
      "하고 사회적 영향을 평가\n",
      "∙평가 우선순위는 △사이버범죄 조장, 허위 정보 유포 등 악의적으로 활용될 수 있는 기능 △사회에 미치는\n"
     ]
    }
   ],
   "source": [
    "# 임베딩 및 리트리버 테스트\n",
    "\n",
    "# 1. PDFRAG 객체 생성 시점까지 실행\n",
    "# file_path = \"../15_Evaluations/data/SPRI_AI_Brief_2023년12월호_F.pdf\"\n",
    "# llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\", temperature=0)\n",
    "\n",
    "rag = PDFRAG(\n",
    "    file_path=\"../15_Evaluations/data/SPRI_AI_Brief_2023년12월호_F.pdf\",\n",
    "    llm=ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\", temperature=0))\n",
    "\n",
    "# 2. 리트리버 객체 가져오기\n",
    "retriever = rag.create_retriever()\n",
    "\n",
    "# 3. 검색 테스트: \n",
    "#   -> 이 부분에서 오류가 발생하면 **임베딩/VectorStore** 문제\n",
    "docs = retriever.invoke(\"삼성전자가 자체 개발한 생성형 AI의 이름은 무엇인가요?\")\n",
    "\n",
    "# 4. 결과 출력: Document 객체 리스트가 출력되어야 함\n",
    "print(f\"검색된 문서 개수: {len(docs)}\")\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef92afaf",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* **`RAG 초기화 완료: 로컬 임베딩과 LangChain LLM 사용`** - (`11.3s`)\n",
    "\n",
    "    ```markdown\n",
    "\n",
    "    검색된 문서 개수: 4\n",
    "    1. 정책/법제  \n",
    "    2. 기업/산업 \n",
    "    3. 기술/연구 \n",
    "    4. 인력/교육\n",
    "    영국 과학혁신기술부, AI 안전 연구소 설립 발표\n",
    "    n 영국 과학혁신기술부가 첨단 AI 시스템에 대한 평가를 통해 안전성을 보장하기 위한 AI \n",
    "    안전 연구소를 설립한다고 발표\n",
    "    n AI 안전 연구소는 핵심 기능으로 첨단 AI 시스템 평가 개발과 시행, AI 안전 연구 촉진, \n",
    "    정보교류 활성화를 추진할 계획\n",
    "    KEY Contents\n",
    "\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177163a6",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 오류 발생 원인 추론 - *(주로 `LLM` 호출 단계에 많음)*\n",
    "\n",
    "  * `LLM API Key` or 환경 변수 문제\n",
    "    * `llm 객체 초기화` 단계에서 `API key`가 제대로 설정되지 않았을 경우\n",
    "    * 디버깅에서 `llm 객체` 생성 과정에서 해결되었을 가능성\n",
    "    \n",
    "    ```python\n",
    "\n",
    "    # 환경변수 설정 확인\n",
    "    # ChatGoogleGenerativeAI()의 경우\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = \"YOUR_API_KEY\"\n",
    "\n",
    "    # ChatOpenAI()의 경우\n",
    "    os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n",
    "\n",
    "    # llm 객체 초기화\n",
    "    llm = ChatGoogleGenerativeAI(...)\n",
    "    # 혹은\n",
    "    llm = ChatOpenAI(...)\n",
    "\n",
    "    ```\n",
    "\n",
    "<br>\n",
    "\n",
    "* \n",
    "  * `Pydantic` 버전 충돌 문제\n",
    "    * `myrag.py` 파일 = `Pydantic v1` 환경 작성된 코드 → 현재 실행 환경이 `Pydantic v2` 요구한다면 실패\n",
    "    * 로컬 임베딩 모델인 `HuggingFaceEmbeddings`로 교체 → 필요한 패키지 설치되었는지 확인 \n",
    "\n",
    "    ```bash\n",
    "\n",
    "    # 패키지 설치 필요한 경우\n",
    "    pip install sentence-transformers   \n",
    "\n",
    "    # 패키지 확인 필요한 경우\n",
    "    pip show sentence-transformers\n",
    "\n",
    "    ```\n",
    "\n",
    "  * * 설치 확인 과정에서 해결되었을 가능성\n",
    "\n",
    "<br>\n",
    "\n",
    "* \n",
    "  * `템플릿 변수 누락` or `포맷 오류`\n",
    "    * `retriever`가 검색한 `context`는 잘 나왔지만, **`question` 키에 해당하는 질문이 `chain`에 전달되지 않아** 프롬프트 템플릿 포맷팅 실패 가능성\n",
    "\n",
    "    ```python\n",
    "    # 정상\n",
    "    rag_chain.invoke(question_string)\n",
    "\n",
    "    # 실패한 포맷\n",
    "    rag_chain.invoke()\n",
    "\n",
    "    # 디버깅\n",
    "    # 정상적인 문자열로 포맷팅\n",
    "    docs = retriever.invoke(\"삼성전자가 자체 개발한 생성형 AI의 이름은 무엇인가요?\")\n",
    "    \n",
    "    ```\n",
    "\n",
    "    * 디버깅 코드 실행 **→ `명확하게 입력 문자열 전달`** → 입력 누락 해결되었을 가능성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6aa196f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97bcefb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jay/.pyenv/versions/lc_env/lib/python3.13/site-packages/pydantic/_internal/_config.py:373: UserWarning: Valid config keys have changed in V2:\n",
      "* 'allow_population_by_field_name' has been renamed to 'validate_by_name'\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    },
    {
     "ename": "PydanticUserError",
     "evalue": "The `__modify_schema__` method is not supported in Pydantic v2. Use `__get_pydantic_json_schema__` instead in class `SecretStr`.\n\nFor further information visit https://errors.pydantic.dev/2.11/u/custom-json-schema",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPydanticUserError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmyrag\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PDFRAG\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_openai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdotenv\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[32m      5\u001b[39m load_dotenv()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/lc_env/lib/python3.13/site-packages/langchain_openai/__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_openai\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchat_models\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AzureChatOpenAI, ChatOpenAI\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_openai\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01membeddings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AzureOpenAIEmbeddings, OpenAIEmbeddings\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_openai\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AzureOpenAI, OpenAI\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/lc_env/lib/python3.13/site-packages/langchain_openai/chat_models/__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_openai\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchat_models\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mazure\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AzureChatOpenAI\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_openai\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchat_models\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[32m      4\u001b[39m __all__ = [\u001b[33m\"\u001b[39m\u001b[33mChatOpenAI\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mAzureChatOpenAI\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/lc_env/lib/python3.13/site-packages/langchain_openai/chat_models/azure.py:41\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunction_calling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m convert_to_openai_tool\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpydantic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_basemodel_subclass\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_openai\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchat_models\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseChatOpenAI\n\u001b[32m     43\u001b[39m logger = logging.getLogger(\u001b[34m__name__\u001b[39m)\n\u001b[32m     46\u001b[39m _BM = TypeVar(\u001b[33m\"\u001b[39m\u001b[33m_BM\u001b[39m\u001b[33m\"\u001b[39m, bound=BaseModel)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/lc_env/lib/python3.13/site-packages/langchain_openai/chat_models/base.py:379\u001b[39m\n\u001b[32m    375\u001b[39m     parsed: Optional[_DictOrPydantic]\n\u001b[32m    376\u001b[39m     parsing_error: Optional[\u001b[38;5;167;01mBaseException\u001b[39;00m]\n\u001b[32m--> \u001b[39m\u001b[32m379\u001b[39m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43;01mBaseChatOpenAI\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mBaseChatModel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mAny\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mField\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdefault\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m#: :meta private:\u001b[39;49;00m\n\u001b[32m    381\u001b[39m \u001b[43m    \u001b[49m\u001b[43masync_client\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mAny\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mField\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdefault\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m#: :meta private:\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/lc_env/lib/python3.13/site-packages/pydantic/_internal/_model_construction.py:237\u001b[39m, in \u001b[36mModelMetaclass.__new__\u001b[39m\u001b[34m(mcs, cls_name, bases, namespace, __pydantic_generic_metadata__, __pydantic_reset_parent_namespace__, _create_model_module, **kwargs)\u001b[39m\n\u001b[32m    233\u001b[39m     set_model_mocks(\u001b[38;5;28mcls\u001b[39m)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Any operation that requires accessing the field infos instances should be put inside\u001b[39;00m\n\u001b[32m    236\u001b[39m     \u001b[38;5;66;03m# `complete_model_class()`:\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m     \u001b[43mcomplete_model_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig_wrapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mraise_errors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mns_resolver\u001b[49m\u001b[43m=\u001b[49m\u001b[43mns_resolver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcreate_model_module\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_create_model_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config_wrapper.frozen \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m__hash__\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m namespace:\n\u001b[32m    246\u001b[39m     set_default_hash_func(\u001b[38;5;28mcls\u001b[39m, bases)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/lc_env/lib/python3.13/site-packages/pydantic/_internal/_model_construction.py:597\u001b[39m, in \u001b[36mcomplete_model_class\u001b[39m\u001b[34m(cls, config_wrapper, raise_errors, ns_resolver, create_model_module)\u001b[39m\n\u001b[32m    590\u001b[39m gen_schema = GenerateSchema(\n\u001b[32m    591\u001b[39m     config_wrapper,\n\u001b[32m    592\u001b[39m     ns_resolver,\n\u001b[32m    593\u001b[39m     typevars_map,\n\u001b[32m    594\u001b[39m )\n\u001b[32m    596\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m597\u001b[39m     schema = \u001b[43mgen_schema\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    598\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m PydanticUndefinedAnnotation \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    599\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m raise_errors:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/lc_env/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:711\u001b[39m, in \u001b[36mGenerateSchema.generate_schema\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    708\u001b[39m schema = \u001b[38;5;28mself\u001b[39m._generate_schema_from_get_schema_method(obj, obj)\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m711\u001b[39m     schema = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_schema_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    713\u001b[39m metadata_js_function = _extract_get_pydantic_json_schema(obj)\n\u001b[32m    714\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m metadata_js_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/lc_env/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:1004\u001b[39m, in \u001b[36mGenerateSchema._generate_schema_inner\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m   1002\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m lenient_issubclass(obj, BaseModel):\n\u001b[32m   1003\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model_type_stack.push(obj):\n\u001b[32m-> \u001b[39m\u001b[32m1004\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_model_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, PydanticRecursiveRef):\n\u001b[32m   1007\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m core_schema.definition_reference_schema(schema_ref=obj.type_ref)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/lc_env/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:837\u001b[39m, in \u001b[36mGenerateSchema._model_schema\u001b[39m\u001b[34m(self, cls)\u001b[39m\n\u001b[32m    825\u001b[39m     model_schema = core_schema.model_schema(\n\u001b[32m    826\u001b[39m         \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m    827\u001b[39m         inner_schema,\n\u001b[32m   (...)\u001b[39m\u001b[32m    833\u001b[39m         ref=model_ref,\n\u001b[32m    834\u001b[39m     )\n\u001b[32m    835\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    836\u001b[39m     fields_schema: core_schema.CoreSchema = core_schema.model_fields_schema(\n\u001b[32m--> \u001b[39m\u001b[32m837\u001b[39m         {k: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_md_field_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecorators\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m fields.items()},\n\u001b[32m    838\u001b[39m         computed_fields=[\n\u001b[32m    839\u001b[39m             \u001b[38;5;28mself\u001b[39m._computed_field_schema(d, decorators.field_serializers)\n\u001b[32m    840\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m computed_fields.values()\n\u001b[32m    841\u001b[39m         ],\n\u001b[32m    842\u001b[39m         extras_schema=extras_schema,\n\u001b[32m    843\u001b[39m         extras_keys_schema=extras_keys_schema,\n\u001b[32m    844\u001b[39m         model_name=\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m,\n\u001b[32m    845\u001b[39m     )\n\u001b[32m    846\u001b[39m     inner_schema = apply_validators(fields_schema, decorators.root_validators.values(), \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    847\u001b[39m     inner_schema = apply_model_validators(inner_schema, model_validators, \u001b[33m'\u001b[39m\u001b[33minner\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/lc_env/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:1206\u001b[39m, in \u001b[36mGenerateSchema._generate_md_field_schema\u001b[39m\u001b[34m(self, name, field_info, decorators)\u001b[39m\n\u001b[32m   1199\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate_md_field_schema\u001b[39m(\n\u001b[32m   1200\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1201\u001b[39m     name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   1202\u001b[39m     field_info: FieldInfo,\n\u001b[32m   1203\u001b[39m     decorators: DecoratorInfos,\n\u001b[32m   1204\u001b[39m ) -> core_schema.ModelField:\n\u001b[32m   1205\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Prepare a ModelField to represent a model field.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1206\u001b[39m     common_field = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_common_field_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfield_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecorators\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1207\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m core_schema.model_field(\n\u001b[32m   1208\u001b[39m         common_field[\u001b[33m'\u001b[39m\u001b[33mschema\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m   1209\u001b[39m         serialization_exclude=common_field[\u001b[33m'\u001b[39m\u001b[33mserialization_exclude\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m   1213\u001b[39m         metadata=common_field[\u001b[33m'\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m   1214\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/lc_env/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:1372\u001b[39m, in \u001b[36mGenerateSchema._common_field_schema\u001b[39m\u001b[34m(self, name, field_info, decorators)\u001b[39m\n\u001b[32m   1368\u001b[39m         schema = \u001b[38;5;28mself\u001b[39m._apply_annotations(\n\u001b[32m   1369\u001b[39m             source_type, annotations + validators_from_decorators, transform_inner_schema=set_discriminator\n\u001b[32m   1370\u001b[39m         )\n\u001b[32m   1371\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1372\u001b[39m         schema = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply_annotations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1373\u001b[39m \u001b[43m            \u001b[49m\u001b[43msource_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1374\u001b[39m \u001b[43m            \u001b[49m\u001b[43mannotations\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidators_from_decorators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1375\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1377\u001b[39m \u001b[38;5;66;03m# This V1 compatibility shim should eventually be removed\u001b[39;00m\n\u001b[32m   1378\u001b[39m \u001b[38;5;66;03m# push down any `each_item=True` validators\u001b[39;00m\n\u001b[32m   1379\u001b[39m \u001b[38;5;66;03m# note that this won't work for any Annotated types that get wrapped by a function validator\u001b[39;00m\n\u001b[32m   1380\u001b[39m \u001b[38;5;66;03m# but that's okay because that didn't exist in V1\u001b[39;00m\n\u001b[32m   1381\u001b[39m this_field_validators = filter_field_decorator_info_by_field(decorators.validators.values(), name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/lc_env/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:2297\u001b[39m, in \u001b[36mGenerateSchema._apply_annotations\u001b[39m\u001b[34m(self, source_type, annotations, transform_inner_schema)\u001b[39m\n\u001b[32m   2292\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   2293\u001b[39m     get_inner_schema = \u001b[38;5;28mself\u001b[39m._get_wrapped_inner_schema(\n\u001b[32m   2294\u001b[39m         get_inner_schema, annotation, pydantic_js_annotation_functions\n\u001b[32m   2295\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2297\u001b[39m schema = \u001b[43mget_inner_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2298\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pydantic_js_annotation_functions:\n\u001b[32m   2299\u001b[39m     core_metadata = schema.setdefault(\u001b[33m'\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m'\u001b[39m, {})\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/lc_env/lib/python3.13/site-packages/pydantic/_internal/_schema_generation_shared.py:83\u001b[39m, in \u001b[36mCallbackGetCoreSchemaHandler.__call__\u001b[39m\u001b[34m(self, source_type)\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, source_type: Any, /) -> core_schema.CoreSchema:\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m     schema = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._ref_mode == \u001b[33m'\u001b[39m\u001b[33mto-def\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m     85\u001b[39m         ref = schema.get(\u001b[33m'\u001b[39m\u001b[33mref\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/lc_env/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:2279\u001b[39m, in \u001b[36mGenerateSchema._apply_annotations.<locals>.inner_handler\u001b[39m\u001b[34m(obj)\u001b[39m\n\u001b[32m   2276\u001b[39m schema = \u001b[38;5;28mself\u001b[39m._generate_schema_from_get_schema_method(obj, source_type)\n\u001b[32m   2278\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2279\u001b[39m     schema = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_schema_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2281\u001b[39m metadata_js_function = _extract_get_pydantic_json_schema(obj)\n\u001b[32m   2282\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m metadata_js_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/lc_env/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:1009\u001b[39m, in \u001b[36mGenerateSchema._generate_schema_inner\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m   1006\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, PydanticRecursiveRef):\n\u001b[32m   1007\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m core_schema.definition_reference_schema(schema_ref=obj.type_ref)\n\u001b[32m-> \u001b[39m\u001b[32m1009\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmatch_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/lc_env/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:1123\u001b[39m, in \u001b[36mGenerateSchema.match_type\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m   1121\u001b[39m origin = get_origin(obj)\n\u001b[32m   1122\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m origin \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1123\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_match_generic_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morigin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1125\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._arbitrary_types:\n\u001b[32m   1126\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._arbitrary_type_schema(obj)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/lc_env/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:1146\u001b[39m, in \u001b[36mGenerateSchema._match_generic_type\u001b[39m\u001b[34m(self, obj, origin)\u001b[39m\n\u001b[32m   1144\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._type_alias_type_schema(obj)\n\u001b[32m   1145\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m is_union_origin(origin):\n\u001b[32m-> \u001b[39m\u001b[32m1146\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_union_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1147\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m origin \u001b[38;5;129;01min\u001b[39;00m TUPLE_TYPES:\n\u001b[32m   1148\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tuple_schema(obj)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/lc_env/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:1434\u001b[39m, in \u001b[36mGenerateSchema._union_schema\u001b[39m\u001b[34m(self, union_type)\u001b[39m\n\u001b[32m   1432\u001b[39m         nullable = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1433\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1434\u001b[39m         choices.append(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   1436\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(choices) == \u001b[32m1\u001b[39m:\n\u001b[32m   1437\u001b[39m     s = choices[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/lc_env/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:713\u001b[39m, in \u001b[36mGenerateSchema.generate_schema\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    711\u001b[39m     schema = \u001b[38;5;28mself\u001b[39m._generate_schema_inner(obj)\n\u001b[32m--> \u001b[39m\u001b[32m713\u001b[39m metadata_js_function = \u001b[43m_extract_get_pydantic_json_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    714\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m metadata_js_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    715\u001b[39m     metadata_schema = resolve_original_schema(schema, \u001b[38;5;28mself\u001b[39m.defs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/lc_env/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:2641\u001b[39m, in \u001b[36m_extract_get_pydantic_json_schema\u001b[39m\u001b[34m(tp)\u001b[39m\n\u001b[32m   2639\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_custom_v2_modify_js_func:\n\u001b[32m   2640\u001b[39m         cls_name = \u001b[38;5;28mgetattr\u001b[39m(tp, \u001b[33m'\u001b[39m\u001b[33m__name__\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2641\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m PydanticUserError(\n\u001b[32m   2642\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mThe `__modify_schema__` method is not supported in Pydantic v2. \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   2643\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mUse `__get_pydantic_json_schema__` instead\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m in class `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcls_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mcls_name\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   2644\u001b[39m             code=\u001b[33m'\u001b[39m\u001b[33mcustom-json-schema\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   2645\u001b[39m         )\n\u001b[32m   2647\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (origin := get_origin(tp)) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2648\u001b[39m     \u001b[38;5;66;03m# Generic aliases proxy attribute access to the origin, *except* dunder attributes,\u001b[39;00m\n\u001b[32m   2649\u001b[39m     \u001b[38;5;66;03m# such as `__get_pydantic_json_schema__`, hence the explicit check.\u001b[39;00m\n\u001b[32m   2650\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _extract_get_pydantic_json_schema(origin)\n",
      "\u001b[31mPydanticUserError\u001b[39m: The `__modify_schema__` method is not supported in Pydantic v2. Use `__get_pydantic_json_schema__` instead in class `SecretStr`.\n\nFor further information visit https://errors.pydantic.dev/2.11/u/custom-json-schema"
     ]
    }
   ],
   "source": [
    "from myrag import PDFRAG\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# OpenAI LLM\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8569077f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c494dc8d",
   "metadata": {},
   "source": [
    "* **`ask_question()` 함수 생성하기** \n",
    "\n",
    "  * 입력 = `inputs` = `딕셔너리`\n",
    "  * 출력 = `answer` = `딕셔너리`를 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f5fa9960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문에 대한 답변하는 함수를 생성\n",
    "def ask_question(inputs: dict):\n",
    "    return {\"answer\": chain.invoke(inputs[\"question\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ac37d9f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': '주어진 문서에서 삼성전자가 자체 개발한 생성형 AI의 이름에 대한 정보는 찾을 수 없습니다.'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 사용자 질문 예시\n",
    "llm_answer = ask_question(\n",
    "    {\"question\": \"삼성전자가 자체 개발한 생성형 AI의 이름은 무엇인가요?\"}\n",
    ")\n",
    "llm_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19fccce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b92354e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b125ab30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57fd990",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b0d2cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a911d77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2d5f2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9527f3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa373788",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc26f35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afff134",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8167ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb8ba2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb74f9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0294ba9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45cfc33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8d3f45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c9a969",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc2a9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 질문과 답변 목록\n",
    "inputs = [\n",
    "    \"삼성전자가 만든 생성형 AI의 이름은 무엇인가요?\",\n",
    "    \"미국 바이든 대통령이 안전하고 신뢰할 수 있는 AI 개발과 사용을 보장하기 위한 행정명령을 발표한 날은 언제인가요?\",\n",
    "    \"코히어의 데이터 출처 탐색기에 대해서 간략히 말해주세요.\",\n",
    "]\n",
    "\n",
    "# 질문에 대한 답변 목록\n",
    "outputs = [\n",
    "    \"삼성전자가 만든 생성형 AI의 이름은 삼성 가우스 입니다.\",\n",
    "    \"2023년 10월 30일 미국 바이든 대통령이 행정명령을 발표했습니다.\",\n",
    "    \"코히어의 데이터 출처 탐색기는 AI 모델 훈련에 사용되는 데이터셋의 출처와 라이선스 상태를 추적하고 투명성을 확보하기 위한 플랫폼입니다. 12개 기관과 협력하여 2,000여 개 데이터셋의 출처 정보를 제공하며, 개발자들이 데이터의 구성과 계보를 쉽게 파악할 수 있게 돕습니다.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b4c167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문과 답변 쌍 생성\n",
    "qa_pairs = [{\"question\": q, \"answer\": a} for q, a in zip(inputs, outputs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296e9426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터프레임으로 변환\n",
    "df = pd.DataFrame(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf1d2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터프레임 출력\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe08182",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* `df.head()`\n",
    "\n",
    "  * ![df.head()](../15_Evaluations/assets/df_head_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e206505",
   "metadata": {},
   "source": [
    "##### **`➁ 이전에서 생성한`** ***`Synthetic Dataset`*** **`활용하기`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365cc82b",
   "metadata": {},
   "source": [
    "* 업로드했던 `HuggingFace Dataset` 활용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d54310",
   "metadata": {},
   "source": [
    "* 사전에 `VS Code` 터미널에 설치할 것\n",
    "\n",
    "```bash\n",
    "            pip install -qU datasets\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0392c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "import os\n",
    "\n",
    "# huggingface Dataset에서 repo_id로 데이터셋 다운로드\n",
    "dataset = load_dataset(\n",
    "    \"livemylife23/rag-synthetic-dataset-korean\",   # 데이터셋 이름\n",
    "    token=os.environ[\"HUGGINGFACEHUB_API_TOKEN\"],  # private 데이터인 경우 필요\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae21461",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 다운로드 완료 - (`7.1s`)\n",
    "\n",
    "  * ![hugginface_dataset_download](../15_Evaluations/assets/hugging_face_download.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14854914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋에서 split 기준으로 조회\n",
    "huggingface_df = dataset[\"train\"].to_pandas()\n",
    "huggingface_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b1a435",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "  * `huggingface_df.head()`\n",
    "\n",
    "    * ![huggingface_df.head()](../15_Evaluations/assets/df_head_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701a711e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3db68a",
   "metadata": {},
   "source": [
    "#### **2) `LangSmith 테스트를 위한 Dataset 생성`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb905c0",
   "metadata": {},
   "source": [
    "* **`Dataset & Testing`에 새로운 데이터셋 생성하기**\n",
    "\n",
    "<small>\n",
    "\n",
    "  * ![langsmith에 새로운 데이터셋 생성하기](../15_Evaluations/assets/eval-06.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a8bae4",
   "metadata": {},
   "source": [
    "* `csv` 파일에서 `LangSmith UI` 사용 → 직접 데이터셋 생성 가능\n",
    "\n",
    "* *참고: [`LangSmith UI 문서`](https://docs.langchain.com/langsmith/manage-datasets)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615871d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "dataset_name = \"RAG_EVAL_DATASET\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509aaa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 생성 함수\n",
    "\n",
    "def create_dataset(client, dataset_name, description=None):\n",
    "    for dataset in client.list_datasets():\n",
    "        if dataset.name == dataset_name:\n",
    "            return dataset\n",
    "\n",
    "    dataset = client.create_dataset(\n",
    "        dataset_name=dataset_name,\n",
    "        description=description,\n",
    "    )\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea80981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 생성\n",
    "dataset = create_dataset(client, dataset_name)          # 2.9s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef9d52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 생성된 데이터셋에 예제 추가\n",
    "\n",
    "client.create_examples(\n",
    "    inputs=[{\"question\": q} for q in df[\"question\"].tolist()],\n",
    "    outputs=[{\"answer\": a} for a in df[\"answer\"].tolist()],\n",
    "    dataset_id=dataset.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe3ee15",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* **`client.creat_examples() 추가하기`** - (`0.1s`)\n",
    "\n",
    "    ```python\n",
    "    {'example_ids': ['8d34ad4e-e3be-47ca-9a6a-9929608b60ca',\n",
    "    '71841ffc-cfbe-4e05-b0b0-ef85e8a6ebd4',\n",
    "    'e0d645a9-6d97-41da-8d45-cc5e8d6f0a37'],\n",
    "    'count': 3}\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3df2b1a",
   "metadata": {},
   "source": [
    "* **`데이터셋에 예제 나중에 추가 가능`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89400605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새로운 질문 목록\n",
    "new_questions = [\n",
    "    \"삼성전자가 만든 생성형 AI의 이름은 무엇인가요?\",\n",
    "    \"구글이 테디노트에게 20억달러를 투자한 것이 사실입니까?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1612eec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새로운 답변 목록\n",
    "new_answers = [\n",
    "    \"삼성전자가 만든 생성형 AI의 이름은 테디노트 입니다.\",\n",
    "    \"사실이 아닙니다. 구글은 앤스로픽에 최대 20억 달러를 투자하기로 합의했으며, 이 중 5억 달러를 우선 투자하고 향후 15억 달러를 추가로 투자하기로 했습니다.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7cb209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UI에서 업데이트된 버전 확인\n",
    "\n",
    "client.create_examples(\n",
    "    inputs=[{\"question\": q} for q in new_questions],\n",
    "    outputs=[{\"answer\": a} for a in new_answers],\n",
    "    dataset_id=dataset.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e8351c",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* **`UI에서 업데이트된 버전 확인하기`** - (`0.1s`)\n",
    "\n",
    "    ```python\n",
    "    {'example_ids': ['7080404f-878d-4e22-9256-70a5d1b86ab3',\n",
    "    'b8599d26-9990-4153-8a92-0dc779087f2c'],\n",
    "    'count': 2}\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9215acbf",
   "metadata": {},
   "source": [
    "* **✓ `데이터셋 준비 완료됨`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a333ef40",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff26a34",
   "metadata": {},
   "source": [
    "* next: ***`05. LLM-as-Judge`***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719f0c8c",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc_eval_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
