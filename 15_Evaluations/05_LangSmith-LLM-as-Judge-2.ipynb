{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "574f6502",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf366bf9",
   "metadata": {},
   "source": [
    "* ì¶œì²˜: LangChain ê³µì‹ ë¬¸ì„œ ë˜ëŠ” í•´ë‹¹ êµì¬ëª…\n",
    "* ì›ë³¸ URL: https://smith.langchain.com/hub/teddynote/summary-stuff-documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31f7552",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af770941",
   "metadata": {},
   "source": [
    "### **5. `05. LLM-as-Judge`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb2944b",
   "metadata": {},
   "source": [
    "#### **2) `Question-Answer Evaluator`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300b2621",
   "metadata": {},
   "source": [
    "* ê°€ì¥ ê¸°ë³¸ ê¸°ëŠ¥ì„ ê°€ì§„ `Evaluator` = `Query` - `Answer` í‰ê°€í•˜ê¸°\n",
    "\n",
    "* ì‚¬ìš©ì ì…ë ¥ = `input` â†’ `LLM`ì´ ìƒì„±í•œ ë‹µë³€ â†’ `prediction`ìœ¼ë¡œ ì •ë‹µ ë‹µë³€ì€ `reference`ë¡œ ì •ì˜ë¨\n",
    "\n",
    "  * `Prompt`ë³€ìˆ˜ \n",
    "    * `query`: ì§ˆë¬¸\n",
    "    * `result`: `LLM` ë‹µë³€\n",
    "    * `answer`: ì •ë‹µ ë‹µë³€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213e7ff9",
   "metadata": {},
   "source": [
    "* **ìƒˆë¡œìš´ ê°€ìƒí™˜ê²½ ìƒì„± - `lc_eval_env`**\n",
    "\n",
    "  * `Python-3.12`\n",
    "  * `Pydantic`: `ver 1.10.18`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa96e875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pydantic ë²„ì „: 1.10.18\n"
     ]
    }
   ],
   "source": [
    "import pydantic\n",
    "print(f\"Pydantic ë²„ì „: {pydantic.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0a0018",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* **`Pydantic ë²„ì „: 1.10.18`** - (`0.1s`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f802a885",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb9c7b9",
   "metadata": {},
   "source": [
    "* **`í™˜ê²½ ì„¤ì •`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3acf7ae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API í‚¤ë¥¼ í™˜ê²½ë³€ìˆ˜ë¡œ ê´€ë¦¬í•˜ê¸° ìœ„í•œ ì„¤ì • íŒŒì¼\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API í‚¤ ì •ë³´ ë¡œë“œ\n",
    "load_dotenv()                           # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9d33436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- LangSmith í™˜ê²½ ë³€ìˆ˜ í™•ì¸ ---\n",
      "âœ… LangSmith ì¶”ì  í™œì„±í™”ë¨ (LANGCHAIN_TRACING_V2='true')\n",
      "âœ… LangSmith í”„ë¡œì íŠ¸: 'LangChain-prantice'\n",
      "âœ… LangSmith API Key: ì„¤ì •ë¨\n",
      "  -> ì´ì œ LangSmith ëŒ€ì‹œë³´ë“œì—ì„œ ì´ í”„ë¡œì íŠ¸ë¥¼ í™•ì¸í•´ ë³´ì„¸ìš”.\n"
     ]
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "from langsmith import traceable\n",
    "\n",
    "import os\n",
    "\n",
    "# LangSmith í™˜ê²½ ë³€ìˆ˜ í™•ì¸\n",
    "\n",
    "print(\"\\n--- LangSmith í™˜ê²½ ë³€ìˆ˜ í™•ì¸ ---\")\n",
    "langchain_tracing_v2 = os.getenv('LANGCHAIN_TRACING_V2')\n",
    "langchain_project = os.getenv('LANGCHAIN_PROJECT')\n",
    "langchain_api_key_status = \"ì„¤ì •ë¨\" if os.getenv('LANGCHAIN_API_KEY') else \"ì„¤ì •ë˜ì§€ ì•ŠìŒ\" # API í‚¤ ê°’ì€ ì§ì ‘ ì¶œë ¥í•˜ì§€ ì•ŠìŒ\n",
    "\n",
    "if langchain_tracing_v2 == \"true\" and os.getenv('LANGCHAIN_API_KEY') and langchain_project:\n",
    "    print(f\"âœ… LangSmith ì¶”ì  í™œì„±í™”ë¨ (LANGCHAIN_TRACING_V2='{langchain_tracing_v2}')\")\n",
    "    print(f\"âœ… LangSmith í”„ë¡œì íŠ¸: '{langchain_project}'\")\n",
    "    print(f\"âœ… LangSmith API Key: {langchain_api_key_status}\")\n",
    "    print(\"  -> ì´ì œ LangSmith ëŒ€ì‹œë³´ë“œì—ì„œ ì´ í”„ë¡œì íŠ¸ë¥¼ í™•ì¸í•´ ë³´ì„¸ìš”.\")\n",
    "else:\n",
    "    print(\"âŒ LangSmith ì¶”ì ì´ ì™„ì „íˆ í™œì„±í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‹¤ìŒì„ í™•ì¸í•˜ì„¸ìš”:\")\n",
    "    if langchain_tracing_v2 != \"true\":\n",
    "        print(f\"  - LANGCHAIN_TRACING_V2ê°€ 'true'ë¡œ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤ (í˜„ì¬: '{langchain_tracing_v2}').\")\n",
    "    if not os.getenv('LANGCHAIN_API_KEY'):\n",
    "        print(\"  - LANGCHAIN_API_KEYê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
    "    if not langchain_project:\n",
    "        print(\"  - LANGCHAIN_PROJECTê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3cee3b",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* ì…€ ì¶œë ¥\n",
    "\n",
    "    ```bash\n",
    "    --- LangSmith í™˜ê²½ ë³€ìˆ˜ í™•ì¸ ---\n",
    "    âœ… LangSmith ì¶”ì  í™œì„±í™”ë¨ (LANGCHAIN_TRACING_V2='true')\n",
    "    âœ… LangSmith í”„ë¡œì íŠ¸: 'LangChain-prantice'\n",
    "    âœ… LangSmith API Key: ì„¤ì •ë¨\n",
    "    -> ì´ì œ LangSmith ëŒ€ì‹œë³´ë“œì—ì„œ ì´ í”„ë¡œì íŠ¸ë¥¼ í™•ì¸í•´ ë³´ì„¸ìš”.\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "36c37fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from myrag import PDFRAG                        # local ì„ë² ë”© ë²„ì „ìœ¼ë¡œ ìˆ˜ì •í•œ myrag.py ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# API í‚¤ í™•ì¸\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "GOOGLE_API_KEY=os.getenv(\"GOOGLE_API_KEY2\")\n",
    "\n",
    "if not os.getenv(\"GOOGLE_API_KEY2\"):\n",
    "    os.environ[\"GOOGLE_API_KEY2\"] = input(\"Enter your GOOGLE_API_KEY2: \")\n",
    "\n",
    "if \"GOOGLE_API_KEY2\" not in os.environ:\n",
    "    print(\"âŒ ê²½ê³ : GOOGLE_API_KEY2 í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë°˜ë“œì‹œ ì„¤ì •í•´ì•¼ gemini LLMì´ ì‘ë™í•©ë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5d0a2b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… gemini-2.5-flash-lite ì„±ê³µ!\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm2 = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\", temperature=0, api_key=os.getenv(\"GOOGLE_API_KEY2\"))\n",
    "\n",
    "print(\"âœ… gemini-2.5-flash-lite ì„±ê³µ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95081584",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* **`âœ… ì„±ê³µ!`** \n",
    "\n",
    "  * *`API í• ë‹¹ëŸ‰ ë¶€ì¡±ìœ¼ë¡œ ë‘ë²ˆì¨° ê³„ì •ìœ¼ë¡œ ë‹¤ì‹œ ì‹œë„`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1c51b692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ: 23ê°œ í˜ì´ì§€\n",
      "âœ… ë¬¸ì„œ ë¶„í•  ì™„ë£Œ: 43ê°œ ì²­í¬\n",
      "âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ: all-MiniLM-L6-v2\n",
      "âœ… ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ\n",
      "âœ… PDFRAG ìƒì„± ì„±ê³µ!\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm2 = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\", temperature=0, api_key=os.getenv(\"GOOGLE_API_KEY2\"))\n",
    "\n",
    "rag = PDFRAG(\n",
    "    \"../15_Evaluations/data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf\",\n",
    "    llm2,\n",
    ")\n",
    "\n",
    "print(\"âœ… PDFRAG ìƒì„± ì„±ê³µ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8dd84b",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* **`PDFRAG ìƒì„±`** - (`4.9s`)\n",
    "\n",
    "    ```markdown\n",
    "\n",
    "    âœ… ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ: 23ê°œ í˜ì´ì§€\n",
    "    âœ… ë¬¸ì„œ ë¶„í•  ì™„ë£Œ: 43ê°œ ì²­í¬\n",
    "    âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ: all-MiniLM-L6-v2\n",
    "    âœ… ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ\n",
    "    âœ… PDFRAG ìƒì„± ì„±ê³µ!\n",
    "\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2d43340c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ê²€ìƒ‰ê¸° ìƒì„± ì™„ë£Œ (k=4)\n"
     ]
    }
   ],
   "source": [
    "# ê²€ìƒ‰ê¸°(retriever) ìƒì„±\n",
    "retriever = rag.create_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344ce808",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* **`âœ… ê²€ìƒ‰ê¸° ìƒì„± ì™„ë£Œ (k=4)`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ac9d8a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… RAG ì²´ì¸ ìƒì„± ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# ì²´ì¸(chain) ìƒì„±\n",
    "chain = rag.create_chain(retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04637af",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* **`âœ… RAG ì²´ì¸ ìƒì„± ì™„ë£Œ`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "869c8197",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1760508400.597600 9355863 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì‚¼ì„±ì „ìê°€ ìì²´ ê°œë°œí•œ ìƒì„±í˜• AIì˜ ì´ë¦„ì€ 'ì‚¼ì„± ê°€ìš°ìŠ¤'ì…ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# ì§ˆë¬¸\n",
    "answer = chain.invoke(\"ì‚¼ì„±ì „ìê°€ ìì²´ ê°œë°œí•œ ìƒì„±í˜• AIì˜ ì´ë¦„ì€ ë¬´ì—‡ì¸ê°€ìš”?\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f68a2e",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* **`ì‚¼ì„±ì „ìê°€ ìì²´ ê°œë°œí•œ ìƒì„±í˜• AIì˜ ì´ë¦„ì€ 'ì‚¼ì„± ê°€ìš°ìŠ¤'ì…ë‹ˆë‹¤.`** - (`2.0s`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b47c2bcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': \"ì‚¼ì„±ì „ìê°€ ìì²´ ê°œë°œí•œ ìƒì„±í˜• AIì˜ ì´ë¦„ì€ 'ì‚¼ì„± ê°€ìš°ìŠ¤'ì…ë‹ˆë‹¤.\"}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€í•˜ëŠ” í•¨ìˆ˜ë¥¼ ìƒì„±\n",
    "def ask_question(inputs: dict):\n",
    "    return {\"answer\": chain.invoke(inputs[\"question\"])}\n",
    "\n",
    "# ì‚¬ìš©ì ì§ˆë¬¸ ì˜ˆì‹œ\n",
    "llm_answer = ask_question(\n",
    "    {\"question\": \"ì‚¼ì„±ì „ìê°€ ìì²´ ê°œë°œí•œ ìƒì„±í˜• AIì˜ ì´ë¦„ì€ ë¬´ì—‡ì¸ê°€ìš”?\"}\n",
    ")\n",
    "\n",
    "llm_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0efe342",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* **`ask_question()`** - (`1.2s`)\n",
    "\n",
    "    ```python\n",
    "\n",
    "        {'answer': \"ì‚¼ì„±ì „ìê°€ ìì²´ ê°œë°œí•œ ìƒì„±í˜• AIì˜ ì´ë¦„ì€ 'ì‚¼ì„± ê°€ìš°ìŠ¤'ì…ë‹ˆë‹¤.\"}\n",
    "\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "747018bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluator prompt ì¶œë ¥ì„ ìœ„í•œ í•¨ìˆ˜\n",
    "def print_evaluator_prompt(evaluator):\n",
    "    return evaluator.evaluator.prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bf06b760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a teacher grading a quiz.\n",
      "You are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\n",
      "\n",
      "Example Format:\n",
      "QUESTION: question here\n",
      "STUDENT ANSWER: student's answer here\n",
      "TRUE ANSWER: true answer here\n",
      "GRADE: CORRECT or INCORRECT here\n",
      "\n",
      "Grade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \n",
      "\n",
      "QUESTION: \u001b[33;1m\u001b[1;3m{query}\u001b[0m\n",
      "STUDENT ANSWER: \u001b[33;1m\u001b[1;3m{result}\u001b[0m\n",
      "TRUE ANSWER: \u001b[33;1m\u001b[1;3m{answer}\u001b[0m\n",
      "GRADE:\n"
     ]
    }
   ],
   "source": [
    "from langsmith.evaluation import evaluate, LangChainStringEvaluator\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# qa í‰ê°€ì ìƒì„±\n",
    "qa_evalulator = LangChainStringEvaluator(\n",
    "    \"qa\",\n",
    "    config={\n",
    "        \"llm\":ChatGoogleGenerativeAI(\n",
    "            model=\"gemini-2.5-flash-lite\", \n",
    "            temperature=0, \n",
    "            google_api_key=os.getenv(\"GOOGLE_API_KEY2\"))\n",
    "        }\n",
    ")\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ ì¶œë ¥\n",
    "print_evaluator_prompt(qa_evalulator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafb79c3",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* **`qa_evaluator`** - (`0.1s`)\n",
    "\n",
    "    ```markdown\n",
    "\n",
    "    You are a teacher grading a quiz.\n",
    "    You are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\n",
    "\n",
    "    Example Format:\n",
    "    QUESTION: question here\n",
    "    STUDENT ANSWER: student's answer here\n",
    "    TRUE ANSWER: true answer here\n",
    "    GRADE: CORRECT or INCORRECT here\n",
    "\n",
    "    Grade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \n",
    "\n",
    "    QUESTION: *{query}*\n",
    "    STUDENT ANSWER: *{result}*\n",
    "    TRUE ANSWER: *{answer}*\n",
    "    GRADE:\n",
    "\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54472586",
   "metadata": {},
   "source": [
    "* **`í‰ê°€ ì§„í–‰` â†’ ì¶œë ¥í•œ `URL` ì´ë™ â†’ `ê²°ê³¼ í™•ì¸í•˜ê¸°`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "448b0dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'RAG_EVAL-7ff2d7c9' at:\n",
      "https://smith.langchain.com/o/2c3342d3-1170-4ffa-86fd-f621199e0b9c/datasets/420dd308-2ebd-44c9-8ce8-9aff3886dc8e/compare?selectedSessions=6d370b7d-2aa7-4b8e-9d38-78c005eea377\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]E0000 00:00:1760508510.914989 9404755 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
      "E0000 00:00:1760508510.921383 9404756 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
      "5it [00:04,  1.02it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"RAG_EVAL_DATASET\"\n",
    "\n",
    "# í‰ê°€ ì‹¤í–‰\n",
    "experiment_results = evaluate(\n",
    "    ask_question,\n",
    "    data=dataset_name,\n",
    "    evaluators=[qa_evalulator],\n",
    "    experiment_prefix=\"RAG_EVAL\",\n",
    "    # ì‹¤í—˜ ë©”íƒ€ë°ì´í„° ì§€ì •\n",
    "    metadata={\n",
    "        \"variant\": \"QA Evaluator ë¥¼ í™œìš©í•œ í‰ê°€\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992242c1",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* ê²°ê³¼ í™•ì¸í•˜ê¸°\n",
    "\n",
    "  * `RAG_EVAL` / *`COMPILED`* - (*`45s`*)\n",
    "  * ![`RAG_EVAL_COMPILED`](../15_Evaluations/assets/RAG_EVAL_1.png)\n",
    "\n",
    "  * \n",
    "\n",
    "  * `RAG_EVAL` / *`CORRECT=1`, `INCORRECT=4`* - (*`7.8s`*)\n",
    "  * ![`RAG_EVAL_MORE_INCORRECT`](../15_Evaluations/assets/RAG_EVAL_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa2a070",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1775b968",
   "metadata": {},
   "source": [
    "* **`RAG`** ì‹œìŠ¤í…œ ê°œì„ í•˜ê¸°\n",
    "\n",
    "  * **a. `Retriever` ê°œì„ í•˜ê¸°**\n",
    "  * **b. `Chunk Size` ì¡°ì •í•˜ê¸°**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96641657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ: 23ê°œ í˜ì´ì§€\n",
      "âœ… ë¬¸ì„œ ë¶„í•  ì™„ë£Œ: 72ê°œ ì²­í¬\n",
      "âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ: all-MiniLM-L6-v2\n",
      "âœ… ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# Chunk Size ì¡°ì •\n",
    "# ========================================\n",
    "from myrag2 import PDFRAG\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv() \n",
    "\n",
    "# Chunk Size ì¡°ì •\n",
    "rag2 = PDFRAG(\n",
    "    \"../15_Evaluations/data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf\",\n",
    "    ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-2.0-flash-lite\", \n",
    "        temperature=0, \n",
    "        google_api_key=os.getenv(\"GOOGLE_API_KEY2\")),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b022410",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* **`rag2 ìƒì„±`** - (`12.2s`)\n",
    "\n",
    "    ```mardkwon\n",
    "\n",
    "    âœ… ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ: 23ê°œ í˜ì´ì§€\n",
    "    âœ… ë¬¸ì„œ ë¶„í•  ì™„ë£Œ: 72ê°œ ì²­í¬\n",
    "    âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ: all-MiniLM-L6-v2\n",
    "    âœ… ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ\n",
    "\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f0b0126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ê²€ìƒ‰ê¸° ìƒì„± ì™„ë£Œ \n",
      "âœ… RAG ì²´ì¸ ìƒì„± ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# Top-K ì¦ê°€\n",
    "# ========================================\n",
    "\n",
    "# ê°œì„ \n",
    "retriever2 = rag2.create_retriever()        # myrag2.py ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "\n",
    "# ì²´ì¸ ì¬ìƒì„±\n",
    "chain2 = rag2.create_chain(retriever2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad59bb6c",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* **`âœ… ê²€ìƒ‰ê¸° ìƒì„± ì™„ë£Œ`**\n",
    "\n",
    "* **`âœ… RAG ì²´ì¸ ìƒì„± ì™„ë£Œ`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70e94e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1760512905.700200 9492174 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I am sorry, but the provided context does not contain the answer to your question.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±\n",
    "chain2.invoke(\"ì‚¼ì„±ì „ìê°€ ìì²´ ê°œë°œí•œ ìƒì„±í˜• AIì˜ ì´ë¦„ì€ ë¬´ì—‡ì¸ê°€ìš”?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24623124",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* **`'ì‚¼ì„±ì „ìê°€ ìì²´ ê°œë°œí•œ ìƒì„±í˜• AIì˜ ì´ë¦„ì€ ì œê³µëœ ë¬¸ì„œì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'`** - (`1.4s`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d2b1ec0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'ì‚¼ì„±ì „ìê°€ ìì²´ ê°œë°œí•œ ìƒì„±í˜• AIì˜ ì´ë¦„ì€ ì œê³µëœ ë¬¸ì„œì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€í•˜ëŠ” í•¨ìˆ˜ë¥¼ ìƒì„±\n",
    "def ask_question(inputs: dict):\n",
    "    return {\"answer\": chain2.invoke(inputs[\"question\"])}\n",
    "\n",
    "# ì‚¬ìš©ì ì§ˆë¬¸ ì˜ˆì‹œ\n",
    "llm_answer = ask_question(\n",
    "    {\"question\": \"ì‚¼ì„±ì „ìê°€ ìì²´ ê°œë°œí•œ ìƒì„±í˜• AIì˜ ì´ë¦„ì€ ë¬´ì—‡ì¸ê°€ìš”?\"}\n",
    ")\n",
    "\n",
    "llm_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d3d51e",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* **`ask_question()`** - (`1.3s`)\n",
    "\n",
    "    ```python\n",
    "\n",
    "    {'answer': 'ì‚¼ì„±ì „ìê°€ ìì²´ ê°œë°œí•œ ìƒì„±í˜• AIì˜ ì´ë¦„ì€ ì œê³µëœ ë¬¸ì„œì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'}\n",
    "\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7798a392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluator prompt ì¶œë ¥ì„ ìœ„í•œ í•¨ìˆ˜\n",
    "def print_evaluator_prompt(evaluator):\n",
    "    return evaluator.evaluator.prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a31c0c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a teacher grading a quiz.\n",
      "You are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\n",
      "\n",
      "Example Format:\n",
      "QUESTION: question here\n",
      "STUDENT ANSWER: student's answer here\n",
      "TRUE ANSWER: true answer here\n",
      "GRADE: CORRECT or INCORRECT here\n",
      "\n",
      "Grade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \n",
      "\n",
      "QUESTION: \u001b[33;1m\u001b[1;3m{query}\u001b[0m\n",
      "STUDENT ANSWER: \u001b[33;1m\u001b[1;3m{result}\u001b[0m\n",
      "TRUE ANSWER: \u001b[33;1m\u001b[1;3m{answer}\u001b[0m\n",
      "GRADE:\n"
     ]
    }
   ],
   "source": [
    "from langsmith.evaluation import evaluate, LangChainStringEvaluator\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "\n",
    "# qa í‰ê°€ì ìƒì„±\n",
    "qa_evalulator = LangChainStringEvaluator(\n",
    "    \"qa\",\n",
    "    config={\n",
    "        \"llm\":ChatGoogleGenerativeAI(\n",
    "            model=\"gemini-2.5-flash-lite\", \n",
    "            temperature=0, \n",
    "            google_api_key=os.getenv(\"GOOGLE_API_KEY2\"))\n",
    "        }\n",
    ")\n",
    "\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ ì¶œë ¥\n",
    "print_evaluator_prompt(qa_evalulator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ec610bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'RAG_EVAL_2-00c422bd' at:\n",
      "https://smith.langchain.com/o/2c3342d3-1170-4ffa-86fd-f621199e0b9c/datasets/420dd308-2ebd-44c9-8ce8-9aff3886dc8e/compare?selectedSessions=cfb85525-ad18-4c76-9e47-4b2c1beb95fa\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]E0000 00:00:1760510512.220081 9442399 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
      "5it [00:04,  1.12it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"RAG_EVAL_DATASET\"\n",
    "\n",
    "# í‰ê°€ ì‹¤í–‰\n",
    "experiment_results = evaluate(\n",
    "    ask_question,\n",
    "    data=dataset_name,\n",
    "    evaluators=[qa_evalulator],\n",
    "    experiment_prefix=\"RAG_EVAL_2\",\n",
    "    # ì‹¤í—˜ ë©”íƒ€ë°ì´í„° ì§€ì •\n",
    "    metadata={\n",
    "        \"variant\": \"myrag2.py ë°˜ì˜\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2154ce",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* **ê°œì„  ì‹œë„: `RAG_EVAL_2`**\n",
    "\n",
    "  ```bash\n",
    "\n",
    "  View the evaluation results for experiment: 'RAG_EVAL_2-00c422bd' at:\n",
    "  https://smith.langchain.com/o/2c3342d3-1170-4ffa-86fd-f621199e0b9c/datasets/420dd308-2ebd-44c9-8ce8-9aff3886dc8e/compare?selectedSessions=cfb85525-ad18-4c76-9e47-4b2c1beb95fa\n",
    "\n",
    "\n",
    "  0it [00:00, ?it/s]\n",
    "\n",
    "  5it [00:04,  1.12it/s]\n",
    "\n",
    "  ```\n",
    "\n",
    "* ê²°ê³¼ í™•ì¸í•˜ê¸°\n",
    "\n",
    "<br>\n",
    "\n",
    "* \n",
    "  * `RAG_EVAL` / *`CORRECT=2`, `INCORRECT=3`* - (*`7.3s`*)\n",
    "  * ![`RAG_EVAL_MORE_INCORRECT`](../15_Evaluations/assets/RAG_EVAL_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9620d711",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc52808b",
   "metadata": {},
   "source": [
    "* **`RAG`** ì‹œìŠ¤í…œ ê°œì„ í•˜ê¸°_2\n",
    "\n",
    "  * **c. `chunk_size` ì¡°ì •**\n",
    "  * **d. ì¦ì€ ì»¤ë„ ì¶©ëŒ â†’ `python` íŒŒì¼ë¡œ ì‹¤í–‰í•˜ê¸°**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a7f716",
   "metadata": {},
   "source": [
    "* c.\n",
    "\n",
    "  * `chunck_size` ì¡°ì ˆ test\n",
    "    * **`chunk_size` ì§€ì • âŒ** or **`chunk_size` = `4` â†’ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ**\n",
    "      * *`'ì£„ì†¡í•©ë‹ˆë‹¤. ì œê³µëœ ë¬¸ì„œì—ì„œ ì‚¼ì„±ì „ìê°€ ìì²´ ê°œë°œí•œ ìƒì„±í˜• AIì˜ ì´ë¦„ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'`*\n",
    "\n",
    "    * **`chunk_size` = `7` â†’ ê²€ìƒ‰ ê²°ê³¼ â­•ï¸**\n",
    "      * *`\"ì‚¼ì„±ì „ìê°€ ìì²´ ê°œë°œí•œ ìƒì„±í˜• AIì˜ ì´ë¦„ì€ 'ì‚¼ì„± ê°€ìš°ìŠ¤'ì…ë‹ˆë‹¤.\"`*\n",
    "\n",
    "  * [**`myrag4.py`**](../15_Evaluations/myrag4.py)ë¡œ ì •ë¦¬"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8482815d",
   "metadata": {},
   "source": [
    "* d.\n",
    "  * **`ì¦ì€ ì»¤ë„ ì¶©ëŒë¡œ ì¸í•œ ë¬¸ì œ` â†’ `python` íŒŒì¼ë¡œ ì‹¤í–‰ â†’ í„°ë¯¸ë„ì—ì„œ ì§ì ‘ í™•ì¸í•˜ê¸°**\n",
    "\n",
    "  * [**`eval_script.py`**](../15_Evaluations/eval_script.py) ì‹¤í–‰ â†’ í„°ë¯¸ë„ì—ì„œ ê²°ê³¼ ë°”ë¡œ í™•ì¸\n",
    "\n",
    "  * ![`RAG_EVAL_4`](../15_Evaluations/assets/RAG_EVAL_4.png)\n",
    "\n",
    "<small>\n",
    "\n",
    "```bash\n",
    "\n",
    "      (ê°€ìƒí™˜ê²½)/15_Evaluations/eval_script.py\n",
    "\n",
    "      ğŸš€ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹œì‘...\n",
    "\n",
    "      ğŸ“‚ ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜: ... /15_Evaluations\n",
    "\n",
    "      ğŸ“„ PDF ê²½ë¡œ: ... /data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf\n",
    "\n",
    "      âœ… íŒŒì¼ í™•ì¸ ì™„ë£Œ!\n",
    "\n",
    "      âœ… LLM ìƒì„± ì™„ë£Œ: gemini-2.5-flash-lite\n",
    "\n",
    "      âœ… PDF ë¡œë“œ ì™„ë£Œ: 23 í˜ì´ì§€\n",
    "\n",
    "      âœ… ì²­í¬ ë¶„í•  ì™„ë£Œ: 119 ì²­í¬ (í¬ê¸°=300, ì˜¤ë²„ë©=50)\n",
    "\n",
    "\n",
    "      âœ… LLM ìƒì„± ì™„ë£Œ: gemini-2.5-flash-lite\n",
    "\n",
    "      âœ… ì²­í¬ ë¶„í•  ì™„ë£Œ: 119 ì²­í¬ (í¬ê¸°=300, ì˜¤ë²„ë©=50)\n",
    "      pytorch_model.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.27G/2.27G [00:03<00:00, 695MB/s]\n",
    "      tokenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 444/444 [00:00<00:00, 1.92MB/s]\n",
    "      sentencepiece.bpe.model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.07M/5.07M [00:06<00:00, 781kB/s]\n",
    "      tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17.1M/17.1M [00:06<00:00, 2.70MB/s]\n",
    "      special_tokens_map.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 964/964 [00:00<00:00, 3.61MB/s]\n",
    "      config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 191/191 [00:00<00:00, 628kB/s]\n",
    "\n",
    "      âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: BAAI/bge-m3                                                                 | 0.00/191 [00:00<?, ?B/s]\n",
    "      model.safetensors:   0%|                                                                  | 2.19M/2.27G [00:24<1:26:03, 439kB/s]âœ… ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ: FAISS\n",
    "\n",
    "      âœ… ê²€ìƒ‰ê¸° ìƒì„± ì™„ë£Œ (k=7, search_type=similarity)\n",
    "\n",
    "      âœ… RAG ì²´ì¸ ìƒì„± ì™„ë£Œ\n",
    "\n",
    "      ==================================================\n",
    "      ğŸ§ª í…ŒìŠ¤íŠ¸ ì‹¤í–‰...\n",
    "      ==================================================\n",
    "\n",
    "      View the evaluation results for experiment: 'RAG_EVAL_K7-3a35bd45' at:\n",
    "      https://smith.langchain.com/o/2c3342d3-1170-4ffa-86fd-f621199e0b9c/datasets/420dd308-2ebd-44c9-8ce8-9aff3886dc8e/compare?selectedSessions=cdb9ea10-e3c1-47bc-bfe1-48f651364c28\n",
    "\n",
    "\n",
    "      5it [00:14,  2.90s/it]\n",
    "      5it [00:14,  2.18s/it]\n",
    "\n",
    "      ==================================================\n",
    "      âœ… í‰ê°€ ì™„ë£Œ!\n",
    "      ==================================================\n",
    "\n",
    "      ê²°ê³¼: <ExperimentResults RAG_EVAL_K7-3a35bd45>\n",
    "\n",
    "      model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.27G/2.27G [02:14<00:00, 16.9MB/s]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f5e95d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06ace2f",
   "metadata": {},
   "source": [
    "* **`í˜„ì¬ ìƒí™© ë¶„ì„`**\n",
    "\n",
    "  | í•­ëª© | ê°œìˆ˜ | ì •í™•ë„ |\n",
    "  |------|------|--------|\n",
    "  | **CORRECT** | 3/5 | 60% |\n",
    "  | **INCORRECT** | 2/5 | 40% |\n",
    "\n",
    "* ì§„ì „\n",
    "  * ì´ì „: 1/5 (20%)\n",
    "  * í˜„ì¬: 3/5 (60%)\n",
    "  * **í–¥ìƒ: +200%!**\n",
    "\n",
    "* **`RAG` í‰ê°€**\n",
    "\n",
    "  * ì–´ë ¤ì›€\n",
    "\n",
    "    | ì´ìœ  | ì„¤ëª… |\n",
    "    |------|------|\n",
    "    | **ì´ˆê¸° ì •í™•ë„** | 20-40%ëŠ” ì •ìƒ |\n",
    "    | **ë°˜ë³µ ê°œì„ ** | 60% â†’ 80% â†’ 90%ë¡œ ì ì§„ì  í–¥ìƒ |\n",
    "    | **ì™„ë²½ì€ ì—†ìŒ** | 100%ëŠ” ê±°ì˜ ë¶ˆê°€ëŠ¥ |\n",
    "\n",
    "<br>\n",
    "\n",
    "* \n",
    "\n",
    "  * í˜„ì¬ ë‹¨ê³„\n",
    "\n",
    "    ```markdown\n",
    "        ì¼ë°˜ì ì¸ RAG í‰ê°€ ë°œì „:\n",
    "\n",
    "        1ë‹¨ê³„: 20-40% (ì´ˆê¸°)\n",
    "        2ë‹¨ê³„: 50-60% (ê°œì„  í›„)  â†  ì—¬ê¸°!\n",
    "        3ë‹¨ê³„: 70-80% (ìµœì í™”)\n",
    "        4ë‹¨ê³„: 80-90% (ì™„ì„±)\n",
    "    ```\n",
    "\n",
    "<br>\n",
    "\n",
    "* **`í–¥ìƒ ë°©ë²•`**\n",
    "\n",
    "  * **a. `kê°’ ì¦ê°€`**\n",
    "\n",
    "  * **b. `chunk_size` ì¡°ì •í•˜ê¸°**\n",
    "\n",
    "  * **c. `í”„ë¡¬í”„íŠ¸ ê°œì„ í•˜ê¸°`**\n",
    "\n",
    "  * **d. `ë‹¤ë¥¸ ì„ë² ë”© ëª¨ë¸ ì‹œë„í•˜ê¸°`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77e6d4d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270678d7",
   "metadata": {},
   "source": [
    "#### **3) `Context ì— ê¸°ë°˜í•œ ë‹µë³€ Evaluator`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7afa6d",
   "metadata": {},
   "source": [
    "* **`LangChainStringEvaluator(\"context_qa\")`**: `LLM` ì²´ì¸ì— ì •í™•ì„±ì„ íŒë‹¨í•˜ëŠ” ë° ì°¸ì¡° **`\"context\"`** ë¥¼ ì‚¬ìš© â†’ ì§€ì‹œ\n",
    "\n",
    "* **`LangChainStringEvaluator(\"cot_qa\")`**: \n",
    "  * `\"cot_qa\"` = `\"context_qa\"` í‰ê°€ìì™€ ìœ ì‚¬\n",
    "  * `but` ìµœì¢… íŒê²°ì„ ê²°ì •í•˜ê¸° ì „ì— `LLM` ì˜ `'ì¶”ë¡ '`ì„ ì‚¬ìš©í•˜ë„ë¡ ì§€ì‹œí•œë‹¤ëŠ” ì ì—ì„œ ì°¨ì´ ìˆìŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4d8e4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c9ae99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132b8f52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2756a0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f957c44e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b77f17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e2f3fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dc1748",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9fbfd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c62f0b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaa9472",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9447887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ì°¸ê³  ìë£Œì— í•´ë‹¹ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# query_1 \n",
    "# ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±\n",
    "chain.invoke(\"ì‚¼ì„±ì „ìê°€ ìì²´ ê°œë°œí•œ ìƒì„±í˜• AIì˜ ì´ë¦„ì€ ë¬´ì—‡ì¸ê°€ìš”?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec9e8e6",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* **`chain.invoke()`** - (`0.6s`)\n",
    "\n",
    "    ```markdown\n",
    "\n",
    "    'ì£¼ì–´ì§„ ë¬¸ì„œì—ì„œ ì‚¼ì„±ì „ìê°€ ìì²´ ê°œë°œí•œ ìƒì„±í˜• AIì˜ ì´ë¦„ì— ëŒ€í•œ ì •ë³´ëŠ” ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'\n",
    "\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c794d03",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d3fd2b",
   "metadata": {},
   "source": [
    "* **`ë””ë²„ê¹…í•˜ê¸°`**\n",
    "\n",
    "  * `RAG ì²´ì¸`ì˜ í•µì‹¬ êµ¬ì„± ìš”ì†Œ = **`ë¬¸ì„œ ë¡œë“œ`, `ë¶„í• `, `ì„ë² ë”©`, `ê²€ìƒ‰`(Retrieval) ë‹¨ê³„**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "201dbc16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1760439003.197106 7875803 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> BAAI/bge-small-en-v1.5 ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì„ë² ë”©ì„ ìƒì„± ì¤‘...\n",
      ">>> ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "ê²€ìƒ‰ëœ ë¬¸ì„œ ê°œìˆ˜: 3\n",
      "ì˜êµ­ì„ AI ì•ˆì „ ì—°êµ¬ì˜ ê¸€ë¡œë²Œ í—ˆë¸Œë¡œ í™•ë¦½í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•¨\n",
      "âˆ™ì˜êµ­ ì •ë¶€ëŠ” í–¥í›„ 10ë…„ê°„ ì—°êµ¬ì†Œì— ê³µê³µìê¸ˆì„ íˆ¬ìí•´ ì—°êµ¬ë¥¼ ì§€ì›í•  ê³„íšìœ¼ë¡œ, ì—°êµ¬ì†ŒëŠ” â–³ì²¨ë‹¨ AI ì‹œìŠ¤í…œ \n",
      "í‰ê°€ ê°œë°œê³¼ ì‹œí–‰ â–³AI ì•ˆì „ ì—°êµ¬ ì´‰ì§„ â–³ì •ë³´ êµë¥˜ í™œì„±í™”ë¥¼ í•µì‹¬ ê¸°ëŠ¥ìœ¼ë¡œ í•¨\n",
      "n (ì²¨ë‹¨ AI ì‹œìŠ¤í…œ í‰ê°€ ê°œë°œê³¼ ì‹œí–‰) ì‹œìŠ¤í…œì˜ ì•ˆì „ ê´€ë ¨ ì†ì„±ì„ ì¤‘ì‹¬ìœ¼ë¡œ ì•ˆì „ê³¼ ë³´ì•ˆ ê¸°ëŠ¥ì„ ì´í•´\n",
      "í•˜ê³  ì‚¬íšŒì  ì˜í–¥ì„ í‰ê°€\n",
      "âˆ™í‰ê°€ ìš°ì„ ìˆœìœ„ëŠ” â–³ì‚¬ì´ë²„ë²”ì£„ ì¡°ì¥, í—ˆìœ„ ì •ë³´ ìœ í¬ ë“± ì•…ì˜ì ìœ¼ë¡œ í™œìš©ë  ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ â–³ì‚¬íšŒì— ë¯¸ì¹˜ëŠ”\n"
     ]
    }
   ],
   "source": [
    "# ì„ë² ë”© ë° ë¦¬íŠ¸ë¦¬ë²„ í…ŒìŠ¤íŠ¸\n",
    "\n",
    "# 1. PDFRAG ê°ì²´ ìƒì„± ì‹œì ê¹Œì§€ ì‹¤í–‰\n",
    "# file_path = \"../15_Evaluations/data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf\"\n",
    "# llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\", temperature=0)\n",
    "\n",
    "rag = PDFRAG(\n",
    "    file_path=\"../15_Evaluations/data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf\",\n",
    "    llm=ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\", temperature=0))\n",
    "\n",
    "# 2. ë¦¬íŠ¸ë¦¬ë²„ ê°ì²´ ê°€ì ¸ì˜¤ê¸°\n",
    "retriever = rag.create_retriever()\n",
    "\n",
    "# 3. ê²€ìƒ‰ í…ŒìŠ¤íŠ¸: \n",
    "#   -> ì´ ë¶€ë¶„ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´ **ì„ë² ë”©/VectorStore** ë¬¸ì œ\n",
    "docs = retriever.invoke(\"ì‚¼ì„±ì „ìê°€ ìì²´ ê°œë°œí•œ ìƒì„±í˜• AIì˜ ì´ë¦„ì€ ë¬´ì—‡ì¸ê°€ìš”?\")\n",
    "\n",
    "# 4. ê²°ê³¼ ì¶œë ¥: Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ê°€ ì¶œë ¥ë˜ì–´ì•¼ í•¨\n",
    "print(f\"ê²€ìƒ‰ëœ ë¬¸ì„œ ê°œìˆ˜: {len(docs)}\")\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef92afaf",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* **`RAG ì´ˆê¸°í™” ì™„ë£Œ: ë¡œì»¬ ì„ë² ë”©ê³¼ LangChain LLM ì‚¬ìš©`** - (`11.3s`)\n",
    "\n",
    "    ```markdown\n",
    "\n",
    "    ê²€ìƒ‰ëœ ë¬¸ì„œ ê°œìˆ˜: 4\n",
    "    1. ì •ì±…/ë²•ì œ  \n",
    "    2. ê¸°ì—…/ì‚°ì—… \n",
    "    3. ê¸°ìˆ /ì—°êµ¬ \n",
    "    4. ì¸ë ¥/êµìœ¡\n",
    "    ì˜êµ­ ê³¼í•™í˜ì‹ ê¸°ìˆ ë¶€, AI ì•ˆì „ ì—°êµ¬ì†Œ ì„¤ë¦½ ë°œí‘œ\n",
    "    n ì˜êµ­ ê³¼í•™í˜ì‹ ê¸°ìˆ ë¶€ê°€ ì²¨ë‹¨ AI ì‹œìŠ¤í…œì— ëŒ€í•œ í‰ê°€ë¥¼ í†µí•´ ì•ˆì „ì„±ì„ ë³´ì¥í•˜ê¸° ìœ„í•œ AI \n",
    "    ì•ˆì „ ì—°êµ¬ì†Œë¥¼ ì„¤ë¦½í•œë‹¤ê³  ë°œí‘œ\n",
    "    n AI ì•ˆì „ ì—°êµ¬ì†ŒëŠ” í•µì‹¬ ê¸°ëŠ¥ìœ¼ë¡œ ì²¨ë‹¨ AI ì‹œìŠ¤í…œ í‰ê°€ ê°œë°œê³¼ ì‹œí–‰, AI ì•ˆì „ ì—°êµ¬ ì´‰ì§„, \n",
    "    ì •ë³´êµë¥˜ í™œì„±í™”ë¥¼ ì¶”ì§„í•  ê³„íš\n",
    "    KEY Contents\n",
    "\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177163a6",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* ì˜¤ë¥˜ ë°œìƒ ì›ì¸ ì¶”ë¡  - *(ì£¼ë¡œ `LLM` í˜¸ì¶œ ë‹¨ê³„ì— ë§ìŒ)*\n",
    "\n",
    "  * `LLM API Key` or í™˜ê²½ ë³€ìˆ˜ ë¬¸ì œ\n",
    "    * `llm ê°ì²´ ì´ˆê¸°í™”` ë‹¨ê³„ì—ì„œ `API key`ê°€ ì œëŒ€ë¡œ ì„¤ì •ë˜ì§€ ì•Šì•˜ì„ ê²½ìš°\n",
    "    * ë””ë²„ê¹…ì—ì„œ `llm ê°ì²´` ìƒì„± ê³¼ì •ì—ì„œ í•´ê²°ë˜ì—ˆì„ ê°€ëŠ¥ì„±\n",
    "    \n",
    "    ```python\n",
    "\n",
    "    # í™˜ê²½ë³€ìˆ˜ ì„¤ì • í™•ì¸\n",
    "    # ChatGoogleGenerativeAI()ì˜ ê²½ìš°\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = \"YOUR_API_KEY\"\n",
    "\n",
    "    # ChatOpenAI()ì˜ ê²½ìš°\n",
    "    os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n",
    "\n",
    "    # llm ê°ì²´ ì´ˆê¸°í™”\n",
    "    llm = ChatGoogleGenerativeAI(...)\n",
    "    # í˜¹ì€\n",
    "    llm = ChatOpenAI(...)\n",
    "\n",
    "    ```\n",
    "\n",
    "<br>\n",
    "\n",
    "* \n",
    "  * `Pydantic` ë²„ì „ ì¶©ëŒ ë¬¸ì œ\n",
    "    * `myrag.py` íŒŒì¼ = `Pydantic v1` í™˜ê²½ ì‘ì„±ëœ ì½”ë“œ â†’ í˜„ì¬ ì‹¤í–‰ í™˜ê²½ì´ `Pydantic v2` ìš”êµ¬í•œë‹¤ë©´ ì‹¤íŒ¨\n",
    "    * ë¡œì»¬ ì„ë² ë”© ëª¨ë¸ì¸ `HuggingFaceEmbeddings`ë¡œ êµì²´ â†’ í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜ë˜ì—ˆëŠ”ì§€ í™•ì¸ \n",
    "\n",
    "    ```bash\n",
    "\n",
    "    # íŒ¨í‚¤ì§€ ì„¤ì¹˜ í•„ìš”í•œ ê²½ìš°\n",
    "    pip install sentence-transformers   \n",
    "\n",
    "    # íŒ¨í‚¤ì§€ í™•ì¸ í•„ìš”í•œ ê²½ìš°\n",
    "    pip show sentence-transformers\n",
    "\n",
    "    ```\n",
    "\n",
    "  * * ì„¤ì¹˜ í™•ì¸ ê³¼ì •ì—ì„œ í•´ê²°ë˜ì—ˆì„ ê°€ëŠ¥ì„±\n",
    "\n",
    "<br>\n",
    "\n",
    "* \n",
    "  * `í…œí”Œë¦¿ ë³€ìˆ˜ ëˆ„ë½` or `í¬ë§· ì˜¤ë¥˜`\n",
    "    * `retriever`ê°€ ê²€ìƒ‰í•œ `context`ëŠ” ì˜ ë‚˜ì™”ì§€ë§Œ, **`question` í‚¤ì— í•´ë‹¹í•˜ëŠ” ì§ˆë¬¸ì´ `chain`ì— ì „ë‹¬ë˜ì§€ ì•Šì•„** í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ í¬ë§·íŒ… ì‹¤íŒ¨ ê°€ëŠ¥ì„±\n",
    "\n",
    "    ```python\n",
    "    # ì •ìƒ\n",
    "    rag_chain.invoke(question_string)\n",
    "\n",
    "    # ì‹¤íŒ¨í•œ í¬ë§·\n",
    "    rag_chain.invoke()\n",
    "\n",
    "    # ë””ë²„ê¹…\n",
    "    # ì •ìƒì ì¸ ë¬¸ìì—´ë¡œ í¬ë§·íŒ…\n",
    "    docs = retriever.invoke(\"ì‚¼ì„±ì „ìê°€ ìì²´ ê°œë°œí•œ ìƒì„±í˜• AIì˜ ì´ë¦„ì€ ë¬´ì—‡ì¸ê°€ìš”?\")\n",
    "    \n",
    "    ```\n",
    "\n",
    "    * ë””ë²„ê¹… ì½”ë“œ ì‹¤í–‰ **â†’ `ëª…í™•í•˜ê²Œ ì…ë ¥ ë¬¸ìì—´ ì „ë‹¬`** â†’ ì…ë ¥ ëˆ„ë½ í•´ê²°ë˜ì—ˆì„ ê°€ëŠ¥ì„±"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6aa196f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97bcefb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jay/.pyenv/versions/lc_env/lib/python3.13/site-packages/pydantic/_internal/_config.py:373: UserWarning: Valid config keys have changed in V2:\n",
      "* 'allow_population_by_field_name' has been renamed to 'validate_by_name'\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    },
    {
     "ename": "PydanticUserError",
     "evalue": "The `__modify_schema__` method is not supported in Pydantic v2. Use `__get_pydantic_json_schema__` instead in class `SecretStr`.\n\nFor further information visit https://errors.pydantic.dev/2.11/u/custom-json-schema",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPydanticUserError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmyrag\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PDFRAG\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_openai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdotenv\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[32m      5\u001b[39m load_dotenv()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/lc_env/lib/python3.13/site-packages/langchain_openai/__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_openai\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchat_models\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AzureChatOpenAI, ChatOpenAI\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_openai\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01membeddings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AzureOpenAIEmbeddings, OpenAIEmbeddings\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_openai\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AzureOpenAI, OpenAI\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/lc_env/lib/python3.13/site-packages/langchain_openai/chat_models/__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_openai\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchat_models\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mazure\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AzureChatOpenAI\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_openai\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchat_models\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[32m      4\u001b[39m __all__ = [\u001b[33m\"\u001b[39m\u001b[33mChatOpenAI\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mAzureChatOpenAI\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/lc_env/lib/python3.13/site-packages/langchain_openai/chat_models/azure.py:41\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunction_calling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m convert_to_openai_tool\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpydantic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_basemodel_subclass\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_openai\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchat_models\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseChatOpenAI\n\u001b[32m     43\u001b[39m logger = logging.getLogger(\u001b[34m__name__\u001b[39m)\n\u001b[32m     46\u001b[39m _BM = TypeVar(\u001b[33m\"\u001b[39m\u001b[33m_BM\u001b[39m\u001b[33m\"\u001b[39m, bound=BaseModel)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/lc_env/lib/python3.13/site-packages/langchain_openai/chat_models/base.py:379\u001b[39m\n\u001b[32m    375\u001b[39m     parsed: Optional[_DictOrPydantic]\n\u001b[32m    376\u001b[39m     parsing_error: Optional[\u001b[38;5;167;01mBaseException\u001b[39;00m]\n\u001b[32m--> \u001b[39m\u001b[32m379\u001b[39m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43;01mBaseChatOpenAI\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mBaseChatModel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mAny\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mField\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdefault\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m#: :meta private:\u001b[39;49;00m\n\u001b[32m    381\u001b[39m \u001b[43m    \u001b[49m\u001b[43masync_client\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mAny\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mField\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdefault\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m#: :meta private:\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/lc_env/lib/python3.13/site-packages/pydantic/_internal/_model_construction.py:237\u001b[39m, in \u001b[36mModelMetaclass.__new__\u001b[39m\u001b[34m(mcs, cls_name, bases, namespace, __pydantic_generic_metadata__, __pydantic_reset_parent_namespace__, _create_model_module, **kwargs)\u001b[39m\n\u001b[32m    233\u001b[39m     set_model_mocks(\u001b[38;5;28mcls\u001b[39m)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Any operation that requires accessing the field infos instances should be put inside\u001b[39;00m\n\u001b[32m    236\u001b[39m     \u001b[38;5;66;03m# `complete_model_class()`:\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m     \u001b[43mcomplete_model_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig_wrapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mraise_errors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mns_resolver\u001b[49m\u001b[43m=\u001b[49m\u001b[43mns_resolver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcreate_model_module\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_create_model_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config_wrapper.frozen \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m__hash__\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m namespace:\n\u001b[32m    246\u001b[39m     set_default_hash_func(\u001b[38;5;28mcls\u001b[39m, bases)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/lc_env/lib/python3.13/site-packages/pydantic/_internal/_model_construction.py:597\u001b[39m, in \u001b[36mcomplete_model_class\u001b[39m\u001b[34m(cls, config_wrapper, raise_errors, ns_resolver, create_model_module)\u001b[39m\n\u001b[32m    590\u001b[39m gen_schema = GenerateSchema(\n\u001b[32m    591\u001b[39m     config_wrapper,\n\u001b[32m    592\u001b[39m     ns_resolver,\n\u001b[32m    593\u001b[39m     typevars_map,\n\u001b[32m    594\u001b[39m )\n\u001b[32m    596\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m597\u001b[39m     schema = \u001b[43mgen_schema\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    598\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m PydanticUndefinedAnnotation \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    599\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m raise_errors:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/lc_env/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:711\u001b[39m, in \u001b[36mGenerateSchema.generate_schema\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    708\u001b[39m schema = \u001b[38;5;28mself\u001b[39m._generate_schema_from_get_schema_method(obj, obj)\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m711\u001b[39m     schema = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_schema_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    713\u001b[39m metadata_js_function = _extract_get_pydantic_json_schema(obj)\n\u001b[32m    714\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m metadata_js_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/lc_env/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:1004\u001b[39m, in \u001b[36mGenerateSchema._generate_schema_inner\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m   1002\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m lenient_issubclass(obj, BaseModel):\n\u001b[32m   1003\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model_type_stack.push(obj):\n\u001b[32m-> \u001b[39m\u001b[32m1004\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_model_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, PydanticRecursiveRef):\n\u001b[32m   1007\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m core_schema.definition_reference_schema(schema_ref=obj.type_ref)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/lc_env/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:837\u001b[39m, in \u001b[36mGenerateSchema._model_schema\u001b[39m\u001b[34m(self, cls)\u001b[39m\n\u001b[32m    825\u001b[39m     model_schema = core_schema.model_schema(\n\u001b[32m    826\u001b[39m         \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m    827\u001b[39m         inner_schema,\n\u001b[32m   (...)\u001b[39m\u001b[32m    833\u001b[39m         ref=model_ref,\n\u001b[32m    834\u001b[39m     )\n\u001b[32m    835\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    836\u001b[39m     fields_schema: core_schema.CoreSchema = core_schema.model_fields_schema(\n\u001b[32m--> \u001b[39m\u001b[32m837\u001b[39m         {k: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_md_field_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecorators\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m fields.items()},\n\u001b[32m    838\u001b[39m         computed_fields=[\n\u001b[32m    839\u001b[39m             \u001b[38;5;28mself\u001b[39m._computed_field_schema(d, decorators.field_serializers)\n\u001b[32m    840\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m computed_fields.values()\n\u001b[32m    841\u001b[39m         ],\n\u001b[32m    842\u001b[39m         extras_schema=extras_schema,\n\u001b[32m    843\u001b[39m         extras_keys_schema=extras_keys_schema,\n\u001b[32m    844\u001b[39m         model_name=\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m,\n\u001b[32m    845\u001b[39m     )\n\u001b[32m    846\u001b[39m     inner_schema = apply_validators(fields_schema, decorators.root_validators.values(), \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    847\u001b[39m     inner_schema = apply_model_validators(inner_schema, model_validators, \u001b[33m'\u001b[39m\u001b[33minner\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/lc_env/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:1206\u001b[39m, in \u001b[36mGenerateSchema._generate_md_field_schema\u001b[39m\u001b[34m(self, name, field_info, decorators)\u001b[39m\n\u001b[32m   1199\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate_md_field_schema\u001b[39m(\n\u001b[32m   1200\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1201\u001b[39m     name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   1202\u001b[39m     field_info: FieldInfo,\n\u001b[32m   1203\u001b[39m     decorators: DecoratorInfos,\n\u001b[32m   1204\u001b[39m ) -> core_schema.ModelField:\n\u001b[32m   1205\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Prepare a ModelField to represent a model field.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1206\u001b[39m     common_field = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_common_field_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfield_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecorators\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1207\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m core_schema.model_field(\n\u001b[32m   1208\u001b[39m         common_field[\u001b[33m'\u001b[39m\u001b[33mschema\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m   1209\u001b[39m         serialization_exclude=common_field[\u001b[33m'\u001b[39m\u001b[33mserialization_exclude\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m   1213\u001b[39m         metadata=common_field[\u001b[33m'\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m   1214\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/lc_env/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:1372\u001b[39m, in \u001b[36mGenerateSchema._common_field_schema\u001b[39m\u001b[34m(self, name, field_info, decorators)\u001b[39m\n\u001b[32m   1368\u001b[39m         schema = \u001b[38;5;28mself\u001b[39m._apply_annotations(\n\u001b[32m   1369\u001b[39m             source_type, annotations + validators_from_decorators, transform_inner_schema=set_discriminator\n\u001b[32m   1370\u001b[39m         )\n\u001b[32m   1371\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1372\u001b[39m         schema = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply_annotations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1373\u001b[39m \u001b[43m            \u001b[49m\u001b[43msource_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1374\u001b[39m \u001b[43m            \u001b[49m\u001b[43mannotations\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidators_from_decorators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1375\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1377\u001b[39m \u001b[38;5;66;03m# This V1 compatibility shim should eventually be removed\u001b[39;00m\n\u001b[32m   1378\u001b[39m \u001b[38;5;66;03m# push down any `each_item=True` validators\u001b[39;00m\n\u001b[32m   1379\u001b[39m \u001b[38;5;66;03m# note that this won't work for any Annotated types that get wrapped by a function validator\u001b[39;00m\n\u001b[32m   1380\u001b[39m \u001b[38;5;66;03m# but that's okay because that didn't exist in V1\u001b[39;00m\n\u001b[32m   1381\u001b[39m this_field_validators = filter_field_decorator_info_by_field(decorators.validators.values(), name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/lc_env/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:2297\u001b[39m, in \u001b[36mGenerateSchema._apply_annotations\u001b[39m\u001b[34m(self, source_type, annotations, transform_inner_schema)\u001b[39m\n\u001b[32m   2292\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   2293\u001b[39m     get_inner_schema = \u001b[38;5;28mself\u001b[39m._get_wrapped_inner_schema(\n\u001b[32m   2294\u001b[39m         get_inner_schema, annotation, pydantic_js_annotation_functions\n\u001b[32m   2295\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2297\u001b[39m schema = \u001b[43mget_inner_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2298\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pydantic_js_annotation_functions:\n\u001b[32m   2299\u001b[39m     core_metadata = schema.setdefault(\u001b[33m'\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m'\u001b[39m, {})\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/lc_env/lib/python3.13/site-packages/pydantic/_internal/_schema_generation_shared.py:83\u001b[39m, in \u001b[36mCallbackGetCoreSchemaHandler.__call__\u001b[39m\u001b[34m(self, source_type)\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, source_type: Any, /) -> core_schema.CoreSchema:\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m     schema = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._ref_mode == \u001b[33m'\u001b[39m\u001b[33mto-def\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m     85\u001b[39m         ref = schema.get(\u001b[33m'\u001b[39m\u001b[33mref\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/lc_env/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:2279\u001b[39m, in \u001b[36mGenerateSchema._apply_annotations.<locals>.inner_handler\u001b[39m\u001b[34m(obj)\u001b[39m\n\u001b[32m   2276\u001b[39m schema = \u001b[38;5;28mself\u001b[39m._generate_schema_from_get_schema_method(obj, source_type)\n\u001b[32m   2278\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2279\u001b[39m     schema = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_schema_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2281\u001b[39m metadata_js_function = _extract_get_pydantic_json_schema(obj)\n\u001b[32m   2282\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m metadata_js_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/lc_env/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:1009\u001b[39m, in \u001b[36mGenerateSchema._generate_schema_inner\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m   1006\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, PydanticRecursiveRef):\n\u001b[32m   1007\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m core_schema.definition_reference_schema(schema_ref=obj.type_ref)\n\u001b[32m-> \u001b[39m\u001b[32m1009\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmatch_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/lc_env/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:1123\u001b[39m, in \u001b[36mGenerateSchema.match_type\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m   1121\u001b[39m origin = get_origin(obj)\n\u001b[32m   1122\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m origin \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1123\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_match_generic_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morigin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1125\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._arbitrary_types:\n\u001b[32m   1126\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._arbitrary_type_schema(obj)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/lc_env/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:1146\u001b[39m, in \u001b[36mGenerateSchema._match_generic_type\u001b[39m\u001b[34m(self, obj, origin)\u001b[39m\n\u001b[32m   1144\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._type_alias_type_schema(obj)\n\u001b[32m   1145\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m is_union_origin(origin):\n\u001b[32m-> \u001b[39m\u001b[32m1146\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_union_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1147\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m origin \u001b[38;5;129;01min\u001b[39;00m TUPLE_TYPES:\n\u001b[32m   1148\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tuple_schema(obj)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/lc_env/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:1434\u001b[39m, in \u001b[36mGenerateSchema._union_schema\u001b[39m\u001b[34m(self, union_type)\u001b[39m\n\u001b[32m   1432\u001b[39m         nullable = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1433\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1434\u001b[39m         choices.append(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   1436\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(choices) == \u001b[32m1\u001b[39m:\n\u001b[32m   1437\u001b[39m     s = choices[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/lc_env/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:713\u001b[39m, in \u001b[36mGenerateSchema.generate_schema\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    711\u001b[39m     schema = \u001b[38;5;28mself\u001b[39m._generate_schema_inner(obj)\n\u001b[32m--> \u001b[39m\u001b[32m713\u001b[39m metadata_js_function = \u001b[43m_extract_get_pydantic_json_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    714\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m metadata_js_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    715\u001b[39m     metadata_schema = resolve_original_schema(schema, \u001b[38;5;28mself\u001b[39m.defs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/lc_env/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:2641\u001b[39m, in \u001b[36m_extract_get_pydantic_json_schema\u001b[39m\u001b[34m(tp)\u001b[39m\n\u001b[32m   2639\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_custom_v2_modify_js_func:\n\u001b[32m   2640\u001b[39m         cls_name = \u001b[38;5;28mgetattr\u001b[39m(tp, \u001b[33m'\u001b[39m\u001b[33m__name__\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2641\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m PydanticUserError(\n\u001b[32m   2642\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mThe `__modify_schema__` method is not supported in Pydantic v2. \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   2643\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mUse `__get_pydantic_json_schema__` instead\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m in class `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcls_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mcls_name\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   2644\u001b[39m             code=\u001b[33m'\u001b[39m\u001b[33mcustom-json-schema\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   2645\u001b[39m         )\n\u001b[32m   2647\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (origin := get_origin(tp)) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2648\u001b[39m     \u001b[38;5;66;03m# Generic aliases proxy attribute access to the origin, *except* dunder attributes,\u001b[39;00m\n\u001b[32m   2649\u001b[39m     \u001b[38;5;66;03m# such as `__get_pydantic_json_schema__`, hence the explicit check.\u001b[39;00m\n\u001b[32m   2650\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _extract_get_pydantic_json_schema(origin)\n",
      "\u001b[31mPydanticUserError\u001b[39m: The `__modify_schema__` method is not supported in Pydantic v2. Use `__get_pydantic_json_schema__` instead in class `SecretStr`.\n\nFor further information visit https://errors.pydantic.dev/2.11/u/custom-json-schema"
     ]
    }
   ],
   "source": [
    "from myrag import PDFRAG\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# OpenAI LLM\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8569077f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c494dc8d",
   "metadata": {},
   "source": [
    "* **`ask_question()` í•¨ìˆ˜ ìƒì„±í•˜ê¸°** \n",
    "\n",
    "  * ì…ë ¥ = `inputs` = `ë”•ì…”ë„ˆë¦¬`\n",
    "  * ì¶œë ¥ = `answer` = `ë”•ì…”ë„ˆë¦¬`ë¥¼ ë°˜í™˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f5fa9960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€í•˜ëŠ” í•¨ìˆ˜ë¥¼ ìƒì„±\n",
    "def ask_question(inputs: dict):\n",
    "    return {\"answer\": chain.invoke(inputs[\"question\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ac37d9f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'ì£¼ì–´ì§„ ë¬¸ì„œì—ì„œ ì‚¼ì„±ì „ìê°€ ìì²´ ê°œë°œí•œ ìƒì„±í˜• AIì˜ ì´ë¦„ì— ëŒ€í•œ ì •ë³´ëŠ” ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ì‚¬ìš©ì ì§ˆë¬¸ ì˜ˆì‹œ\n",
    "llm_answer = ask_question(\n",
    "    {\"question\": \"ì‚¼ì„±ì „ìê°€ ìì²´ ê°œë°œí•œ ìƒì„±í˜• AIì˜ ì´ë¦„ì€ ë¬´ì—‡ì¸ê°€ìš”?\"}\n",
    ")\n",
    "llm_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19fccce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b92354e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b125ab30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57fd990",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b0d2cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a911d77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2d5f2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9527f3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa373788",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc26f35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afff134",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8167ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb8ba2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb74f9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0294ba9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45cfc33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8d3f45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c9a969",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc2a9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ì§ˆë¬¸ê³¼ ë‹µë³€ ëª©ë¡\n",
    "inputs = [\n",
    "    \"ì‚¼ì„±ì „ìê°€ ë§Œë“  ìƒì„±í˜• AIì˜ ì´ë¦„ì€ ë¬´ì—‡ì¸ê°€ìš”?\",\n",
    "    \"ë¯¸êµ­ ë°”ì´ë“  ëŒ€í†µë ¹ì´ ì•ˆì „í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” AI ê°œë°œê³¼ ì‚¬ìš©ì„ ë³´ì¥í•˜ê¸° ìœ„í•œ í–‰ì •ëª…ë ¹ì„ ë°œí‘œí•œ ë‚ ì€ ì–¸ì œì¸ê°€ìš”?\",\n",
    "    \"ì½”íˆì–´ì˜ ë°ì´í„° ì¶œì²˜ íƒìƒ‰ê¸°ì— ëŒ€í•´ì„œ ê°„ëµíˆ ë§í•´ì£¼ì„¸ìš”.\",\n",
    "]\n",
    "\n",
    "# ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ëª©ë¡\n",
    "outputs = [\n",
    "    \"ì‚¼ì„±ì „ìê°€ ë§Œë“  ìƒì„±í˜• AIì˜ ì´ë¦„ì€ ì‚¼ì„± ê°€ìš°ìŠ¤ ì…ë‹ˆë‹¤.\",\n",
    "    \"2023ë…„ 10ì›” 30ì¼ ë¯¸êµ­ ë°”ì´ë“  ëŒ€í†µë ¹ì´ í–‰ì •ëª…ë ¹ì„ ë°œí‘œí–ˆìŠµë‹ˆë‹¤.\",\n",
    "    \"ì½”íˆì–´ì˜ ë°ì´í„° ì¶œì²˜ íƒìƒ‰ê¸°ëŠ” AI ëª¨ë¸ í›ˆë ¨ì— ì‚¬ìš©ë˜ëŠ” ë°ì´í„°ì…‹ì˜ ì¶œì²˜ì™€ ë¼ì´ì„ ìŠ¤ ìƒíƒœë¥¼ ì¶”ì í•˜ê³  íˆ¬ëª…ì„±ì„ í™•ë³´í•˜ê¸° ìœ„í•œ í”Œë«í¼ì…ë‹ˆë‹¤. 12ê°œ ê¸°ê´€ê³¼ í˜‘ë ¥í•˜ì—¬ 2,000ì—¬ ê°œ ë°ì´í„°ì…‹ì˜ ì¶œì²˜ ì •ë³´ë¥¼ ì œê³µí•˜ë©°, ê°œë°œìë“¤ì´ ë°ì´í„°ì˜ êµ¬ì„±ê³¼ ê³„ë³´ë¥¼ ì‰½ê²Œ íŒŒì•…í•  ìˆ˜ ìˆê²Œ ë•ìŠµë‹ˆë‹¤.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b4c167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì§ˆë¬¸ê³¼ ë‹µë³€ ìŒ ìƒì„±\n",
    "qa_pairs = [{\"question\": q, \"answer\": a} for q, a in zip(inputs, outputs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296e9426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜\n",
    "df = pd.DataFrame(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf1d2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„°í”„ë ˆì„ ì¶œë ¥\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe08182",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* `df.head()`\n",
    "\n",
    "  * ![df.head()](../15_Evaluations/assets/df_head_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e206505",
   "metadata": {},
   "source": [
    "##### **`â ì´ì „ì—ì„œ ìƒì„±í•œ`** ***`Synthetic Dataset`*** **`í™œìš©í•˜ê¸°`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365cc82b",
   "metadata": {},
   "source": [
    "* ì—…ë¡œë“œí–ˆë˜ `HuggingFace Dataset` í™œìš©í•˜ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d54310",
   "metadata": {},
   "source": [
    "* ì‚¬ì „ì— `VS Code` í„°ë¯¸ë„ì— ì„¤ì¹˜í•  ê²ƒ\n",
    "\n",
    "```bash\n",
    "            pip install -qU datasets\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0392c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "import os\n",
    "\n",
    "# huggingface Datasetì—ì„œ repo_idë¡œ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ\n",
    "dataset = load_dataset(\n",
    "    \"livemylife23/rag-synthetic-dataset-korean\",   # ë°ì´í„°ì…‹ ì´ë¦„\n",
    "    token=os.environ[\"HUGGINGFACEHUB_API_TOKEN\"],  # private ë°ì´í„°ì¸ ê²½ìš° í•„ìš”\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae21461",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* ë‹¤ìš´ë¡œë“œ ì™„ë£Œ - (`7.1s`)\n",
    "\n",
    "  * ![hugginface_dataset_download](../15_Evaluations/assets/hugging_face_download.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14854914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„°ì…‹ì—ì„œ split ê¸°ì¤€ìœ¼ë¡œ ì¡°íšŒ\n",
    "huggingface_df = dataset[\"train\"].to_pandas()\n",
    "huggingface_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b1a435",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "  * `huggingface_df.head()`\n",
    "\n",
    "    * ![huggingface_df.head()](../15_Evaluations/assets/df_head_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701a711e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3db68a",
   "metadata": {},
   "source": [
    "#### **2) `LangSmith í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ Dataset ìƒì„±`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb905c0",
   "metadata": {},
   "source": [
    "* **`Dataset & Testing`ì— ìƒˆë¡œìš´ ë°ì´í„°ì…‹ ìƒì„±í•˜ê¸°**\n",
    "\n",
    "<small>\n",
    "\n",
    "  * ![langsmithì— ìƒˆë¡œìš´ ë°ì´í„°ì…‹ ìƒì„±í•˜ê¸°](../15_Evaluations/assets/eval-06.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a8bae4",
   "metadata": {},
   "source": [
    "* `csv` íŒŒì¼ì—ì„œ `LangSmith UI` ì‚¬ìš© â†’ ì§ì ‘ ë°ì´í„°ì…‹ ìƒì„± ê°€ëŠ¥\n",
    "\n",
    "* *ì°¸ê³ : [`LangSmith UI ë¬¸ì„œ`](https://docs.langchain.com/langsmith/manage-datasets)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615871d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "dataset_name = \"RAG_EVAL_DATASET\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509aaa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„°ì…‹ ìƒì„± í•¨ìˆ˜\n",
    "\n",
    "def create_dataset(client, dataset_name, description=None):\n",
    "    for dataset in client.list_datasets():\n",
    "        if dataset.name == dataset_name:\n",
    "            return dataset\n",
    "\n",
    "    dataset = client.create_dataset(\n",
    "        dataset_name=dataset_name,\n",
    "        description=description,\n",
    "    )\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea80981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„°ì…‹ ìƒì„±\n",
    "dataset = create_dataset(client, dataset_name)          # 2.9s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef9d52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìƒì„±ëœ ë°ì´í„°ì…‹ì— ì˜ˆì œ ì¶”ê°€\n",
    "\n",
    "client.create_examples(\n",
    "    inputs=[{\"question\": q} for q in df[\"question\"].tolist()],\n",
    "    outputs=[{\"answer\": a} for a in df[\"answer\"].tolist()],\n",
    "    dataset_id=dataset.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe3ee15",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* **`client.creat_examples() ì¶”ê°€í•˜ê¸°`** - (`0.1s`)\n",
    "\n",
    "    ```python\n",
    "    {'example_ids': ['8d34ad4e-e3be-47ca-9a6a-9929608b60ca',\n",
    "    '71841ffc-cfbe-4e05-b0b0-ef85e8a6ebd4',\n",
    "    'e0d645a9-6d97-41da-8d45-cc5e8d6f0a37'],\n",
    "    'count': 3}\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3df2b1a",
   "metadata": {},
   "source": [
    "* **`ë°ì´í„°ì…‹ì— ì˜ˆì œ ë‚˜ì¤‘ì— ì¶”ê°€ ê°€ëŠ¥`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89400605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìƒˆë¡œìš´ ì§ˆë¬¸ ëª©ë¡\n",
    "new_questions = [\n",
    "    \"ì‚¼ì„±ì „ìê°€ ë§Œë“  ìƒì„±í˜• AIì˜ ì´ë¦„ì€ ë¬´ì—‡ì¸ê°€ìš”?\",\n",
    "    \"êµ¬ê¸€ì´ í…Œë””ë…¸íŠ¸ì—ê²Œ 20ì–µë‹¬ëŸ¬ë¥¼ íˆ¬ìí•œ ê²ƒì´ ì‚¬ì‹¤ì…ë‹ˆê¹Œ?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1612eec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìƒˆë¡œìš´ ë‹µë³€ ëª©ë¡\n",
    "new_answers = [\n",
    "    \"ì‚¼ì„±ì „ìê°€ ë§Œë“  ìƒì„±í˜• AIì˜ ì´ë¦„ì€ í…Œë””ë…¸íŠ¸ ì…ë‹ˆë‹¤.\",\n",
    "    \"ì‚¬ì‹¤ì´ ì•„ë‹™ë‹ˆë‹¤. êµ¬ê¸€ì€ ì•¤ìŠ¤ë¡œí”½ì— ìµœëŒ€ 20ì–µ ë‹¬ëŸ¬ë¥¼ íˆ¬ìí•˜ê¸°ë¡œ í•©ì˜í–ˆìœ¼ë©°, ì´ ì¤‘ 5ì–µ ë‹¬ëŸ¬ë¥¼ ìš°ì„  íˆ¬ìí•˜ê³  í–¥í›„ 15ì–µ ë‹¬ëŸ¬ë¥¼ ì¶”ê°€ë¡œ íˆ¬ìí•˜ê¸°ë¡œ í–ˆìŠµë‹ˆë‹¤.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7cb209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UIì—ì„œ ì—…ë°ì´íŠ¸ëœ ë²„ì „ í™•ì¸\n",
    "\n",
    "client.create_examples(\n",
    "    inputs=[{\"question\": q} for q in new_questions],\n",
    "    outputs=[{\"answer\": a} for a in new_answers],\n",
    "    dataset_id=dataset.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e8351c",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* **`UIì—ì„œ ì—…ë°ì´íŠ¸ëœ ë²„ì „ í™•ì¸í•˜ê¸°`** - (`0.1s`)\n",
    "\n",
    "    ```python\n",
    "    {'example_ids': ['7080404f-878d-4e22-9256-70a5d1b86ab3',\n",
    "    'b8599d26-9990-4153-8a92-0dc779087f2c'],\n",
    "    'count': 2}\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9215acbf",
   "metadata": {},
   "source": [
    "* **âœ“ `ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œë¨`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a333ef40",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff26a34",
   "metadata": {},
   "source": [
    "* next: ***`05. LLM-as-Judge`***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719f0c8c",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc_eval_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
