# ========================================
# eval_labeled_criteria.py
# ì •ë‹µ ê¸°ë°˜ Criteria í‰ê°€
# ========================================

from dotenv import load_dotenv
load_dotenv()

import os
from myrag5 import PDFRAG
from langchain_ollama import ChatOllama             # Ollama ì‚¬ìš©
from langsmith.evaluation import evaluate, LangChainStringEvaluator
import warnings
warnings.filterwarnings("ignore")

print("ğŸš€ Labeled Criteria í‰ê°€ ì‹œì‘...\n")


# ========================================
# íŒŒì¼ ê²½ë¡œ & LLM ì„¤ì •
# ========================================

script_dir = os.path.dirname(os.path.abspath(__file__))
pdf_path = os.path.join(script_dir, "data", "SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf")

if not os.path.exists(pdf_path):
    print(f"âŒ íŒŒì¼ ì—†ìŒ: {pdf_path}")
    exit(1)

print(f"âœ… PDF í™•ì¸: {pdf_path}\n")

# LLM ìƒì„±

llm = ChatOllama(
    model="qwen2.5-coder:7b-instruct",
    temperature=0
)

print("âœ… ë¡œì»¬ LLM ìƒì„± ì™„ë£Œ: Qwen2.5-Coder-7B \n")

# ========================================
# RAG ì‹œìŠ¤í…œ ìƒì„±
# ========================================

rag = PDFRAG(pdf_path, llm, chunk_size=300, chunk_overlap=50)
retriever = rag.create_retriever(k=10, search_type="similarity")

# Context í¬í•¨ ì²´ì¸
chain_with_context = rag.create_chain_with_context(retriever)

print("\n" + "="*50)
print("ğŸ§ª í…ŒìŠ¤íŠ¸ ì‹¤í–‰...")
print("="*50 + "\n")

# í…ŒìŠ¤íŠ¸
test_question = "ì‚¼ì„±ì „ìê°€ ìì²´ ê°œë°œí•œ ìƒì„±í˜• AIì˜ ì´ë¦„ì€ ë¬´ì—‡ì¸ê°€ìš”?"
#test_result = chain_with_context.invoke(test_question)
#print(f"ì§ˆë¬¸: {test_question}")
#print(f"Context: {test_result['context'][:200]}...")
#print(f"ë‹µë³€: {test_result['answer']}\n")

try:
    test_result = chain_with_context.invoke(test_question)
    print(f"ì§ˆë¬¸: {test_question}")
    print(f"Context: {test_result['context'][:100]}...")
    print(f"ë‹µë³€: {test_result['answer']}\n")
except Exception as e:
    print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}\n")
    import traceback
    traceback.print_exc()
    exit(1)


# ========================================
# Context + Answer ë°˜í™˜ í•¨ìˆ˜
# ========================================

def context_answer_rag_answer(inputs: dict):
    """
    Contextì™€ Answerë¥¼ í•¨ê»˜ ë°˜í™˜
    """
    result = chain_with_context.invoke(inputs["question"])
    return {
        "context": result["context"],
        "answer": result["answer"],
        "query": inputs["question"],
    }

# ========================================
# Labeled Criteria í‰ê°€ì ìƒì„±
# ========================================

print("="*50)
print("ğŸ“Š Labeled Criteria í‰ê°€ì ìƒì„±...")
print("="*50 + "\n")

# í‰ê°€ìš© LLM 
eval_llm = ChatOllama(
    model="qwen2.5-coder:7b-instruct",
    temperature=0
)

# 1. Helpfulness Evaluator (Ground Truth ê¸°ë°˜)
helpfulness_evaluator = LangChainStringEvaluator(
    "labeled_criteria",
    config={
        "criteria": {
            "helpfulness": (
                "Is this submission helpful to the user, "
                "taking into account the correct reference answer?"
            )
        },
        "llm": llm,
    },
    prepare_data=lambda run, example: {
        "prediction": run.outputs["answer"],           # LLM ë‹µë³€
        "reference": example.outputs["answer"],        # Ground Truth
        "input": example.inputs["question"],           # ì§ˆë¬¸
    },
)

# 2. Relevance Evaluator (Context ê¸°ë°˜)
relevance_evaluator = LangChainStringEvaluator(
    "labeled_criteria",
    config={
        "criteria": "relevance",                # ë‹µë³€ì´ Contextë¥¼ ì°¸ì¡°í•˜ëŠ”ê°€?
        "llm": llm,
    },
    prepare_data=lambda run, example: {
        "prediction": run.outputs["answer"],           # LLM ë‹µë³€
        "reference": run.outputs["context"],           # Context
        "input": example.inputs["question"],           # ì§ˆë¬¸
    },
)

# 3. Accuracy Evaluator (Ground Truth ê¸°ë°˜)
accuracy_evaluator = LangChainStringEvaluator(
    "labeled_criteria",
    config={
        "criteria": {
            "accuracy": (
                "Is this submission factually accurate "
                "compared to the reference answer?"
            )
        },
        "llm": llm,
    },
    prepare_data=lambda run, example: {
        "prediction": run.outputs["answer"],           # LLM ë‹µë³€
        "reference": example.outputs["answer"],        # Ground Truth
        "input": example.inputs["question"],           # ì§ˆë¬¸
    },
)

print("âœ… í‰ê°€ì ìƒì„± ì™„ë£Œ\n")

# ========================================
# í‰ê°€ ì‹¤í–‰
# ========================================

print("="*50)
print("ğŸ“Š í‰ê°€ ì‹¤í–‰...")
print("="*50 + "\n")

try:
    experiment_results = evaluate(
        context_answer_rag_answer,
        data="RAG_EVAL_DATASET",
        evaluators=[
            helpfulness_evaluator,
            relevance_evaluator,
            accuracy_evaluator,
        ],
        experiment_prefix="RAG_EVAL_LABELED_CRITERIA_LOCAL",
        metadata={
            "variant": "Labeled Criteria (Helpfulness + Relevance + Accuracy) - Local",
            "k": 10,
            "chunk_size": 300,
            "llm": "qwen2.5-coder:7b-instruct",
        },
        max_concurrency=1,
    )
    
    print("\n" + "="*50)
    print("âœ… í‰ê°€ ì™„ë£Œ!")
    print("="*50)
    print(f"\nê²°ê³¼: {experiment_results}\n")
    
except Exception as e:
    print(f"\nâŒ ì—ëŸ¬ ë°œìƒ: {e}\n")
    import traceback
    traceback.print_exc()