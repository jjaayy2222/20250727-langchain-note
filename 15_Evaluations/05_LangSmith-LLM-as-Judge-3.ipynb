{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "574f6502",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf366bf9",
   "metadata": {},
   "source": [
    "* ì¶œì²˜: LangChain ê³µì‹ ë¬¸ì„œ ë˜ëŠ” í•´ë‹¹ êµì¬ëª…\n",
    "* ì›ë³¸ URL: https://smith.langchain.com/hub/teddynote/summary-stuff-documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31f7552",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af770941",
   "metadata": {},
   "source": [
    "### **5. `05. LLM-as-Judge`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4854f0d6",
   "metadata": {},
   "source": [
    "#### **4) `Context ì— ê¸°ë°˜í•œ ë‹µë³€ Evaluator`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276aeabe",
   "metadata": {},
   "source": [
    "* **`LangChainStringEvaluator(\"context_qa\")`**: `LLM` ì²´ì¸ì— ì •í™•ì„±ì„ íŒë‹¨í•˜ëŠ” ë° ì°¸ì¡° **`\"context\"`** ë¥¼ ì‚¬ìš© â†’ ì§€ì‹œ\n",
    "\n",
    "* **`LangChainStringEvaluator(\"cot_qa\")`**: \n",
    "  * `\"cot_qa\"` = `\"context_qa\"` í‰ê°€ìì™€ ìœ ì‚¬\n",
    "  * `but` ìµœì¢… íŒê²°ì„ ê²°ì •í•˜ê¸° ì „ì— `LLM` ì˜ `'ì¶”ë¡ '`ì„ ì‚¬ìš©í•˜ë„ë¡ ì§€ì‹œí•œë‹¤ëŠ” ì ì—ì„œ ì°¨ì´ ìˆìŒ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14d0dc6",
   "metadata": {},
   "source": [
    "* `Context`ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ ì •ì˜í•˜ê¸°\n",
    "\n",
    "  * **a. `context_answer_rag_answer()`**\n",
    "\n",
    "  * **b. `LangChainStringEvaluator`** ìƒì„±í•˜ê¸° \n",
    "    * **`prepare_data`** í†µí•´ ì •ì˜í•œ í•¨ìˆ˜ì˜ ë°˜í™˜ ê°’ì„ ì ì ˆí•˜ê²Œ ë§¤í•‘í•˜ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfb895a",
   "metadata": {},
   "source": [
    "* ì„¸ë¶€ì‚¬í•­\n",
    "\n",
    "  * **`run`**: `LLM`ì´ ìƒì„±í•œ ê²°ê³¼\n",
    "    * `context`, `answer`, `input`\n",
    "\n",
    "  * **`example`**: ë°ì´í„°ì…‹ì— ì •ì˜ëœ ë°ì´í„°\n",
    "    * `question`, `answer`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a151dec",
   "metadata": {},
   "source": [
    "* **`LangChainStringEvaluator`** í‰ê°€ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ í•„ìš”í•œ 3ê°€ì§€ ì •ë³´\n",
    "\n",
    "  * **`prediction`**: `LLM`ì´ ìƒì„±í•œ ë‹µë³€\n",
    "\n",
    "  * **`reference`**: ë°ì´í„°ì…‹ì— ì •ì˜ëœ ë‹µë³€\n",
    "\n",
    "  * **`input`**: ë°ì´í„°ì…‹ì— ì •ì˜ëœ ì§ˆë¬¸"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434b82bb",
   "metadata": {},
   "source": [
    "* **`LangChainStringEvalutor(\"context_qa\")`** = **`reference`** â†’ `Context`ë¡œ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸\n",
    "\n",
    "  * **`context_qa`** í‰ê°€ì í™œìš©ì„ ìœ„í•´ `context`, `answer`, `question`ì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì •ì˜í•˜ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb8bc3e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c0642f",
   "metadata": {},
   "source": [
    "* **`í™˜ê²½ ì„¤ì •`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c3e062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API í‚¤ë¥¼ í™˜ê²½ë³€ìˆ˜ë¡œ ê´€ë¦¬í•˜ê¸° ìœ„í•œ ì„¤ì • íŒŒì¼\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API í‚¤ ì •ë³´ ë¡œë“œ\n",
    "load_dotenv()                           # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d33436",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "from langsmith import traceable\n",
    "\n",
    "import os\n",
    "\n",
    "# LangSmith í™˜ê²½ ë³€ìˆ˜ í™•ì¸\n",
    "\n",
    "print(\"\\n--- LangSmith í™˜ê²½ ë³€ìˆ˜ í™•ì¸ ---\")\n",
    "langchain_tracing_v2 = os.getenv('LANGCHAIN_TRACING_V2')\n",
    "langchain_project = os.getenv('LANGCHAIN_PROJECT')\n",
    "langchain_api_key_status = \"ì„¤ì •ë¨\" if os.getenv('LANGCHAIN_API_KEY') else \"ì„¤ì •ë˜ì§€ ì•ŠìŒ\" # API í‚¤ ê°’ì€ ì§ì ‘ ì¶œë ¥í•˜ì§€ ì•ŠìŒ\n",
    "\n",
    "if langchain_tracing_v2 == \"true\" and os.getenv('LANGCHAIN_API_KEY') and langchain_project:\n",
    "    print(f\"âœ… LangSmith ì¶”ì  í™œì„±í™”ë¨ (LANGCHAIN_TRACING_V2='{langchain_tracing_v2}')\")\n",
    "    print(f\"âœ… LangSmith í”„ë¡œì íŠ¸: '{langchain_project}'\")\n",
    "    print(f\"âœ… LangSmith API Key: {langchain_api_key_status}\")\n",
    "    print(\"  -> ì´ì œ LangSmith ëŒ€ì‹œë³´ë“œì—ì„œ ì´ í”„ë¡œì íŠ¸ë¥¼ í™•ì¸í•´ ë³´ì„¸ìš”.\")\n",
    "else:\n",
    "    print(\"âŒ LangSmith ì¶”ì ì´ ì™„ì „íˆ í™œì„±í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‹¤ìŒì„ í™•ì¸í•˜ì„¸ìš”:\")\n",
    "    if langchain_tracing_v2 != \"true\":\n",
    "        print(f\"  - LANGCHAIN_TRACING_V2ê°€ 'true'ë¡œ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤ (í˜„ì¬: '{langchain_tracing_v2}').\")\n",
    "    if not os.getenv('LANGCHAIN_API_KEY'):\n",
    "        print(\"  - LANGCHAIN_API_KEYê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
    "    if not langchain_project:\n",
    "        print(\"  - LANGCHAIN_PROJECTê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3cee3b",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* ì…€ ì¶œë ¥\n",
    "\n",
    "    ```bash\n",
    "    --- LangSmith í™˜ê²½ ë³€ìˆ˜ í™•ì¸ ---\n",
    "    âœ… LangSmith ì¶”ì  í™œì„±í™”ë¨ (LANGCHAIN_TRACING_V2='true')\n",
    "    âœ… LangSmith í”„ë¡œì íŠ¸: 'LangChain-prantice'\n",
    "    âœ… LangSmith API Key: ì„¤ì •ë¨\n",
    "    -> ì´ì œ LangSmith ëŒ€ì‹œë³´ë“œì—ì„œ ì´ í”„ë¡œì íŠ¸ë¥¼ í™•ì¸í•´ ë³´ì„¸ìš”.\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fc4a4f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44bf615",
   "metadata": {},
   "source": [
    "* **`ì¦ì€ ì»¤ë„ ì¶©ëŒ` + `ì˜ì¡´ì„± íŒ¨í‚¤ì§€ íŒŒì¼ ì¶©ëŒ` â†’ `Python`íŒŒì¼ë¡œ ë§Œë“¤ì–´ ì‹¤í–‰**\n",
    "\n",
    "  * [**`myrag5.py`**](../15_Evaluations/myrag5.py)\n",
    "\n",
    "  * [**`eval_context.py`**](../15_Evaluations/eval_context.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cfe99f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d103de62",
   "metadata": {},
   "source": [
    "##### **`â€ myrag5.py`**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ì½”ë“œ ë‚´ìš© \n",
    "\n",
    "<small> <small>\n",
    "\n",
    "```python\n",
    "\n",
    "        # myrag5.py\n",
    "\n",
    "        from langchain_community.document_loaders import PyMuPDFLoader\n",
    "        from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "        from langchain_community.vectorstores import FAISS\n",
    "        from langchain_core.output_parsers import StrOutputParser\n",
    "        from langchain_core.runnables import RunnablePassthrough\n",
    "        from langchain_core.prompts import PromptTemplate\n",
    "        from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "        class PDFRAG:\n",
    "            def __init__(self, pdf_path, llm, chunk_size=300, chunk_overlap=50):\n",
    "                \"\"\"\n",
    "                PDF RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”\n",
    "                \n",
    "                Parameters\n",
    "                ----------\n",
    "                pdf_path : str\n",
    "                    PDF íŒŒì¼ ê²½ë¡œ\n",
    "                llm : ChatOpenAI\n",
    "                    ì‚¬ìš©í•  LLM\n",
    "                chunk_size : int\n",
    "                    ì²­í¬ í¬ê¸° (ê¸°ë³¸ê°’: 300)\n",
    "                chunk_overlap : int\n",
    "                    ì²­í¬ ì˜¤ë²„ë© (ê¸°ë³¸ê°’: 50)\n",
    "                \"\"\"\n",
    "                self.pdf_path = pdf_path\n",
    "                self.llm = llm\n",
    "                self.chunk_size = chunk_size\n",
    "                self.chunk_overlap = chunk_overlap\n",
    "                self.documents = None\n",
    "                self.vectorstore = None\n",
    "                self.retriever = None\n",
    "                \n",
    "                # ì´ˆê¸°í™”\n",
    "                self._load_and_process()\n",
    "            \n",
    "            def _load_and_process(self):\n",
    "                \"\"\"PDF ë¡œë“œ ë° ì²˜ë¦¬\"\"\"\n",
    "                # 1. PDF ë¡œë“œ\n",
    "                loader = PyMuPDFLoader(self.pdf_path)\n",
    "                docs = loader.load()\n",
    "                print(f\"âœ… PDF ë¡œë“œ ì™„ë£Œ: {len(docs)} í˜ì´ì§€\")\n",
    "                \n",
    "                # 2. ì²­í¬ ë¶„í• \n",
    "                text_splitter = RecursiveCharacterTextSplitter(\n",
    "                    chunk_size=self.chunk_size,\n",
    "                    chunk_overlap=self.chunk_overlap\n",
    "                )\n",
    "                self.documents = text_splitter.split_documents(docs)\n",
    "                print(f\"âœ… ì²­í¬ ë¶„í•  ì™„ë£Œ: {len(self.documents)} ì²­í¬ (í¬ê¸°={self.chunk_size}, ì˜¤ë²„ë©={self.chunk_overlap})\")\n",
    "                \n",
    "                # 3. ì„ë² ë”©\n",
    "                embeddings = HuggingFaceEmbeddings(\n",
    "                    model_name=\"BAAI/bge-m3\",                                  # ì„ë² ë”© ëª¨ë¸ êµì²´\n",
    "                    model_kwargs={\"device\": \"cpu\"},\n",
    "                    encode_kwargs={\"normalize_embeddings\": True},\n",
    "                )\n",
    "                print(f\"âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {embeddings.model_name}\")\n",
    "                \n",
    "                # 4. ë²¡í„°ìŠ¤í† ì–´ ìƒì„±\n",
    "                self.vectorstore = FAISS.from_documents(\n",
    "                    documents=self.documents,\n",
    "                    embedding=embeddings\n",
    "                )\n",
    "                print(f\"âœ… ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ: FAISS\")\n",
    "            \n",
    "            def create_retriever(self, k=10, search_type=\"similarity\"):         # k=10ìœ¼ë¡œ ì¦ê°€\n",
    "                \"\"\"\n",
    "                ê²€ìƒ‰ê¸° ìƒì„±\n",
    "                \n",
    "                Parameters\n",
    "                ----------\n",
    "                k : int, optional\n",
    "                    ê²€ìƒ‰í•  ë¬¸ì„œ ê°œìˆ˜ (ê¸°ë³¸ê°’: 10)\n",
    "                search_type : str, optional\n",
    "                    ê²€ìƒ‰ íƒ€ì… (ê¸°ë³¸ê°’: similarity)\n",
    "                \n",
    "                Returns\n",
    "                -------\n",
    "                retriever\n",
    "                    ë¬¸ì„œ ê²€ìƒ‰ê¸°\n",
    "                \"\"\"\n",
    "                self.retriever = self.vectorstore.as_retriever(\n",
    "                    search_type=search_type,\n",
    "                    search_kwargs={\"k\": k}\n",
    "                )\n",
    "                print(f\"âœ… ê²€ìƒ‰ê¸° ìƒì„± ì™„ë£Œ (k={k}, search_type={search_type})\")\n",
    "                return self.retriever\n",
    "            \n",
    "            def create_chain(self, retriever):\n",
    "                \"\"\"\n",
    "                RAG ì²´ì¸ ìƒì„±\n",
    "                \n",
    "                Parameters\n",
    "                ----------\n",
    "                retriever\n",
    "                    ë¬¸ì„œ ê²€ìƒ‰ê¸°\n",
    "                \n",
    "                Returns\n",
    "                -------\n",
    "                chain\n",
    "                    RAG ì²´ì¸\n",
    "                \"\"\"\n",
    "                # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
    "                prompt = PromptTemplate.from_template(\n",
    "                    \"\"\"ë‹¹ì‹ ì€ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n",
    "\n",
    "                ì£¼ì–´ì§„ Contextë¥¼ **ë°˜ë“œì‹œ** ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.\n",
    "                Contextì— ì •ë³´ê°€ ì—†ìœ¼ë©´ \"ì£„ì†¡í•©ë‹ˆë‹¤. ì œê³µëœ ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.\n",
    "\n",
    "                Context:\n",
    "                {context}\n",
    "\n",
    "                Question: {question}\n",
    "\n",
    "                Answer:\"\"\"\n",
    "                )\n",
    "                \n",
    "                # ì²´ì¸ ìƒì„±\n",
    "                chain = (\n",
    "                    {\n",
    "                        \"context\": retriever,\n",
    "                        \"question\": RunnablePassthrough(),\n",
    "                    }\n",
    "                    | prompt\n",
    "                    | self.llm\n",
    "                    | StrOutputParser()\n",
    "                )\n",
    "                \n",
    "                print(f\"âœ… RAG ì²´ì¸ ìƒì„± ì™„ë£Œ\")\n",
    "                return chain\n",
    "\n",
    "        # ========================================\n",
    "        # Question-Answer Evaluator ì¶”ê°€\n",
    "        # ========================================\n",
    "            \n",
    "            def create_chain_with_context(self, retriever):\n",
    "                \"\"\"\n",
    "                Contextë¥¼ í¬í•¨í•˜ì—¬ ë°˜í™˜í•˜ëŠ” ì²´ì¸ ìƒì„±\n",
    "                \n",
    "                Returns\n",
    "                -------\n",
    "                chain\n",
    "                    Contextì™€ Answerë¥¼ í•¨ê»˜ ë°˜í™˜í•˜ëŠ” ì²´ì¸\n",
    "                \"\"\"\n",
    "                from langchain_core.runnables import RunnablePassthrough\n",
    "                \n",
    "                # Context ì¶”ì¶œ í•¨ìˆ˜\n",
    "                def format_docs(docs):\n",
    "                    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "                \n",
    "                # í”„ë¡¬í”„íŠ¸ëŠ” ê¸°ì¡´ê³¼ ë™ì¼\n",
    "                prompt = PromptTemplate.from_template(\n",
    "                    \"\"\"ë‹¹ì‹ ì€ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n",
    "\n",
    "                ì£¼ì–´ì§„ Contextë¥¼ **ë°˜ë“œì‹œ** ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.\n",
    "                Contextì— ì •ë³´ê°€ ì—†ìœ¼ë©´ \"ì£„ì†¡í•©ë‹ˆë‹¤. ì œê³µëœ ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.\n",
    "\n",
    "                Context:\n",
    "                {context}\n",
    "\n",
    "                Question: {question}\n",
    "\n",
    "                Answer:\"\"\"\n",
    "                )\n",
    "                \n",
    "                # Contextì™€ Answerë¥¼ í•¨ê»˜ ë°˜í™˜í•˜ëŠ” ì²´ì¸\n",
    "                chain = (\n",
    "                    {\n",
    "                        \"context\": retriever | format_docs,\n",
    "                        \"question\": RunnablePassthrough(),\n",
    "                    }\n",
    "                    | RunnablePassthrough.assign(\n",
    "                        answer=prompt | self.llm | StrOutputParser()\n",
    "                    )\n",
    "                )\n",
    "                \n",
    "                print(f\"âœ… Context í¬í•¨ RAG ì²´ì¸ ìƒì„± ì™„ë£Œ\")\n",
    "                return chain\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8812d1",
   "metadata": {},
   "source": [
    "##### **`â eval_context.py` â†’ ì‹¤í–‰í•˜ê¸°**\n",
    "\n",
    "* ì½”ë“œ ë‚´ìš© \n",
    "\n",
    "<small> <small>\n",
    "\n",
    "```python\n",
    "\n",
    "            # eval_script.py\n",
    "\n",
    "            from dotenv import load_dotenv\n",
    "            load_dotenv()\n",
    "\n",
    "            import os\n",
    "            from myrag5 import PDFRAG\n",
    "            from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "            from langsmith.evaluation import evaluate, LangChainStringEvaluator\n",
    "\n",
    "            print(\"ğŸš€ Context ê¸°ë°˜ í‰ê°€ ì‹œì‘...\\n\")\n",
    "\n",
    "            # í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜ í™•ì¸\n",
    "            script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "            print(f\"ğŸ“‚ ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜: {script_dir}\\n\")\n",
    "\n",
    "            # PDF íŒŒì¼ ì ˆëŒ€ ê²½ë¡œ ìƒì„±\n",
    "            pdf_path = os.path.join(script_dir, \"data\", \"SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf\")\n",
    "            print(f\"ğŸ“„ PDF ê²½ë¡œ: {pdf_path}\")\n",
    "\n",
    "            # íŒŒì¼ ì¡´ì¬ í™•ì¸\n",
    "            if not os.path.exists(pdf_path):\n",
    "                print(f\"âŒ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤!\")\n",
    "                print(f\"âŒ ê²½ë¡œ: {pdf_path}\\n\")\n",
    "                exit(1)\n",
    "                \n",
    "            print(f\"âœ… PDF í™•ì¸: {pdf_path}\\n\")\n",
    "                \n",
    "\n",
    "            # LLM ìƒì„±\n",
    "            llm = ChatGoogleGenerativeAI(\n",
    "                model=\"gemini-2.5-flash-lite\",\n",
    "                temperature=0\n",
    "            )\n",
    "            print(\"âœ… LLM ìƒì„± ì™„ë£Œ: gemini-2.5-flash-lite\\n\")\n",
    "\n",
    "            # RAG ìƒì„±\n",
    "            rag = PDFRAG(\n",
    "                pdf_path,  # ì ˆëŒ€ ê²½ë¡œ ì‚¬ìš©!\n",
    "                llm,\n",
    "                chunk_size=300,\n",
    "                chunk_overlap=50,\n",
    "            )\n",
    "\n",
    "            # Retriever & Chain (k ëŠ˜ë¦¬ê¸°)\n",
    "            retriever = rag.create_retriever(k=10)\n",
    "\n",
    "            # Context í¬í•¨ ì²´ì¸ ìƒì„±\n",
    "            chain_with_context = rag.create_chain_with_context(retriever)\n",
    "\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(\"ğŸ§ª í…ŒìŠ¤íŠ¸ ì‹¤í–‰...\")\n",
    "            print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "            # í…ŒìŠ¤íŠ¸\n",
    "            test_question = \"ì‚¼ì„±ì „ìê°€ ìì²´ ê°œë°œí•œ ìƒì„±í˜• AIì˜ ì´ë¦„ì€ ë¬´ì—‡ì¸ê°€ìš”?\"\n",
    "            test_result = chain_with_context.invoke(test_question)\n",
    "            print(f\"ì§ˆë¬¸: {test_question}\")\n",
    "            print(f\"Context: {test_result['context'][:200]}...\")\n",
    "            print(f\"ë‹µë³€: {test_result['answer']}\\n\")\n",
    "\n",
    "            # ========================================\n",
    "            # Context ê¸°ë°˜ í‰ê°€ í•¨ìˆ˜\n",
    "            # ========================================\n",
    "\n",
    "            def context_answer_rag_answer(inputs: dict):\n",
    "                \"\"\"\n",
    "                Contextì™€ Answerë¥¼ í•¨ê»˜ ë°˜í™˜\n",
    "                \n",
    "                Returns\n",
    "                -------\n",
    "                dict\n",
    "                    - context: ê²€ìƒ‰ëœ Context\n",
    "                    - answer: LLM ë‹µë³€\n",
    "                    - query: ì§ˆë¬¸\n",
    "                \"\"\"\n",
    "                result = chain_with_context.invoke(inputs[\"question\"])\n",
    "                return {\n",
    "                    \"context\": result[\"context\"],\n",
    "                    \"answer\": result[\"answer\"],\n",
    "                    \"query\": inputs[\"question\"],\n",
    "                }\n",
    "\n",
    "            # ========================================\n",
    "            # í‰ê°€ì ìƒì„±\n",
    "            # ========================================\n",
    "\n",
    "            print(\"=\"*50)\n",
    "            print(\"ğŸ“Š í‰ê°€ì ìƒì„±...\")\n",
    "            print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "            # 1. COT_QA Evaluator (Chain-of-Thought)\n",
    "            cot_qa_evaluator = LangChainStringEvaluator(\n",
    "                \"cot_qa\",\n",
    "                config={\"llm\": llm},\n",
    "                prepare_data=lambda run, example: {\n",
    "                    \"prediction\": run.outputs[\"answer\"],      # LLM ë‹µë³€\n",
    "                    \"reference\": run.outputs[\"context\"],      # Context\n",
    "                    \"input\": example.inputs[\"question\"],      # ì§ˆë¬¸\n",
    "                },\n",
    "            )\n",
    "\n",
    "            # 2. Context_QA Evaluator\n",
    "            context_qa_evaluator = LangChainStringEvaluator(\n",
    "                \"context_qa\",\n",
    "                config={\"llm\": llm},\n",
    "                prepare_data=lambda run, example: {\n",
    "                    \"prediction\": run.outputs[\"answer\"],      # LLM ë‹µë³€\n",
    "                    \"reference\": run.outputs[\"context\"],      # Context\n",
    "                    \"input\": example.inputs[\"question\"],      # ì§ˆë¬¸\n",
    "                },\n",
    "            )\n",
    "\n",
    "            print(\"âœ… í‰ê°€ì ìƒì„± ì™„ë£Œ\\n\")\n",
    "\n",
    "            # ========================================\n",
    "            # í‰ê°€ ì‹¤í–‰\n",
    "            # ========================================\n",
    "\n",
    "            print(\"=\"*50)\n",
    "            print(\"ğŸ“Š í‰ê°€ ì‹¤í–‰...\")\n",
    "            print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "            try:\n",
    "                experiment_results = evaluate(\n",
    "                    context_answer_rag_answer,\n",
    "                    data=\"RAG_EVAL_DATASET\",\n",
    "                    evaluators=[cot_qa_evaluator, context_qa_evaluator],\n",
    "                    experiment_prefix=\"RAG_EVAL_CONTEXT\",\n",
    "                    metadata={\n",
    "                        \"variant\": \"Context ê¸°ë°˜ í‰ê°€ (COT_QA + Context_QA)\",\n",
    "                        \"k\": 7,\n",
    "                        \"chunk_size\": 300,\n",
    "                    },\n",
    "                    max_concurrency=1,\n",
    "                )\n",
    "                \n",
    "                print(\"\\n\" + \"=\"*50)\n",
    "                print(\"âœ… í‰ê°€ ì™„ë£Œ!\")\n",
    "                print(\"=\"*50)\n",
    "                print(f\"\\nê²°ê³¼: {experiment_results}\\n\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\nâŒ ì—ëŸ¬ ë°œìƒ: {e}\\n\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **`RAG ê¸°ë°˜ í‰ê°€` vs `Context ê¸°ë°˜ í‰ê°€`**\n",
    "\n",
    "  * ë¹„êµ \n",
    "\n",
    "  | í‰ê°€ ë°©ì‹ | ì„¤ëª… | ì¥ì  | ë‹¨ì  |\n",
    "  |----------|------|------|------|\n",
    "  | **QA** (ê¸°ì¡´) | Ground Truth ë¹„êµ | ì •í™•ë„ ëª…í™• | ë‹µë³€ í˜•ì‹ì— ë¯¼ê° |\n",
    "  | **Context_QA** (ìƒˆë¡œìš´) | Context ê¸°ë°˜ í‰ê°€ | ìœ ì—°í•œ í‰ê°€ | Ground Truth ë¬´ì‹œ |\n",
    "\n",
    "<br>\n",
    "\n",
    "* **`Context ê¸°ë°˜ í‰ê°€`ì˜ `ì¥ì `**\n",
    "\n",
    "  * Ground Truth ë¬´ì‹œ\n",
    "  * **`Context` ê¸°ë°˜ í‰ê°€**\n",
    "  * **ë” ìœ ì—°í•œ í‰ê°€**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d2fc3b",
   "metadata": {},
   "source": [
    "#### **5) `Criteria`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d186f4",
   "metadata": {},
   "source": [
    "* ê¸°ì¤€ê°’ ì°¸ì¡° ë ˆì´ë¸”(ì •ë‹µ ë‹µë³€)ì´ ì—†ê±°ë‚˜ ì–»ê¸° í˜ë“  ê²½ìš° \n",
    "\n",
    "* **`criteria`** or **`score`** í‰ê°€ì ì‚¬ìš© â†’ **`ì‚¬ìš©ì ì§€ì • ê¸°ì¤€ ì§‘í•©`ì— ëŒ€í•´ ì‹¤í–‰ í‰ê°€ ê°€ëŠ¥**\n",
    "\n",
    "* **`ëª¨ë¸ì˜ ë‹µë³€ì— ëŒ€í•œ ë†’ì€ ìˆ˜ì¤€ì˜ ì˜ë¯¸ë¡ ì  ì¸¡ë©´ì„ ëª¨ë‹ˆí„°ë§ í•˜ë ¤ëŠ” ê²½ìš°ì— ìœ ìš©`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d211a2d5",
   "metadata": {},
   "source": [
    "* LangChainStringEvaluator\n",
    "    (\"criteria\", \n",
    "     config={ \"criteria\": *ì•„ë˜ ì¤‘ í•˜ë‚˜ì˜ criterion* })\n",
    "\n",
    "* *`criteria`*\n",
    "\n",
    "  | ê¸°ì¤€               | ì„¤ëª…                   |\n",
    "  |------------------|----------------------|\n",
    "  | conciseness      | ë‹µë³€ì´ ê°„ê²°í•˜ê³  ê°„ë‹¨í•œì§€ í‰ê°€     |\n",
    "  | relevance        | ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ìˆëŠ”ì§€ í‰ê°€   |\n",
    "  | correctness      | ë‹µë³€ì´ ì˜³ì€ì§€ í‰ê°€           |\n",
    "  | coherence        | ë‹µë³€ì´ ì¼ê´€ì„±ì´ ìˆëŠ”ì§€ í‰ê°€      |\n",
    "  | harmfulness      | ë‹µë³€ì´ í•´ë¡­ê±°ë‚˜ ìœ í•´í•œì§€ í‰ê°€     |\n",
    "  | maliciousness    | ë‹µë³€ì´ ì•…ì˜ì ì´ê±°ë‚˜ ì•…í™”ì‹œí‚¤ëŠ”ì§€ í‰ê°€ |\n",
    "  | helpfulness      | ë‹µë³€ì´ ë„ì›€ì´ ë˜ëŠ”ì§€ í‰ê°€       |\n",
    "  | controversiality | ë‹µë³€ì´ ë…¼ë€ì´ ë˜ëŠ”ì§€ í‰ê°€       |\n",
    "  | misogyny         | ë‹µë³€ì´ ì—¬ì„±ì„ ë¹„í•˜í•˜ëŠ”ì§€ í‰ê°€     |\n",
    "  | criminality      | ë‹µë³€ì´ ë²”ì£„ë¥¼ ì´‰ì§„í•˜ëŠ”ì§€ í‰ê°€     |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d3c9f1",
   "metadata": {},
   "source": [
    "* ì½”ë“œ ì˜ˆì‹œ\n",
    "\n",
    "```python\n",
    "\n",
    "    from langsmith.evaluation import evaluate, LangChainStringEvaluator\n",
    "\n",
    "    # í‰ê°€ì ì„¤ì •\n",
    "    criteria_evaluator = [\n",
    "        LangChainStringEvaluator(\"criteria\", config={\"criteria\": \"conciseness\"}),\n",
    "        LangChainStringEvaluator(\"criteria\", config={\"criteria\": \"misogyny\"}),\n",
    "        LangChainStringEvaluator(\"criteria\", config={\"criteria\": \"criminality\"}),\n",
    "    ]\n",
    "\n",
    "    # ë°ì´í„°ì…‹ ì´ë¦„ ì„¤ì •\n",
    "    dataset_name = \"RAG_EVAL_DATASET\"\n",
    "\n",
    "    # í‰ê°€ ì‹¤í–‰\n",
    "    experiment_results = evaluate(\n",
    "        ask_question,\n",
    "        data=dataset_name,\n",
    "        evaluators=criteria_evaluator,\n",
    "        experiment_prefix=\"CRITERIA-EVAL\",\n",
    "        # ì‹¤í—˜ ë©”íƒ€ë°ì´í„° ì§€ì •\n",
    "        metadata={\n",
    "            \"variant\": \"criteria ë¥¼ í™œìš©í•œ í‰ê°€\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "* `LangChain`ì—ì„œ í™•ì¸í•˜ê¸°\n",
    "\n",
    "  * ![`LangChain-conciseness`](../15_Evaluations/assets/output-03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1815cff6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32111921",
   "metadata": {},
   "source": [
    "##### **`â€ ì •ë‹µì´ ì¡´ì¬í•˜ëŠ” ê²½ìš° Evaluator í™œìš©`** (*labeled_criteria*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4429c53c",
   "metadata": {},
   "source": [
    "* ì •ë‹µì´ ì¡´ì¬í•˜ëŠ” ê²½ìš°: **`LLM` ì´ `ìƒì„±í•œ ë‹µë³€`ê³¼ `ì •ë‹µ ë‹µë³€`ì„ `ë¹„êµ` â†’ í‰ê°€ ê°€ëŠ¥**\n",
    "\n",
    "  * **`reference` = `ì •ë‹µ ë‹µë³€`**\n",
    "  * **`prediction` = `LLM`ì´ `ìƒì„±í•œ ë‹µë³€ ì „ë‹¬`**\n",
    "\n",
    "  * **`ë³„ë„ì˜ ì„¤ì •` â†’ `prepare_data` â†’ ì •ì˜**\n",
    "\n",
    "  * **`config`ì˜ `llm` â†’ `ë‹µë³€ í‰ê°€`ì— í™œìš©ë˜ëŠ” LLMìœ¼ë¡œ ì •ì˜**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01edba6",
   "metadata": {},
   "source": [
    "##### **â†³ `eval_labeled_criteria.py`**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f018fb37",
   "metadata": {},
   "source": [
    "* ì½”ë“œ ë‚´ìš© \n",
    "\n",
    "<small> <small>\n",
    "\n",
    "```python\n",
    "\n",
    "            # ========================================\n",
    "            # eval_labeled_criteria.py\n",
    "            # ì •ë‹µ ê¸°ë°˜ Criteria í‰ê°€\n",
    "            # ========================================\n",
    "\n",
    "            from dotenv import load_dotenv\n",
    "            load_dotenv()\n",
    "\n",
    "            import os\n",
    "            from myrag5 import PDFRAG\n",
    "            from langchain_ollama import ChatOllama             # Ollama ì‚¬ìš©\n",
    "            from langsmith.evaluation import evaluate, LangChainStringEvaluator\n",
    "            import warnings\n",
    "            warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "            print(\"ğŸš€ Labeled Criteria í‰ê°€ ì‹œì‘...\\n\")\n",
    "\n",
    "\n",
    "            # ========================================\n",
    "            # íŒŒì¼ ê²½ë¡œ & LLM ì„¤ì •\n",
    "            # ========================================\n",
    "\n",
    "            script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "            pdf_path = os.path.join(script_dir, \"data\", \"SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf\")\n",
    "\n",
    "            if not os.path.exists(pdf_path):\n",
    "                print(f\"âŒ íŒŒì¼ ì—†ìŒ: {pdf_path}\")\n",
    "                exit(1)\n",
    "\n",
    "            print(f\"âœ… PDF í™•ì¸: {pdf_path}\\n\")\n",
    "\n",
    "            # LLM ìƒì„±\n",
    "\n",
    "            llm = ChatOllama(\n",
    "                model=\"qwen2.5-coder:7b-instruct\",\n",
    "                temperature=0\n",
    "            )\n",
    "\n",
    "            print(\"âœ… ë¡œì»¬ LLM ìƒì„± ì™„ë£Œ: {llm.model}\\n\")\n",
    "\n",
    "            # ========================================\n",
    "            # RAG ì‹œìŠ¤í…œ ìƒì„±\n",
    "            # ========================================\n",
    "\n",
    "            rag = PDFRAG(pdf_path, llm, chunk_size=300, chunk_overlap=50)\n",
    "            retriever = rag.create_retriever(k=10, search_type=\"similarity\")\n",
    "\n",
    "            # Context í¬í•¨ ì²´ì¸\n",
    "            chain_with_context = rag.create_chain_with_context(retriever)\n",
    "\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(\"ğŸ§ª í…ŒìŠ¤íŠ¸ ì‹¤í–‰...\")\n",
    "            print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "            # í…ŒìŠ¤íŠ¸\n",
    "            test_question = \"ì‚¼ì„±ì „ìê°€ ìì²´ ê°œë°œí•œ ìƒì„±í˜• AIì˜ ì´ë¦„ì€ ë¬´ì—‡ì¸ê°€ìš”?\"\n",
    "\n",
    "            try:\n",
    "                test_result = chain_with_context.invoke(test_question)\n",
    "                print(f\"ì§ˆë¬¸: {test_question}\")\n",
    "                print(f\"Context: {test_result['context'][:100]}...\")\n",
    "                print(f\"ë‹µë³€: {test_result['answer']}\\n\")\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}\\n\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                exit(1)\n",
    "\n",
    "\n",
    "            # ========================================\n",
    "            # Context + Answer ë°˜í™˜ í•¨ìˆ˜\n",
    "            # ========================================\n",
    "\n",
    "            def context_answer_rag_answer(inputs: dict):\n",
    "                \"\"\"\n",
    "                Contextì™€ Answerë¥¼ í•¨ê»˜ ë°˜í™˜\n",
    "                \"\"\"\n",
    "                result = chain_with_context.invoke(inputs[\"question\"])\n",
    "                return {\n",
    "                    \"context\": result[\"context\"],\n",
    "                    \"answer\": result[\"answer\"],\n",
    "                    \"query\": inputs[\"question\"],\n",
    "                }\n",
    "\n",
    "            # ========================================\n",
    "            # Labeled Criteria í‰ê°€ì ìƒì„±\n",
    "            # ========================================\n",
    "\n",
    "            print(\"=\"*50)\n",
    "            print(\"ğŸ“Š Labeled Criteria í‰ê°€ì ìƒì„±...\")\n",
    "            print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "            # í‰ê°€ìš© LLM \n",
    "            eval_llm = ChatOllama(\n",
    "                model=\"qwen2.5-coder:7b-instruct\",\n",
    "                temperature=0\n",
    "            )\n",
    "\n",
    "            # 1. Helpfulness Evaluator (Ground Truth ê¸°ë°˜)\n",
    "            helpfulness_evaluator = LangChainStringEvaluator(\n",
    "                \"labeled_criteria\",\n",
    "                config={\n",
    "                    \"criteria\": {\n",
    "                        \"helpfulness\": (\n",
    "                            \"Is this submission helpful to the user, \"\n",
    "                            \"taking into account the correct reference answer?\"\n",
    "                        )\n",
    "                    },\n",
    "                    \"llm\": llm,\n",
    "                },\n",
    "                prepare_data=lambda run, example: {\n",
    "                    \"prediction\": run.outputs[\"answer\"],           # LLM ë‹µë³€\n",
    "                    \"reference\": example.outputs[\"answer\"],        # Ground Truth\n",
    "                    \"input\": example.inputs[\"question\"],           # ì§ˆë¬¸\n",
    "                },\n",
    "            )\n",
    "\n",
    "            # 2. Relevance Evaluator (Context ê¸°ë°˜)\n",
    "            relevance_evaluator = LangChainStringEvaluator(\n",
    "                \"labeled_criteria\",\n",
    "                config={\n",
    "                    \"criteria\": \"relevance\",                # ë‹µë³€ì´ Contextë¥¼ ì°¸ì¡°í•˜ëŠ”ê°€?\n",
    "                    \"llm\": llm,\n",
    "                },\n",
    "                prepare_data=lambda run, example: {\n",
    "                    \"prediction\": run.outputs[\"answer\"],           # LLM ë‹µë³€\n",
    "                    \"reference\": run.outputs[\"context\"],           # Context\n",
    "                    \"input\": example.inputs[\"question\"],           # ì§ˆë¬¸\n",
    "                },\n",
    "            )\n",
    "\n",
    "            # 3. Accuracy Evaluator (Ground Truth ê¸°ë°˜)\n",
    "            accuracy_evaluator = LangChainStringEvaluator(\n",
    "                \"labeled_criteria\",\n",
    "                config={\n",
    "                    \"criteria\": {\n",
    "                        \"accuracy\": (\n",
    "                            \"Is this submission factually accurate \"\n",
    "                            \"compared to the reference answer?\"\n",
    "                        )\n",
    "                    },\n",
    "                    \"llm\": llm,\n",
    "                },\n",
    "                prepare_data=lambda run, example: {\n",
    "                    \"prediction\": run.outputs[\"answer\"],           # LLM ë‹µë³€\n",
    "                    \"reference\": example.outputs[\"answer\"],        # Ground Truth\n",
    "                    \"input\": example.inputs[\"question\"],           # ì§ˆë¬¸\n",
    "                },\n",
    "            )\n",
    "\n",
    "            print(\"âœ… í‰ê°€ì ìƒì„± ì™„ë£Œ\\n\")\n",
    "\n",
    "            # ========================================\n",
    "            # í‰ê°€ ì‹¤í–‰\n",
    "            # ========================================\n",
    "\n",
    "            print(\"=\"*50)\n",
    "            print(\"ğŸ“Š í‰ê°€ ì‹¤í–‰...\")\n",
    "            print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "            try:\n",
    "                experiment_results = evaluate(\n",
    "                    context_answer_rag_answer,\n",
    "                    data=\"RAG_EVAL_DATASET\",\n",
    "                    evaluators=[\n",
    "                        helpfulness_evaluator,\n",
    "                        relevance_evaluator,\n",
    "                        accuracy_evaluator,\n",
    "                    ],\n",
    "                    experiment_prefix=\"RAG_EVAL_LABELED_CRITERIA_LOCAL\",\n",
    "                    metadata={\n",
    "                        \"variant\": \"Labeled Criteria (Helpfulness + Relevance + Accuracy) - Local\",\n",
    "                        \"k\": 10,\n",
    "                        \"chunk_size\": 300,\n",
    "                        \"llm\": \"qwen2.5-coder:7b-instruct\",\n",
    "                    },\n",
    "                    max_concurrency=1,\n",
    "                )\n",
    "                \n",
    "                print(\"\\n\" + \"=\"*50)\n",
    "                print(\"âœ… í‰ê°€ ì™„ë£Œ!\")\n",
    "                print(\"=\"*50)\n",
    "                print(f\"\\nê²°ê³¼: {experiment_results}\\n\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\nâŒ ì—ëŸ¬ ë°œìƒ: {e}\\n\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f232c5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3c67da",
   "metadata": {},
   "source": [
    "* **`ì²« ë²ˆì§¸ ì‹œë„`**\n",
    "\n",
    "  * *API í• ë‹¹ëŸ‰ ë¶€ì¡±ìœ¼ë¡œ ì œëŒ€ë¡œ ëœ ê²°ê³¼ê°€ ë‚˜ì˜¤ì§€ ì•ŠìŒ*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dc1bee",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* ê²°ê³¼\n",
    "\n",
    "  ```markdown\n",
    "\n",
    "  ğŸš€ Labeled Criteria í‰ê°€ ì‹œì‘...\n",
    "\n",
    "  âœ… PDF í™•ì¸: /Users/jay/Projects/20250727-langchain-note/15_Evaluations/data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf\n",
    "\n",
    "  âœ… LLM ìƒì„± ì™„ë£Œ\n",
    "\n",
    "  âœ… PDF ë¡œë“œ ì™„ë£Œ: 23 í˜ì´ì§€\n",
    "  âœ… ì²­í¬ ë¶„í•  ì™„ë£Œ: 119 ì²­í¬ (í¬ê¸°=300, ì˜¤ë²„ë©=50)\n",
    "  âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: BAAI/bge-m3\n",
    "  âœ… ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ: FAISS\n",
    "  âœ… ê²€ìƒ‰ê¸° ìƒì„± ì™„ë£Œ (k=10, search_type=similarity)\n",
    "  âœ… Context í¬í•¨ RAG ì²´ì¸ ìƒì„± ì™„ë£Œ\n",
    "\n",
    "  ==================================================\n",
    "  ğŸ§ª í…ŒìŠ¤íŠ¸ ì‹¤í–‰...\n",
    "  ==================================================\n",
    "\n",
    "  ì§ˆë¬¸: ì‚¼ì„±ì „ìê°€ ìì²´ ê°œë°œí•œ ìƒì„±í˜• AIì˜ ì´ë¦„ì€ ë¬´ì—‡ì¸ê°€ìš”?\n",
    "  Context: SPRi AI Brief |  \n",
    "  2023-12ì›”í˜¸\n",
    "  10\n",
    "  ì‚¼ì„±ì „ì, ìì²´ ê°œë°œ ìƒì„± AI â€˜ì‚¼ì„± ê°€ìš°ìŠ¤â€™ ê³µê°œ\n",
    "  n ì‚¼ì„±ì „ìê°€ ì˜¨ë””ë°”ì´ìŠ¤ì—ì„œ ì‘ë™ ê°€ëŠ¥í•˜ë©° ì–¸ì–´, ì½”ë“œ, ì´ë¯¸ì§€ì˜ 3ê°œ ëª¨ë¸ë¡œ êµ¬ì„±ëœ ìì²´ ê°œë°œ ìƒì„± \n",
    "  AI ëª¨ë¸ â€˜ì‚¼ì„± ê°€ìš°ìŠ¤â€™ë¥¼ ê³µê°œ\n",
    "  n ì‚¼ì„±ì „ìëŠ” ì‚¼ì„± ê°€ìš°ìŠ¤ë¥¼ ë‹¤ì–‘í•œ ì œí’ˆì— ë‹¨ê³„ì ìœ¼ë¡œ íƒ‘ì¬í•  ê³„íšìœ¼ë¡œ, ì˜¨ë””ë°”ì´ìŠ¤ ì‘ë™ì´ ê°€ëŠ¥í•œ \n",
    "  ì‚¼ì„± ê°€ìš°ìŠ¤ëŠ”...\n",
    "  ë‹µë³€: ì‚¼ì„±ì „ìê°€ ìì²´ ê°œë°œí•œ ìƒì„±í˜• AIì˜ ì´ë¦„ì€ 'ì‚¼ì„± ê°€ìš°ìŠ¤'ì…ë‹ˆë‹¤.\n",
    "\n",
    "  ==================================================\n",
    "  ğŸ“Š Labeled Criteria í‰ê°€ì ìƒì„±...\n",
    "  ==================================================\n",
    "\n",
    "  âœ… í‰ê°€ì ìƒì„± ì™„ë£Œ\n",
    "\n",
    "  ==================================================\n",
    "  ğŸ“Š í‰ê°€ ì‹¤í–‰...\n",
    "  ==================================================\n",
    "\n",
    "  4/5\n",
    "\n",
    "  ```\n",
    "\n",
    "<br>\n",
    "\n",
    "* `LangSmith`ì—ì„œ í™•ì¸í•˜ê¸°\n",
    "\n",
    "  * ![apií• ë‹¹ëŸ‰ë¶€ì¡±ìœ¼ë¡œ4/5ë§Œì™„ì„±](../15_Evaluations/assets/Criteria_EVAL_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9df4ff",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95717109",
   "metadata": {},
   "source": [
    "* **`ë‘ë²ˆì§¸ ì‹œë„`**\n",
    "\n",
    "  * *ì—¬ì „íˆ API í• ë‹¹ëŸ‰ì´ ë§ì´ ì†Œìš”*\n",
    "\n",
    "  * `Local LLM` ì„¤ì¹˜ â†’ **`Qwen2.5-Coder-7B`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0ccd4d",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* **`eval_labeled_criteria.py`** \n",
    "\n",
    "    ```markdown\n",
    "\n",
    "    ğŸš€ Labeled Criteria í‰ê°€ ì‹œì‘...\n",
    "\n",
    "    âœ… PDF í™•ì¸: /Users/jay/Projects/20250727-langchain-note/15_Evaluations/data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf\n",
    "\n",
    "    âœ… ë¡œì»¬ LLM ìƒì„± ì™„ë£Œ: Qwen2.5-Coder-7B\n",
    "\n",
    "    âœ… PDF ë¡œë“œ ì™„ë£Œ: 23 í˜ì´ì§€\n",
    "    âœ… ì²­í¬ ë¶„í•  ì™„ë£Œ: 119 ì²­í¬ (í¬ê¸°=300, ì˜¤ë²„ë©=50)\n",
    "    âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: BAAI/bge-m3\n",
    "    âœ… ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ: FAISS\n",
    "    âœ… ê²€ìƒ‰ê¸° ìƒì„± ì™„ë£Œ (k=10, search_type=similarity)\n",
    "    âœ… Context í¬í•¨ RAG ì²´ì¸ ìƒì„± ì™„ë£Œ\n",
    "\n",
    "    ==================================================\n",
    "    ğŸ§ª í…ŒìŠ¤íŠ¸ ì‹¤í–‰...\n",
    "    ==================================================\n",
    "\n",
    "    ì§ˆë¬¸: ì‚¼ì„±ì „ìê°€ ìì²´ ê°œë°œí•œ ìƒì„±í˜• AIì˜ ì´ë¦„ì€ ë¬´ì—‡ì¸ê°€ìš”?\n",
    "    Context: SPRi AI Brief |  \n",
    "    2023-12ì›”í˜¸\n",
    "    10\n",
    "    ì‚¼ì„±ì „ì, ìì²´ ê°œë°œ ìƒì„± AI â€˜ì‚¼ì„± ê°€ìš°ìŠ¤â€™ ê³µê°œ\n",
    "    n ì‚¼ì„±ì „ìê°€ ì˜¨ë””ë°”ì´ìŠ¤ì—ì„œ ì‘ë™ ê°€ëŠ¥í•˜ë©° ì–¸ì–´, ì½”ë“œ, ì´ë¯¸ì§€ì˜ 3ê°œ...\n",
    "    ë‹µë³€: ì‚¼ì„±ì „ìê°€ ìì²´ ê°œë°œí•œ ìƒì„±í˜• AIì˜ ì´ë¦„ì€ 'ì‚¼ì„± ê°€ìš°ìŠ¤'ì…ë‹ˆë‹¤.\n",
    "\n",
    "    ==================================================\n",
    "    ğŸ“Š Labeled Criteria í‰ê°€ì ìƒì„±...\n",
    "    ==================================================\n",
    "\n",
    "    âœ… í‰ê°€ì ìƒì„± ì™„ë£Œ\n",
    "\n",
    "    ==================================================\n",
    "    ğŸ“Š í‰ê°€ ì‹¤í–‰...\n",
    "    ==================================================\n",
    "\n",
    "    5it [23:45, 285.13s/it]\n",
    "\n",
    "    ==================================================\n",
    "    âœ… í‰ê°€ ì™„ë£Œ!\n",
    "    ==================================================\n",
    "\n",
    "    ê²°ê³¼: <ExperimentResults RAG_EVAL_LABELED_CRITERIA_LOCAL-d15e33df>\n",
    "\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e8763a",
   "metadata": {},
   "source": [
    "* **`LangSmith`ì—ì„œ ê²°ê³¼ í™•ì¸í•´ë³´ê¸°**\n",
    "\n",
    "  * ![`Criteria_EVAL2`](../15_Evaluations/assets/Criteria_EVAL_2.png)\n",
    "\n",
    "  * *ê¸€ì”¨ë¡œ í‘œê¸°ëœ ë¶€ë¶„*: `**y**` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee3c539",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e193eb46",
   "metadata": {},
   "source": [
    "* **`ê²°ê³¼ ë¹„êµ`**\n",
    "\n",
    "|              | try 1                  | try 2                 |\n",
    "|--------------|------------------------|-------------------------|\n",
    "| í‰ê°€ ê²°ê³¼        | **`4`/5**                    | **`5/5`**          |\n",
    "| LangSmith í™•ì¸ | ![ê²°ê³¼1](../15_Evaluations/assets/Criteria_EVAL_1.png)                     | ![ê²°ê³¼2](../15_Evaluations/assets/Criteria_EVAL_2.png)           |\n",
    "| ì°¨ì´ì           | `gemini-2.5-flash` ëª¨ë¸ ì‚¬ìš© | `ë¡œì»¬ LLM` ëª¨ë¸ ì‚¬ìš© |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fe3632",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c0942b",
   "metadata": {},
   "source": [
    "##### **`â ì‚¬ìš©ì ì •ì˜ ì ìˆ˜ Evaluator`** (*labeled_score_string*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290efb54",
   "metadata": {},
   "source": [
    "* ì ìˆ˜ë¥¼ ë°˜í™˜í•˜ëŠ” í‰ê°€ì ìƒì„± ì˜ˆì‹œ\n",
    "\n",
    "  * **`normalize_by` â†’ ì ìˆ˜ ì •ê·œí™” ê°€ëŠ¥**\n",
    "\n",
    "  * **`ë³€í™˜ëœ ì ìˆ˜` = `0 ~ 1` ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ `ì •ê·œí™”`**\n",
    "\n",
    "  * **ì½”ë“œ ì˜ˆì‹œ ì† `accuracy` = `ì‚¬ìš©ìê°€ ì„ì˜ë¡œ ì •ì˜í•œ ê¸°ì¤€`**\n",
    "    * âˆŸâ†’ **`ì í•©í•œ Prompt`ë¥¼ `ì •ì˜`í•˜ì—¬ `ì‚¬ìš©` ê°€ëŠ¥**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3626e0c0",
   "metadata": {},
   "source": [
    "##### **â†³ `eval_labeled_score.py`**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1156fe",
   "metadata": {},
   "source": [
    "* ì½”ë“œ ë‚´ìš©\n",
    "\n",
    "<small> <small>\n",
    "\n",
    "```python\n",
    "\n",
    "            # ========================================\n",
    "            # eval_labeled_score.py (ë³µë¶™!)\n",
    "            # ì ìˆ˜ ê¸°ë°˜ í‰ê°€\n",
    "            # ========================================\n",
    "\n",
    "            from dotenv import load_dotenv\n",
    "            load_dotenv()\n",
    "\n",
    "            import os\n",
    "            from myrag5 import PDFRAG\n",
    "            from langchain_ollama import ChatOllama             # Ollama ì‚¬ìš©\n",
    "            from langsmith.evaluation import evaluate, LangChainStringEvaluator\n",
    "            import warnings\n",
    "            warnings.filterwarnings(\"ignore\")\n",
    "            os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"           # Tokenizer Parallelism ì¶œë ¥ ê²½ê³  ë©”ì‹œì§€ ì–µì œ\n",
    "\n",
    "            print(\"ğŸš€ Labeled Score í‰ê°€ ì‹œì‘...\\n\")\n",
    "\n",
    "\n",
    "            # ========================================\n",
    "            # íŒŒì¼ ê²½ë¡œ & LLM ì„¤ì •\n",
    "            # ========================================\n",
    "\n",
    "            script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "            pdf_path = os.path.join(script_dir, \"data\", \"SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf\")\n",
    "\n",
    "            if not os.path.exists(pdf_path):\n",
    "                print(f\"âŒ íŒŒì¼ ì—†ìŒ: {pdf_path}\")\n",
    "                exit(1)\n",
    "\n",
    "            print(f\"âœ… PDF í™•ì¸: {pdf_path}\\n\")\n",
    "\n",
    "            # LLM ìƒì„±\n",
    "\n",
    "            llm = ChatOllama(\n",
    "                model=\"qwen2.5-coder:7b-instruct\",\n",
    "                temperature=0\n",
    "            )\n",
    "\n",
    "            print(\"âœ… ë¡œì»¬ LLM ìƒì„± ì™„ë£Œ: Qwen2.5-Coder-7B \\n\")\n",
    "\n",
    "\n",
    "            # ========================================\n",
    "            # RAG ì‹œìŠ¤í…œ ìƒì„±\n",
    "            # ========================================\n",
    "\n",
    "            rag = PDFRAG(pdf_path, llm, chunk_size=300, chunk_overlap=50)\n",
    "            retriever = rag.create_retriever(k=10, search_type=\"similarity\")\n",
    "\n",
    "            # Context í¬í•¨ ì²´ì¸\n",
    "            chain_with_context = rag.create_chain_with_context(retriever)\n",
    "\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(\"ğŸ§ª í…ŒìŠ¤íŠ¸ ì‹¤í–‰...\")\n",
    "            print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "            # í…ŒìŠ¤íŠ¸\n",
    "            test_question = \"ì‚¼ì„±ì „ìê°€ ìì²´ ê°œë°œí•œ ìƒì„±í˜• AIì˜ ì´ë¦„ì€ ë¬´ì—‡ì¸ê°€ìš”?\"\n",
    "\n",
    "            try:\n",
    "                test_result = chain_with_context.invoke(test_question)\n",
    "                print(f\"ì§ˆë¬¸: {test_question}\")\n",
    "                print(f\"Context: {test_result['context'][:200]}...\")\n",
    "                print(f\"ë‹µë³€: {test_result['answer']}\\n\")\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}\\n\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                exit(1)\n",
    "\n",
    "\n",
    "            # ========================================\n",
    "            # Context + Answer ë°˜í™˜ í•¨ìˆ˜\n",
    "            # ========================================\n",
    "\n",
    "            def context_answer_rag_answer(inputs: dict):\n",
    "                \"\"\"\n",
    "                Contextì™€ Answerë¥¼ í•¨ê»˜ ë°˜í™˜\n",
    "                \"\"\"\n",
    "                result = chain_with_context.invoke(inputs[\"question\"])\n",
    "                return {\n",
    "                    \"context\": result[\"context\"],\n",
    "                    \"answer\": result[\"answer\"],\n",
    "                    \"query\": inputs[\"question\"],\n",
    "                }\n",
    "\n",
    "\n",
    "            # ========================================\n",
    "            # Labeled Score í‰ê°€ì ìƒì„±\n",
    "            # ========================================\n",
    "\n",
    "            print(\"=\"*50)\n",
    "            print(\"ğŸ“Š Labeled Score í‰ê°€ì ìƒì„±...\")\n",
    "            print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "\n",
    "            # í‰ê°€ìš© LLM\n",
    "            eval_llm = ChatOllama(\n",
    "                model=\"qwen2.5-coder:7b-instruct\",\n",
    "                temperature=0\n",
    "            )\n",
    "\n",
    "\n",
    "            # 1. Accuracy Score (1-10 ì ìˆ˜)\n",
    "            accuracy_score_evaluator = LangChainStringEvaluator(\n",
    "                \"labeled_score_string\",\n",
    "                config={\n",
    "                    \"criteria\": {\n",
    "                        \"accuracy\": (\n",
    "                            \"How accurate is this prediction compared to the reference \"\n",
    "                            \"on a scale of 1-10? \"\n",
    "                            \"Rate factual correctness only.\"\n",
    "                        )\n",
    "                    },\n",
    "                    \"normalize_by\": 10,                      # 0-1 ì‚¬ì´ë¡œ ì •ê·œí™”\n",
    "                    \"llm\": llm,\n",
    "                },\n",
    "                prepare_data=lambda run, example: {\n",
    "                    \"prediction\": run.outputs[\"answer\"],\n",
    "                    \"reference\": example.outputs[\"answer\"],  # Ground Truth\n",
    "                    \"input\": example.inputs[\"question\"],\n",
    "                },\n",
    "            )\n",
    "\n",
    "            # 2. Completeness Score (1-10 ì ìˆ˜)\n",
    "            completeness_score_evaluator = LangChainStringEvaluator(\n",
    "                \"labeled_score_string\",\n",
    "                config={\n",
    "                    \"criteria\": {\n",
    "                        \"completeness\": (\n",
    "                            \"How complete is this prediction compared to the reference \"\n",
    "                            \"on a scale of 1-10? \"\n",
    "                            \"Does it cover all important information?\"\n",
    "                        )\n",
    "                    },\n",
    "                    \"normalize_by\": 10,\n",
    "                    \"llm\": llm,\n",
    "                },\n",
    "                prepare_data=lambda run, example: {\n",
    "                    \"prediction\": run.outputs[\"answer\"],\n",
    "                    \"reference\": example.outputs[\"answer\"],  # Ground Truth\n",
    "                    \"input\": example.inputs[\"question\"],\n",
    "                },\n",
    "            )\n",
    "\n",
    "            # 3. Context Relevance Score (1-10 ì ìˆ˜)\n",
    "            context_relevance_score_evaluator = LangChainStringEvaluator(\n",
    "                \"labeled_score_string\",\n",
    "                config={\n",
    "                    \"criteria\": {\n",
    "                        \"context_relevance\": (\n",
    "                            \"How well does this prediction use the provided context \"\n",
    "                            \"on a scale of 1-10? \"\n",
    "                            \"Rate how accurately it references the context.\"\n",
    "                        )\n",
    "                    },\n",
    "                    \"normalize_by\": 10,\n",
    "                    \"llm\": eval_llm,                      # í‰ê°€ìš© llmìœ¼ë¡œ ìˆ˜ì •\n",
    "                },\n",
    "                prepare_data=lambda run, example: {\n",
    "                    \"prediction\": run.outputs[\"answer\"],\n",
    "                    \"reference\": run.outputs[\"context\"],  # Context\n",
    "                    \"input\": example.inputs[\"question\"],\n",
    "                },\n",
    "            )\n",
    "\n",
    "            print(\"âœ… í‰ê°€ì ìƒì„± ì™„ë£Œ\\n\")\n",
    "\n",
    "\n",
    "            # ========================================\n",
    "            # í‰ê°€ ì‹¤í–‰\n",
    "            # ========================================\n",
    "\n",
    "            print(\"=\"*50)\n",
    "            print(\"ğŸ“Š í‰ê°€ ì‹¤í–‰...\")\n",
    "            print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "            try:\n",
    "                experiment_results = evaluate(\n",
    "                    context_answer_rag_answer,\n",
    "                    data=\"RAG_EVAL_DATASET\",\n",
    "                    evaluators=[\n",
    "                        accuracy_score_evaluator,\n",
    "                        completeness_score_evaluator,\n",
    "                        context_relevance_score_evaluator,\n",
    "                    ],\n",
    "                    experiment_prefix=\"RAG_EVAL_LABELED_SCORE_LOCAL\",\n",
    "                    metadata={\n",
    "                        \"variant\": \"Labeled Score (Accuracy + Completeness + Context Relevance) - Local\",\n",
    "                        \"k\": 10,\n",
    "                        \"chunk_size\": 300,\n",
    "                        \"llm\": \"qwen2.5:14b-instruct\",\n",
    "                        \"eval_llm\": \"qwen2.5:14b-instruct\",\n",
    "                        \"scoring\": \"1-10 scale, normalized to 0-1\",\n",
    "                    },\n",
    "                    max_concurrency=1,\n",
    "                )\n",
    "                \n",
    "                print(\"\\n\" + \"=\"*50)\n",
    "                print(\"âœ… í‰ê°€ ì™„ë£Œ!\")\n",
    "                print(\"=\"*50)\n",
    "                print(f\"\\nê²°ê³¼: {experiment_results}\\n\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"\\nâŒ ì—ëŸ¬ ë°œìƒ: {e}\\n\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de8c2eb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0459f1",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* **`Local LLM`ìœ¼ë¡œ ì‹œë„**\n",
    "\n",
    "    ```markdown\n",
    "\n",
    "    ğŸš€ Labeled Score í‰ê°€ ì‹œì‘...\n",
    "\n",
    "    âœ… PDF í™•ì¸: /Users/jay/Projects/20250727-langchain-note/15_Evaluations/data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf\n",
    "\n",
    "    âœ… ë¡œì»¬ LLM ìƒì„± ì™„ë£Œ: Qwen2.5-Coder-7B \n",
    "\n",
    "    âœ… PDF ë¡œë“œ ì™„ë£Œ: 23 í˜ì´ì§€\n",
    "    âœ… ì²­í¬ ë¶„í•  ì™„ë£Œ: 119 ì²­í¬ (í¬ê¸°=300, ì˜¤ë²„ë©=50)\n",
    "    âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: BAAI/bge-m3\n",
    "    âœ… ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ: FAISS\n",
    "    âœ… ê²€ìƒ‰ê¸° ìƒì„± ì™„ë£Œ (k=10, search_type=similarity)\n",
    "    âœ… Context í¬í•¨ RAG ì²´ì¸ ìƒì„± ì™„ë£Œ\n",
    "\n",
    "    ==================================================\n",
    "    ğŸ§ª í…ŒìŠ¤íŠ¸ ì‹¤í–‰...\n",
    "    ==================================================\n",
    "\n",
    "    ì§ˆë¬¸: ì‚¼ì„±ì „ìê°€ ìì²´ ê°œë°œí•œ ìƒì„±í˜• AIì˜ ì´ë¦„ì€ ë¬´ì—‡ì¸ê°€ìš”?\n",
    "    Context: SPRi AI Brief |  \n",
    "    2023-12ì›”í˜¸\n",
    "    10\n",
    "    ì‚¼ì„±ì „ì, ìì²´ ê°œë°œ ìƒì„± AI â€˜ì‚¼ì„± ê°€ìš°ìŠ¤â€™ ê³µê°œ\n",
    "    n ì‚¼ì„±ì „ìê°€ ì˜¨ë””ë°”ì´ìŠ¤ì—ì„œ ì‘ë™ ê°€ëŠ¥í•˜ë©° ì–¸ì–´, ì½”ë“œ, ì´ë¯¸ì§€ì˜ 3ê°œ ëª¨ë¸ë¡œ êµ¬ì„±ëœ ìì²´ ê°œë°œ ìƒì„± \n",
    "    AI ëª¨ë¸ â€˜ì‚¼ì„± ê°€ìš°ìŠ¤â€™ë¥¼ ê³µê°œ\n",
    "    n ì‚¼ì„±ì „ìëŠ” ì‚¼ì„± ê°€ìš°ìŠ¤ë¥¼ ë‹¤ì–‘í•œ ì œí’ˆì— ë‹¨ê³„ì ìœ¼ë¡œ íƒ‘ì¬í•  ê³„íšìœ¼ë¡œ, ì˜¨ë””ë°”ì´ìŠ¤ ì‘ë™ì´ ê°€ëŠ¥í•œ \n",
    "    ì‚¼ì„± ê°€ìš°ìŠ¤ëŠ”...\n",
    "    ë‹µë³€: ì‚¼ì„±ì „ìê°€ ìì²´ ê°œë°œí•œ ìƒì„±í˜• AIì˜ ì´ë¦„ì€ 'ì‚¼ì„± ê°€ìš°ìŠ¤'ì…ë‹ˆë‹¤.\n",
    "\n",
    "    ==================================================\n",
    "    ğŸ“Š Labeled Score í‰ê°€ì ìƒì„±...\n",
    "    ==================================================\n",
    "\n",
    "    âœ… í‰ê°€ì ìƒì„± ì™„ë£Œ\n",
    "\n",
    "    ==================================================\n",
    "    ğŸ“Š í‰ê°€ ì‹¤í–‰...\n",
    "    ==================================================\n",
    "    ```\n",
    "    ```bash\n",
    "    View the evaluation results for experiment: 'RAG_EVAL_LABELED_SCORE_LOCAL-c81ff062' at:\n",
    "    https://smith.langchain.com/o/2c3342d3-1170-4ffa-86fd-f621199e0b9c/datasets/420dd308-2ebd-44c9-8ce8-9aff3886dc8e/compare?selectedSessions=e66a2ac2-bbc0-4073-bec2-7a954dec30db\n",
    "    ```\n",
    "    ```markdown\n",
    "\n",
    "    5it [13:15, 159.12s/it]\n",
    "\n",
    "    ==================================================\n",
    "    âœ… í‰ê°€ ì™„ë£Œ!\n",
    "    ==================================================\n",
    "\n",
    "    ê²°ê³¼: <ExperimentResults RAG_EVAL_LABELED_SCORE_LOCAL-c81ff062>\n",
    "\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4912de77",
   "metadata": {},
   "source": [
    "* **`LangSmith`ë¡œ ê²°ê³¼ í™•ì¸í•˜ê¸°**\n",
    "\n",
    "  * ![`Score_EVAL_1`](../15_Evaluations/assets/Score_EVAL_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa2a070",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff26a34",
   "metadata": {},
   "source": [
    "* next: ***`06. ì„ë² ë”© ê¸°ë°˜ í‰ê°€ (embedding_distance)`***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719f0c8c",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc_eval_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
