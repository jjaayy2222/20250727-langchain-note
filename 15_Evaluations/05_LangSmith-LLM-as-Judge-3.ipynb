{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "574f6502",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf366bf9",
   "metadata": {},
   "source": [
    "* 출처: LangChain 공식 문서 또는 해당 교재명\n",
    "* 원본 URL: https://smith.langchain.com/hub/teddynote/summary-stuff-documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31f7552",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af770941",
   "metadata": {},
   "source": [
    "### **5. `05. LLM-as-Judge`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4854f0d6",
   "metadata": {},
   "source": [
    "#### **4) `Context 에 기반한 답변 Evaluator`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276aeabe",
   "metadata": {},
   "source": [
    "* **`LangChainStringEvaluator(\"context_qa\")`**: `LLM` 체인에 정확성을 판단하는 데 참조 **`\"context\"`** 를 사용 → 지시\n",
    "\n",
    "* **`LangChainStringEvaluator(\"cot_qa\")`**: \n",
    "  * `\"cot_qa\"` = `\"context_qa\"` 평가자와 유사\n",
    "  * `but` 최종 판결을 결정하기 전에 `LLM` 의 `'추론'`을 사용하도록 지시한다는 점에서 차이 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14d0dc6",
   "metadata": {},
   "source": [
    "* `Context`를 반환하는 함수 정의하기\n",
    "\n",
    "  * **a. `context_answer_rag_answer()`**\n",
    "\n",
    "  * **b. `LangChainStringEvaluator`** 생성하기 \n",
    "    * **`prepare_data`** 통해 정의한 함수의 반환 값을 적절하게 매핑하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfb895a",
   "metadata": {},
   "source": [
    "* 세부사항\n",
    "\n",
    "  * **`run`**: `LLM`이 생성한 결과\n",
    "    * `context`, `answer`, `input`\n",
    "\n",
    "  * **`example`**: 데이터셋에 정의된 데이터\n",
    "    * `question`, `answer`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a151dec",
   "metadata": {},
   "source": [
    "* **`LangChainStringEvaluator`** 평가 수행하기 위해 필요한 3가지 정보\n",
    "\n",
    "  * **`prediction`**: `LLM`이 생성한 답변\n",
    "\n",
    "  * **`reference`**: 데이터셋에 정의된 답변\n",
    "\n",
    "  * **`input`**: 데이터셋에 정의된 질문"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434b82bb",
   "metadata": {},
   "source": [
    "* **`LangChainStringEvalutor(\"context_qa\")`** = **`reference`** → `Context`로 사용하기 때문\n",
    "\n",
    "  * **`context_qa`** 평가자 활용을 위해 `context`, `answer`, `question`을 반환하는 함수를 정의하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb8bc3e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c0642f",
   "metadata": {},
   "source": [
    "* **`환경 설정`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c3e062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API 키를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API 키 정보 로드\n",
    "load_dotenv()                           # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d33436",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "from langsmith import traceable\n",
    "\n",
    "import os\n",
    "\n",
    "# LangSmith 환경 변수 확인\n",
    "\n",
    "print(\"\\n--- LangSmith 환경 변수 확인 ---\")\n",
    "langchain_tracing_v2 = os.getenv('LANGCHAIN_TRACING_V2')\n",
    "langchain_project = os.getenv('LANGCHAIN_PROJECT')\n",
    "langchain_api_key_status = \"설정됨\" if os.getenv('LANGCHAIN_API_KEY') else \"설정되지 않음\" # API 키 값은 직접 출력하지 않음\n",
    "\n",
    "if langchain_tracing_v2 == \"true\" and os.getenv('LANGCHAIN_API_KEY') and langchain_project:\n",
    "    print(f\"✅ LangSmith 추적 활성화됨 (LANGCHAIN_TRACING_V2='{langchain_tracing_v2}')\")\n",
    "    print(f\"✅ LangSmith 프로젝트: '{langchain_project}'\")\n",
    "    print(f\"✅ LangSmith API Key: {langchain_api_key_status}\")\n",
    "    print(\"  -> 이제 LangSmith 대시보드에서 이 프로젝트를 확인해 보세요.\")\n",
    "else:\n",
    "    print(\"❌ LangSmith 추적이 완전히 활성화되지 않았습니다. 다음을 확인하세요:\")\n",
    "    if langchain_tracing_v2 != \"true\":\n",
    "        print(f\"  - LANGCHAIN_TRACING_V2가 'true'로 설정되어 있지 않습니다 (현재: '{langchain_tracing_v2}').\")\n",
    "    if not os.getenv('LANGCHAIN_API_KEY'):\n",
    "        print(\"  - LANGCHAIN_API_KEY가 설정되어 있지 않습니다.\")\n",
    "    if not langchain_project:\n",
    "        print(\"  - LANGCHAIN_PROJECT가 설정되어 있지 않습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3cee3b",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 셀 출력\n",
    "\n",
    "    ```bash\n",
    "    --- LangSmith 환경 변수 확인 ---\n",
    "    ✅ LangSmith 추적 활성화됨 (LANGCHAIN_TRACING_V2='true')\n",
    "    ✅ LangSmith 프로젝트: 'LangChain-prantice'\n",
    "    ✅ LangSmith API Key: 설정됨\n",
    "    -> 이제 LangSmith 대시보드에서 이 프로젝트를 확인해 보세요.\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fc4a4f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44bf615",
   "metadata": {},
   "source": [
    "* **`잦은 커널 충돌` + `의존성 패키지 파일 충돌` → `Python`파일로 만들어 실행**\n",
    "\n",
    "  * [**`myrag5.py`**](../15_Evaluations/myrag5.py)\n",
    "\n",
    "  * [**`eval_context.py`**](../15_Evaluations/eval_context.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cfe99f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d103de62",
   "metadata": {},
   "source": [
    "##### **`➀ myrag5.py`**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 코드 내용 \n",
    "\n",
    "<small> <small>\n",
    "\n",
    "```python\n",
    "\n",
    "        # myrag5.py\n",
    "\n",
    "        from langchain_community.document_loaders import PyMuPDFLoader\n",
    "        from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "        from langchain_community.vectorstores import FAISS\n",
    "        from langchain_core.output_parsers import StrOutputParser\n",
    "        from langchain_core.runnables import RunnablePassthrough\n",
    "        from langchain_core.prompts import PromptTemplate\n",
    "        from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "        class PDFRAG:\n",
    "            def __init__(self, pdf_path, llm, chunk_size=300, chunk_overlap=50):\n",
    "                \"\"\"\n",
    "                PDF RAG 시스템 초기화\n",
    "                \n",
    "                Parameters\n",
    "                ----------\n",
    "                pdf_path : str\n",
    "                    PDF 파일 경로\n",
    "                llm : ChatOpenAI\n",
    "                    사용할 LLM\n",
    "                chunk_size : int\n",
    "                    청크 크기 (기본값: 300)\n",
    "                chunk_overlap : int\n",
    "                    청크 오버랩 (기본값: 50)\n",
    "                \"\"\"\n",
    "                self.pdf_path = pdf_path\n",
    "                self.llm = llm\n",
    "                self.chunk_size = chunk_size\n",
    "                self.chunk_overlap = chunk_overlap\n",
    "                self.documents = None\n",
    "                self.vectorstore = None\n",
    "                self.retriever = None\n",
    "                \n",
    "                # 초기화\n",
    "                self._load_and_process()\n",
    "            \n",
    "            def _load_and_process(self):\n",
    "                \"\"\"PDF 로드 및 처리\"\"\"\n",
    "                # 1. PDF 로드\n",
    "                loader = PyMuPDFLoader(self.pdf_path)\n",
    "                docs = loader.load()\n",
    "                print(f\"✅ PDF 로드 완료: {len(docs)} 페이지\")\n",
    "                \n",
    "                # 2. 청크 분할\n",
    "                text_splitter = RecursiveCharacterTextSplitter(\n",
    "                    chunk_size=self.chunk_size,\n",
    "                    chunk_overlap=self.chunk_overlap\n",
    "                )\n",
    "                self.documents = text_splitter.split_documents(docs)\n",
    "                print(f\"✅ 청크 분할 완료: {len(self.documents)} 청크 (크기={self.chunk_size}, 오버랩={self.chunk_overlap})\")\n",
    "                \n",
    "                # 3. 임베딩\n",
    "                embeddings = HuggingFaceEmbeddings(\n",
    "                    model_name=\"BAAI/bge-m3\",                                  # 임베딩 모델 교체\n",
    "                    model_kwargs={\"device\": \"cpu\"},\n",
    "                    encode_kwargs={\"normalize_embeddings\": True},\n",
    "                )\n",
    "                print(f\"✅ 임베딩 모델 로드 완료: {embeddings.model_name}\")\n",
    "                \n",
    "                # 4. 벡터스토어 생성\n",
    "                self.vectorstore = FAISS.from_documents(\n",
    "                    documents=self.documents,\n",
    "                    embedding=embeddings\n",
    "                )\n",
    "                print(f\"✅ 벡터스토어 생성 완료: FAISS\")\n",
    "            \n",
    "            def create_retriever(self, k=10, search_type=\"similarity\"):         # k=10으로 증가\n",
    "                \"\"\"\n",
    "                검색기 생성\n",
    "                \n",
    "                Parameters\n",
    "                ----------\n",
    "                k : int, optional\n",
    "                    검색할 문서 개수 (기본값: 10)\n",
    "                search_type : str, optional\n",
    "                    검색 타입 (기본값: similarity)\n",
    "                \n",
    "                Returns\n",
    "                -------\n",
    "                retriever\n",
    "                    문서 검색기\n",
    "                \"\"\"\n",
    "                self.retriever = self.vectorstore.as_retriever(\n",
    "                    search_type=search_type,\n",
    "                    search_kwargs={\"k\": k}\n",
    "                )\n",
    "                print(f\"✅ 검색기 생성 완료 (k={k}, search_type={search_type})\")\n",
    "                return self.retriever\n",
    "            \n",
    "            def create_chain(self, retriever):\n",
    "                \"\"\"\n",
    "                RAG 체인 생성\n",
    "                \n",
    "                Parameters\n",
    "                ----------\n",
    "                retriever\n",
    "                    문서 검색기\n",
    "                \n",
    "                Returns\n",
    "                -------\n",
    "                chain\n",
    "                    RAG 체인\n",
    "                \"\"\"\n",
    "                # 프롬프트 템플릿\n",
    "                prompt = PromptTemplate.from_template(\n",
    "                    \"\"\"당신은 질문에 답변하는 AI 어시스턴트입니다.\n",
    "\n",
    "                주어진 Context를 **반드시** 참고하여 정확하게 답변하세요.\n",
    "                Context에 정보가 없으면 \"죄송합니다. 제공된 문서에서 해당 정보를 찾을 수 없습니다.\"라고 답변하세요.\n",
    "\n",
    "                Context:\n",
    "                {context}\n",
    "\n",
    "                Question: {question}\n",
    "\n",
    "                Answer:\"\"\"\n",
    "                )\n",
    "                \n",
    "                # 체인 생성\n",
    "                chain = (\n",
    "                    {\n",
    "                        \"context\": retriever,\n",
    "                        \"question\": RunnablePassthrough(),\n",
    "                    }\n",
    "                    | prompt\n",
    "                    | self.llm\n",
    "                    | StrOutputParser()\n",
    "                )\n",
    "                \n",
    "                print(f\"✅ RAG 체인 생성 완료\")\n",
    "                return chain\n",
    "\n",
    "        # ========================================\n",
    "        # Question-Answer Evaluator 추가\n",
    "        # ========================================\n",
    "            \n",
    "            def create_chain_with_context(self, retriever):\n",
    "                \"\"\"\n",
    "                Context를 포함하여 반환하는 체인 생성\n",
    "                \n",
    "                Returns\n",
    "                -------\n",
    "                chain\n",
    "                    Context와 Answer를 함께 반환하는 체인\n",
    "                \"\"\"\n",
    "                from langchain_core.runnables import RunnablePassthrough\n",
    "                \n",
    "                # Context 추출 함수\n",
    "                def format_docs(docs):\n",
    "                    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "                \n",
    "                # 프롬프트는 기존과 동일\n",
    "                prompt = PromptTemplate.from_template(\n",
    "                    \"\"\"당신은 질문에 답변하는 AI 어시스턴트입니다.\n",
    "\n",
    "                주어진 Context를 **반드시** 참고하여 정확하게 답변하세요.\n",
    "                Context에 정보가 없으면 \"죄송합니다. 제공된 문서에서 해당 정보를 찾을 수 없습니다.\"라고 답변하세요.\n",
    "\n",
    "                Context:\n",
    "                {context}\n",
    "\n",
    "                Question: {question}\n",
    "\n",
    "                Answer:\"\"\"\n",
    "                )\n",
    "                \n",
    "                # Context와 Answer를 함께 반환하는 체인\n",
    "                chain = (\n",
    "                    {\n",
    "                        \"context\": retriever | format_docs,\n",
    "                        \"question\": RunnablePassthrough(),\n",
    "                    }\n",
    "                    | RunnablePassthrough.assign(\n",
    "                        answer=prompt | self.llm | StrOutputParser()\n",
    "                    )\n",
    "                )\n",
    "                \n",
    "                print(f\"✅ Context 포함 RAG 체인 생성 완료\")\n",
    "                return chain\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8812d1",
   "metadata": {},
   "source": [
    "##### **`➁ eval_context.py` → 실행하기**\n",
    "\n",
    "* 코드 내용 \n",
    "\n",
    "<small> <small>\n",
    "\n",
    "```python\n",
    "\n",
    "            # eval_script.py\n",
    "\n",
    "            from dotenv import load_dotenv\n",
    "            load_dotenv()\n",
    "\n",
    "            import os\n",
    "            from myrag5 import PDFRAG\n",
    "            from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "            from langsmith.evaluation import evaluate, LangChainStringEvaluator\n",
    "\n",
    "            print(\"🚀 Context 기반 평가 시작...\\n\")\n",
    "\n",
    "            # 현재 스크립트 위치 확인\n",
    "            script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "            print(f\"📂 스크립트 위치: {script_dir}\\n\")\n",
    "\n",
    "            # PDF 파일 절대 경로 생성\n",
    "            pdf_path = os.path.join(script_dir, \"data\", \"SPRI_AI_Brief_2023년12월호_F.pdf\")\n",
    "            print(f\"📄 PDF 경로: {pdf_path}\")\n",
    "\n",
    "            # 파일 존재 확인\n",
    "            if not os.path.exists(pdf_path):\n",
    "                print(f\"❌ 파일이 존재하지 않습니다!\")\n",
    "                print(f\"❌ 경로: {pdf_path}\\n\")\n",
    "                exit(1)\n",
    "                \n",
    "            print(f\"✅ PDF 확인: {pdf_path}\\n\")\n",
    "                \n",
    "\n",
    "            # LLM 생성\n",
    "            llm = ChatGoogleGenerativeAI(\n",
    "                model=\"gemini-2.5-flash-lite\",\n",
    "                temperature=0\n",
    "            )\n",
    "            print(\"✅ LLM 생성 완료: gemini-2.5-flash-lite\\n\")\n",
    "\n",
    "            # RAG 생성\n",
    "            rag = PDFRAG(\n",
    "                pdf_path,  # 절대 경로 사용!\n",
    "                llm,\n",
    "                chunk_size=300,\n",
    "                chunk_overlap=50,\n",
    "            )\n",
    "\n",
    "            # Retriever & Chain (k 늘리기)\n",
    "            retriever = rag.create_retriever(k=10)\n",
    "\n",
    "            # Context 포함 체인 생성\n",
    "            chain_with_context = rag.create_chain_with_context(retriever)\n",
    "\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(\"🧪 테스트 실행...\")\n",
    "            print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "            # 테스트\n",
    "            test_question = \"삼성전자가 자체 개발한 생성형 AI의 이름은 무엇인가요?\"\n",
    "            test_result = chain_with_context.invoke(test_question)\n",
    "            print(f\"질문: {test_question}\")\n",
    "            print(f\"Context: {test_result['context'][:200]}...\")\n",
    "            print(f\"답변: {test_result['answer']}\\n\")\n",
    "\n",
    "            # ========================================\n",
    "            # Context 기반 평가 함수\n",
    "            # ========================================\n",
    "\n",
    "            def context_answer_rag_answer(inputs: dict):\n",
    "                \"\"\"\n",
    "                Context와 Answer를 함께 반환\n",
    "                \n",
    "                Returns\n",
    "                -------\n",
    "                dict\n",
    "                    - context: 검색된 Context\n",
    "                    - answer: LLM 답변\n",
    "                    - query: 질문\n",
    "                \"\"\"\n",
    "                result = chain_with_context.invoke(inputs[\"question\"])\n",
    "                return {\n",
    "                    \"context\": result[\"context\"],\n",
    "                    \"answer\": result[\"answer\"],\n",
    "                    \"query\": inputs[\"question\"],\n",
    "                }\n",
    "\n",
    "            # ========================================\n",
    "            # 평가자 생성\n",
    "            # ========================================\n",
    "\n",
    "            print(\"=\"*50)\n",
    "            print(\"📊 평가자 생성...\")\n",
    "            print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "            # 1. COT_QA Evaluator (Chain-of-Thought)\n",
    "            cot_qa_evaluator = LangChainStringEvaluator(\n",
    "                \"cot_qa\",\n",
    "                config={\"llm\": llm},\n",
    "                prepare_data=lambda run, example: {\n",
    "                    \"prediction\": run.outputs[\"answer\"],      # LLM 답변\n",
    "                    \"reference\": run.outputs[\"context\"],      # Context\n",
    "                    \"input\": example.inputs[\"question\"],      # 질문\n",
    "                },\n",
    "            )\n",
    "\n",
    "            # 2. Context_QA Evaluator\n",
    "            context_qa_evaluator = LangChainStringEvaluator(\n",
    "                \"context_qa\",\n",
    "                config={\"llm\": llm},\n",
    "                prepare_data=lambda run, example: {\n",
    "                    \"prediction\": run.outputs[\"answer\"],      # LLM 답변\n",
    "                    \"reference\": run.outputs[\"context\"],      # Context\n",
    "                    \"input\": example.inputs[\"question\"],      # 질문\n",
    "                },\n",
    "            )\n",
    "\n",
    "            print(\"✅ 평가자 생성 완료\\n\")\n",
    "\n",
    "            # ========================================\n",
    "            # 평가 실행\n",
    "            # ========================================\n",
    "\n",
    "            print(\"=\"*50)\n",
    "            print(\"📊 평가 실행...\")\n",
    "            print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "            try:\n",
    "                experiment_results = evaluate(\n",
    "                    context_answer_rag_answer,\n",
    "                    data=\"RAG_EVAL_DATASET\",\n",
    "                    evaluators=[cot_qa_evaluator, context_qa_evaluator],\n",
    "                    experiment_prefix=\"RAG_EVAL_CONTEXT\",\n",
    "                    metadata={\n",
    "                        \"variant\": \"Context 기반 평가 (COT_QA + Context_QA)\",\n",
    "                        \"k\": 7,\n",
    "                        \"chunk_size\": 300,\n",
    "                    },\n",
    "                    max_concurrency=1,\n",
    "                )\n",
    "                \n",
    "                print(\"\\n\" + \"=\"*50)\n",
    "                print(\"✅ 평가 완료!\")\n",
    "                print(\"=\"*50)\n",
    "                print(f\"\\n결과: {experiment_results}\\n\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\n❌ 에러 발생: {e}\\n\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **`RAG 기반 평가` vs `Context 기반 평가`**\n",
    "\n",
    "  * 비교 \n",
    "\n",
    "  | 평가 방식 | 설명 | 장점 | 단점 |\n",
    "  |----------|------|------|------|\n",
    "  | **QA** (기존) | Ground Truth 비교 | 정확도 명확 | 답변 형식에 민감 |\n",
    "  | **Context_QA** (새로운) | Context 기반 평가 | 유연한 평가 | Ground Truth 무시 |\n",
    "\n",
    "<br>\n",
    "\n",
    "* **`Context 기반 평가`의 `장점`**\n",
    "\n",
    "  * Ground Truth 무시\n",
    "  * **`Context` 기반 평가**\n",
    "  * **더 유연한 평가**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d2fc3b",
   "metadata": {},
   "source": [
    "#### **5) `Criteria`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d186f4",
   "metadata": {},
   "source": [
    "* 기준값 참조 레이블(정답 답변)이 없거나 얻기 힘든 경우 \n",
    "\n",
    "* **`criteria`** or **`score`** 평가자 사용 → **`사용자 지정 기준 집합`에 대해 실행 평가 가능**\n",
    "\n",
    "* **`모델의 답변에 대한 높은 수준의 의미론적 측면을 모니터링 하려는 경우에 유용`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d211a2d5",
   "metadata": {},
   "source": [
    "* LangChainStringEvaluator\n",
    "    (\"criteria\", \n",
    "     config={ \"criteria\": *아래 중 하나의 criterion* })\n",
    "\n",
    "* *`criteria`*\n",
    "\n",
    "  | 기준               | 설명                   |\n",
    "  |------------------|----------------------|\n",
    "  | conciseness      | 답변이 간결하고 간단한지 평가     |\n",
    "  | relevance        | 답변이 질문과 관련이 있는지 평가   |\n",
    "  | correctness      | 답변이 옳은지 평가           |\n",
    "  | coherence        | 답변이 일관성이 있는지 평가      |\n",
    "  | harmfulness      | 답변이 해롭거나 유해한지 평가     |\n",
    "  | maliciousness    | 답변이 악의적이거나 악화시키는지 평가 |\n",
    "  | helpfulness      | 답변이 도움이 되는지 평가       |\n",
    "  | controversiality | 답변이 논란이 되는지 평가       |\n",
    "  | misogyny         | 답변이 여성을 비하하는지 평가     |\n",
    "  | criminality      | 답변이 범죄를 촉진하는지 평가     |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d3c9f1",
   "metadata": {},
   "source": [
    "* 코드 예시\n",
    "\n",
    "```python\n",
    "\n",
    "    from langsmith.evaluation import evaluate, LangChainStringEvaluator\n",
    "\n",
    "    # 평가자 설정\n",
    "    criteria_evaluator = [\n",
    "        LangChainStringEvaluator(\"criteria\", config={\"criteria\": \"conciseness\"}),\n",
    "        LangChainStringEvaluator(\"criteria\", config={\"criteria\": \"misogyny\"}),\n",
    "        LangChainStringEvaluator(\"criteria\", config={\"criteria\": \"criminality\"}),\n",
    "    ]\n",
    "\n",
    "    # 데이터셋 이름 설정\n",
    "    dataset_name = \"RAG_EVAL_DATASET\"\n",
    "\n",
    "    # 평가 실행\n",
    "    experiment_results = evaluate(\n",
    "        ask_question,\n",
    "        data=dataset_name,\n",
    "        evaluators=criteria_evaluator,\n",
    "        experiment_prefix=\"CRITERIA-EVAL\",\n",
    "        # 실험 메타데이터 지정\n",
    "        metadata={\n",
    "            \"variant\": \"criteria 를 활용한 평가\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "* `LangChain`에서 확인하기\n",
    "\n",
    "  * ![`LangChain-conciseness`](../15_Evaluations/assets/output-03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1815cff6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32111921",
   "metadata": {},
   "source": [
    "##### **`➀ 정답이 존재하는 경우 Evaluator 활용`** (*labeled_criteria*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4429c53c",
   "metadata": {},
   "source": [
    "* 정답이 존재하는 경우: **`LLM` 이 `생성한 답변`과 `정답 답변`을 `비교` → 평가 가능**\n",
    "\n",
    "  * **`reference` = `정답 답변`**\n",
    "  * **`prediction` = `LLM`이 `생성한 답변 전달`**\n",
    "\n",
    "  * **`별도의 설정` → `prepare_data` → 정의**\n",
    "\n",
    "  * **`config`의 `llm` → `답변 평가`에 활용되는 LLM으로 정의**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01edba6",
   "metadata": {},
   "source": [
    "##### **↳ `eval_labeled_criteria.py`**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f018fb37",
   "metadata": {},
   "source": [
    "* 코드 내용 \n",
    "\n",
    "<small> <small>\n",
    "\n",
    "```python\n",
    "\n",
    "            # ========================================\n",
    "            # eval_labeled_criteria.py\n",
    "            # 정답 기반 Criteria 평가\n",
    "            # ========================================\n",
    "\n",
    "            from dotenv import load_dotenv\n",
    "            load_dotenv()\n",
    "\n",
    "            import os\n",
    "            from myrag5 import PDFRAG\n",
    "            from langchain_ollama import ChatOllama             # Ollama 사용\n",
    "            from langsmith.evaluation import evaluate, LangChainStringEvaluator\n",
    "            import warnings\n",
    "            warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "            print(\"🚀 Labeled Criteria 평가 시작...\\n\")\n",
    "\n",
    "\n",
    "            # ========================================\n",
    "            # 파일 경로 & LLM 설정\n",
    "            # ========================================\n",
    "\n",
    "            script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "            pdf_path = os.path.join(script_dir, \"data\", \"SPRI_AI_Brief_2023년12월호_F.pdf\")\n",
    "\n",
    "            if not os.path.exists(pdf_path):\n",
    "                print(f\"❌ 파일 없음: {pdf_path}\")\n",
    "                exit(1)\n",
    "\n",
    "            print(f\"✅ PDF 확인: {pdf_path}\\n\")\n",
    "\n",
    "            # LLM 생성\n",
    "\n",
    "            llm = ChatOllama(\n",
    "                model=\"qwen2.5-coder:7b-instruct\",\n",
    "                temperature=0\n",
    "            )\n",
    "\n",
    "            print(\"✅ 로컬 LLM 생성 완료: {llm.model}\\n\")\n",
    "\n",
    "            # ========================================\n",
    "            # RAG 시스템 생성\n",
    "            # ========================================\n",
    "\n",
    "            rag = PDFRAG(pdf_path, llm, chunk_size=300, chunk_overlap=50)\n",
    "            retriever = rag.create_retriever(k=10, search_type=\"similarity\")\n",
    "\n",
    "            # Context 포함 체인\n",
    "            chain_with_context = rag.create_chain_with_context(retriever)\n",
    "\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(\"🧪 테스트 실행...\")\n",
    "            print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "            # 테스트\n",
    "            test_question = \"삼성전자가 자체 개발한 생성형 AI의 이름은 무엇인가요?\"\n",
    "\n",
    "            try:\n",
    "                test_result = chain_with_context.invoke(test_question)\n",
    "                print(f\"질문: {test_question}\")\n",
    "                print(f\"Context: {test_result['context'][:100]}...\")\n",
    "                print(f\"답변: {test_result['answer']}\\n\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ 테스트 실패: {e}\\n\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                exit(1)\n",
    "\n",
    "\n",
    "            # ========================================\n",
    "            # Context + Answer 반환 함수\n",
    "            # ========================================\n",
    "\n",
    "            def context_answer_rag_answer(inputs: dict):\n",
    "                \"\"\"\n",
    "                Context와 Answer를 함께 반환\n",
    "                \"\"\"\n",
    "                result = chain_with_context.invoke(inputs[\"question\"])\n",
    "                return {\n",
    "                    \"context\": result[\"context\"],\n",
    "                    \"answer\": result[\"answer\"],\n",
    "                    \"query\": inputs[\"question\"],\n",
    "                }\n",
    "\n",
    "            # ========================================\n",
    "            # Labeled Criteria 평가자 생성\n",
    "            # ========================================\n",
    "\n",
    "            print(\"=\"*50)\n",
    "            print(\"📊 Labeled Criteria 평가자 생성...\")\n",
    "            print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "            # 평가용 LLM \n",
    "            eval_llm = ChatOllama(\n",
    "                model=\"qwen2.5-coder:7b-instruct\",\n",
    "                temperature=0\n",
    "            )\n",
    "\n",
    "            # 1. Helpfulness Evaluator (Ground Truth 기반)\n",
    "            helpfulness_evaluator = LangChainStringEvaluator(\n",
    "                \"labeled_criteria\",\n",
    "                config={\n",
    "                    \"criteria\": {\n",
    "                        \"helpfulness\": (\n",
    "                            \"Is this submission helpful to the user, \"\n",
    "                            \"taking into account the correct reference answer?\"\n",
    "                        )\n",
    "                    },\n",
    "                    \"llm\": llm,\n",
    "                },\n",
    "                prepare_data=lambda run, example: {\n",
    "                    \"prediction\": run.outputs[\"answer\"],           # LLM 답변\n",
    "                    \"reference\": example.outputs[\"answer\"],        # Ground Truth\n",
    "                    \"input\": example.inputs[\"question\"],           # 질문\n",
    "                },\n",
    "            )\n",
    "\n",
    "            # 2. Relevance Evaluator (Context 기반)\n",
    "            relevance_evaluator = LangChainStringEvaluator(\n",
    "                \"labeled_criteria\",\n",
    "                config={\n",
    "                    \"criteria\": \"relevance\",                # 답변이 Context를 참조하는가?\n",
    "                    \"llm\": llm,\n",
    "                },\n",
    "                prepare_data=lambda run, example: {\n",
    "                    \"prediction\": run.outputs[\"answer\"],           # LLM 답변\n",
    "                    \"reference\": run.outputs[\"context\"],           # Context\n",
    "                    \"input\": example.inputs[\"question\"],           # 질문\n",
    "                },\n",
    "            )\n",
    "\n",
    "            # 3. Accuracy Evaluator (Ground Truth 기반)\n",
    "            accuracy_evaluator = LangChainStringEvaluator(\n",
    "                \"labeled_criteria\",\n",
    "                config={\n",
    "                    \"criteria\": {\n",
    "                        \"accuracy\": (\n",
    "                            \"Is this submission factually accurate \"\n",
    "                            \"compared to the reference answer?\"\n",
    "                        )\n",
    "                    },\n",
    "                    \"llm\": llm,\n",
    "                },\n",
    "                prepare_data=lambda run, example: {\n",
    "                    \"prediction\": run.outputs[\"answer\"],           # LLM 답변\n",
    "                    \"reference\": example.outputs[\"answer\"],        # Ground Truth\n",
    "                    \"input\": example.inputs[\"question\"],           # 질문\n",
    "                },\n",
    "            )\n",
    "\n",
    "            print(\"✅ 평가자 생성 완료\\n\")\n",
    "\n",
    "            # ========================================\n",
    "            # 평가 실행\n",
    "            # ========================================\n",
    "\n",
    "            print(\"=\"*50)\n",
    "            print(\"📊 평가 실행...\")\n",
    "            print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "            try:\n",
    "                experiment_results = evaluate(\n",
    "                    context_answer_rag_answer,\n",
    "                    data=\"RAG_EVAL_DATASET\",\n",
    "                    evaluators=[\n",
    "                        helpfulness_evaluator,\n",
    "                        relevance_evaluator,\n",
    "                        accuracy_evaluator,\n",
    "                    ],\n",
    "                    experiment_prefix=\"RAG_EVAL_LABELED_CRITERIA_LOCAL\",\n",
    "                    metadata={\n",
    "                        \"variant\": \"Labeled Criteria (Helpfulness + Relevance + Accuracy) - Local\",\n",
    "                        \"k\": 10,\n",
    "                        \"chunk_size\": 300,\n",
    "                        \"llm\": \"qwen2.5-coder:7b-instruct\",\n",
    "                    },\n",
    "                    max_concurrency=1,\n",
    "                )\n",
    "                \n",
    "                print(\"\\n\" + \"=\"*50)\n",
    "                print(\"✅ 평가 완료!\")\n",
    "                print(\"=\"*50)\n",
    "                print(f\"\\n결과: {experiment_results}\\n\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\n❌ 에러 발생: {e}\\n\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f232c5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3c67da",
   "metadata": {},
   "source": [
    "* **`첫 번째 시도`**\n",
    "\n",
    "  * *API 할당량 부족으로 제대로 된 결과가 나오지 않음*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dc1bee",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 결과\n",
    "\n",
    "  ```markdown\n",
    "\n",
    "  🚀 Labeled Criteria 평가 시작...\n",
    "\n",
    "  ✅ PDF 확인: /Users/jay/Projects/20250727-langchain-note/15_Evaluations/data/SPRI_AI_Brief_2023년12월호_F.pdf\n",
    "\n",
    "  ✅ LLM 생성 완료\n",
    "\n",
    "  ✅ PDF 로드 완료: 23 페이지\n",
    "  ✅ 청크 분할 완료: 119 청크 (크기=300, 오버랩=50)\n",
    "  ✅ 임베딩 모델 로드 완료: BAAI/bge-m3\n",
    "  ✅ 벡터스토어 생성 완료: FAISS\n",
    "  ✅ 검색기 생성 완료 (k=10, search_type=similarity)\n",
    "  ✅ Context 포함 RAG 체인 생성 완료\n",
    "\n",
    "  ==================================================\n",
    "  🧪 테스트 실행...\n",
    "  ==================================================\n",
    "\n",
    "  질문: 삼성전자가 자체 개발한 생성형 AI의 이름은 무엇인가요?\n",
    "  Context: SPRi AI Brief |  \n",
    "  2023-12월호\n",
    "  10\n",
    "  삼성전자, 자체 개발 생성 AI ‘삼성 가우스’ 공개\n",
    "  n 삼성전자가 온디바이스에서 작동 가능하며 언어, 코드, 이미지의 3개 모델로 구성된 자체 개발 생성 \n",
    "  AI 모델 ‘삼성 가우스’를 공개\n",
    "  n 삼성전자는 삼성 가우스를 다양한 제품에 단계적으로 탑재할 계획으로, 온디바이스 작동이 가능한 \n",
    "  삼성 가우스는...\n",
    "  답변: 삼성전자가 자체 개발한 생성형 AI의 이름은 '삼성 가우스'입니다.\n",
    "\n",
    "  ==================================================\n",
    "  📊 Labeled Criteria 평가자 생성...\n",
    "  ==================================================\n",
    "\n",
    "  ✅ 평가자 생성 완료\n",
    "\n",
    "  ==================================================\n",
    "  📊 평가 실행...\n",
    "  ==================================================\n",
    "\n",
    "  4/5\n",
    "\n",
    "  ```\n",
    "\n",
    "<br>\n",
    "\n",
    "* `LangSmith`에서 확인하기\n",
    "\n",
    "  * ![api할당량부족으로4/5만완성](../15_Evaluations/assets/Criteria_EVAL_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9df4ff",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95717109",
   "metadata": {},
   "source": [
    "* **`두번째 시도`**\n",
    "\n",
    "  * *여전히 API 할당량이 많이 소요*\n",
    "\n",
    "  * `Local LLM` 설치 → **`Qwen2.5-Coder-7B`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0ccd4d",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* **`eval_labeled_criteria.py`** \n",
    "\n",
    "    ```markdown\n",
    "\n",
    "    🚀 Labeled Criteria 평가 시작...\n",
    "\n",
    "    ✅ PDF 확인: /Users/jay/Projects/20250727-langchain-note/15_Evaluations/data/SPRI_AI_Brief_2023년12월호_F.pdf\n",
    "\n",
    "    ✅ 로컬 LLM 생성 완료: Qwen2.5-Coder-7B\n",
    "\n",
    "    ✅ PDF 로드 완료: 23 페이지\n",
    "    ✅ 청크 분할 완료: 119 청크 (크기=300, 오버랩=50)\n",
    "    ✅ 임베딩 모델 로드 완료: BAAI/bge-m3\n",
    "    ✅ 벡터스토어 생성 완료: FAISS\n",
    "    ✅ 검색기 생성 완료 (k=10, search_type=similarity)\n",
    "    ✅ Context 포함 RAG 체인 생성 완료\n",
    "\n",
    "    ==================================================\n",
    "    🧪 테스트 실행...\n",
    "    ==================================================\n",
    "\n",
    "    질문: 삼성전자가 자체 개발한 생성형 AI의 이름은 무엇인가요?\n",
    "    Context: SPRi AI Brief |  \n",
    "    2023-12월호\n",
    "    10\n",
    "    삼성전자, 자체 개발 생성 AI ‘삼성 가우스’ 공개\n",
    "    n 삼성전자가 온디바이스에서 작동 가능하며 언어, 코드, 이미지의 3개...\n",
    "    답변: 삼성전자가 자체 개발한 생성형 AI의 이름은 '삼성 가우스'입니다.\n",
    "\n",
    "    ==================================================\n",
    "    📊 Labeled Criteria 평가자 생성...\n",
    "    ==================================================\n",
    "\n",
    "    ✅ 평가자 생성 완료\n",
    "\n",
    "    ==================================================\n",
    "    📊 평가 실행...\n",
    "    ==================================================\n",
    "\n",
    "    5it [23:45, 285.13s/it]\n",
    "\n",
    "    ==================================================\n",
    "    ✅ 평가 완료!\n",
    "    ==================================================\n",
    "\n",
    "    결과: <ExperimentResults RAG_EVAL_LABELED_CRITERIA_LOCAL-d15e33df>\n",
    "\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e8763a",
   "metadata": {},
   "source": [
    "* **`LangSmith`에서 결과 확인해보기**\n",
    "\n",
    "  * ![`Criteria_EVAL2`](../15_Evaluations/assets/Criteria_EVAL_2.png)\n",
    "\n",
    "  * *글씨로 표기된 부분*: `**y**` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee3c539",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e193eb46",
   "metadata": {},
   "source": [
    "* **`결과 비교`**\n",
    "\n",
    "|              | try 1                  | try 2                 |\n",
    "|--------------|------------------------|-------------------------|\n",
    "| 평가 결과        | **`4`/5**                    | **`5/5`**          |\n",
    "| LangSmith 확인 | ![결과1](../15_Evaluations/assets/Criteria_EVAL_1.png)                     | ![결과2](../15_Evaluations/assets/Criteria_EVAL_2.png)           |\n",
    "| 차이점          | `gemini-2.5-flash` 모델 사용 | `로컬 LLM` 모델 사용 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fe3632",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c0942b",
   "metadata": {},
   "source": [
    "##### **`➁ 사용자 정의 점수 Evaluator`** (*labeled_score_string*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290efb54",
   "metadata": {},
   "source": [
    "* 점수를 반환하는 평가자 생성 예시\n",
    "\n",
    "  * **`normalize_by` → 점수 정규화 가능**\n",
    "\n",
    "  * **`변환된 점수` = `0 ~ 1` 사이의 값으로 `정규화`**\n",
    "\n",
    "  * **코드 예시 속 `accuracy` = `사용자가 임의로 정의한 기준`**\n",
    "    * ∟→ **`적합한 Prompt`를 `정의`하여 `사용` 가능**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3626e0c0",
   "metadata": {},
   "source": [
    "##### **↳ `eval_labeled_score.py`**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1156fe",
   "metadata": {},
   "source": [
    "* 코드 내용\n",
    "\n",
    "<small> <small>\n",
    "\n",
    "```python\n",
    "\n",
    "            # ========================================\n",
    "            # eval_labeled_score.py (복붙!)\n",
    "            # 점수 기반 평가\n",
    "            # ========================================\n",
    "\n",
    "            from dotenv import load_dotenv\n",
    "            load_dotenv()\n",
    "\n",
    "            import os\n",
    "            from myrag5 import PDFRAG\n",
    "            from langchain_ollama import ChatOllama             # Ollama 사용\n",
    "            from langsmith.evaluation import evaluate, LangChainStringEvaluator\n",
    "            import warnings\n",
    "            warnings.filterwarnings(\"ignore\")\n",
    "            os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"           # Tokenizer Parallelism 출력 경고 메시지 억제\n",
    "\n",
    "            print(\"🚀 Labeled Score 평가 시작...\\n\")\n",
    "\n",
    "\n",
    "            # ========================================\n",
    "            # 파일 경로 & LLM 설정\n",
    "            # ========================================\n",
    "\n",
    "            script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "            pdf_path = os.path.join(script_dir, \"data\", \"SPRI_AI_Brief_2023년12월호_F.pdf\")\n",
    "\n",
    "            if not os.path.exists(pdf_path):\n",
    "                print(f\"❌ 파일 없음: {pdf_path}\")\n",
    "                exit(1)\n",
    "\n",
    "            print(f\"✅ PDF 확인: {pdf_path}\\n\")\n",
    "\n",
    "            # LLM 생성\n",
    "\n",
    "            llm = ChatOllama(\n",
    "                model=\"qwen2.5-coder:7b-instruct\",\n",
    "                temperature=0\n",
    "            )\n",
    "\n",
    "            print(\"✅ 로컬 LLM 생성 완료: Qwen2.5-Coder-7B \\n\")\n",
    "\n",
    "\n",
    "            # ========================================\n",
    "            # RAG 시스템 생성\n",
    "            # ========================================\n",
    "\n",
    "            rag = PDFRAG(pdf_path, llm, chunk_size=300, chunk_overlap=50)\n",
    "            retriever = rag.create_retriever(k=10, search_type=\"similarity\")\n",
    "\n",
    "            # Context 포함 체인\n",
    "            chain_with_context = rag.create_chain_with_context(retriever)\n",
    "\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(\"🧪 테스트 실행...\")\n",
    "            print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "            # 테스트\n",
    "            test_question = \"삼성전자가 자체 개발한 생성형 AI의 이름은 무엇인가요?\"\n",
    "\n",
    "            try:\n",
    "                test_result = chain_with_context.invoke(test_question)\n",
    "                print(f\"질문: {test_question}\")\n",
    "                print(f\"Context: {test_result['context'][:200]}...\")\n",
    "                print(f\"답변: {test_result['answer']}\\n\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ 테스트 실패: {e}\\n\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                exit(1)\n",
    "\n",
    "\n",
    "            # ========================================\n",
    "            # Context + Answer 반환 함수\n",
    "            # ========================================\n",
    "\n",
    "            def context_answer_rag_answer(inputs: dict):\n",
    "                \"\"\"\n",
    "                Context와 Answer를 함께 반환\n",
    "                \"\"\"\n",
    "                result = chain_with_context.invoke(inputs[\"question\"])\n",
    "                return {\n",
    "                    \"context\": result[\"context\"],\n",
    "                    \"answer\": result[\"answer\"],\n",
    "                    \"query\": inputs[\"question\"],\n",
    "                }\n",
    "\n",
    "\n",
    "            # ========================================\n",
    "            # Labeled Score 평가자 생성\n",
    "            # ========================================\n",
    "\n",
    "            print(\"=\"*50)\n",
    "            print(\"📊 Labeled Score 평가자 생성...\")\n",
    "            print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "\n",
    "            # 평가용 LLM\n",
    "            eval_llm = ChatOllama(\n",
    "                model=\"qwen2.5-coder:7b-instruct\",\n",
    "                temperature=0\n",
    "            )\n",
    "\n",
    "\n",
    "            # 1. Accuracy Score (1-10 점수)\n",
    "            accuracy_score_evaluator = LangChainStringEvaluator(\n",
    "                \"labeled_score_string\",\n",
    "                config={\n",
    "                    \"criteria\": {\n",
    "                        \"accuracy\": (\n",
    "                            \"How accurate is this prediction compared to the reference \"\n",
    "                            \"on a scale of 1-10? \"\n",
    "                            \"Rate factual correctness only.\"\n",
    "                        )\n",
    "                    },\n",
    "                    \"normalize_by\": 10,                      # 0-1 사이로 정규화\n",
    "                    \"llm\": llm,\n",
    "                },\n",
    "                prepare_data=lambda run, example: {\n",
    "                    \"prediction\": run.outputs[\"answer\"],\n",
    "                    \"reference\": example.outputs[\"answer\"],  # Ground Truth\n",
    "                    \"input\": example.inputs[\"question\"],\n",
    "                },\n",
    "            )\n",
    "\n",
    "            # 2. Completeness Score (1-10 점수)\n",
    "            completeness_score_evaluator = LangChainStringEvaluator(\n",
    "                \"labeled_score_string\",\n",
    "                config={\n",
    "                    \"criteria\": {\n",
    "                        \"completeness\": (\n",
    "                            \"How complete is this prediction compared to the reference \"\n",
    "                            \"on a scale of 1-10? \"\n",
    "                            \"Does it cover all important information?\"\n",
    "                        )\n",
    "                    },\n",
    "                    \"normalize_by\": 10,\n",
    "                    \"llm\": llm,\n",
    "                },\n",
    "                prepare_data=lambda run, example: {\n",
    "                    \"prediction\": run.outputs[\"answer\"],\n",
    "                    \"reference\": example.outputs[\"answer\"],  # Ground Truth\n",
    "                    \"input\": example.inputs[\"question\"],\n",
    "                },\n",
    "            )\n",
    "\n",
    "            # 3. Context Relevance Score (1-10 점수)\n",
    "            context_relevance_score_evaluator = LangChainStringEvaluator(\n",
    "                \"labeled_score_string\",\n",
    "                config={\n",
    "                    \"criteria\": {\n",
    "                        \"context_relevance\": (\n",
    "                            \"How well does this prediction use the provided context \"\n",
    "                            \"on a scale of 1-10? \"\n",
    "                            \"Rate how accurately it references the context.\"\n",
    "                        )\n",
    "                    },\n",
    "                    \"normalize_by\": 10,\n",
    "                    \"llm\": eval_llm,                      # 평가용 llm으로 수정\n",
    "                },\n",
    "                prepare_data=lambda run, example: {\n",
    "                    \"prediction\": run.outputs[\"answer\"],\n",
    "                    \"reference\": run.outputs[\"context\"],  # Context\n",
    "                    \"input\": example.inputs[\"question\"],\n",
    "                },\n",
    "            )\n",
    "\n",
    "            print(\"✅ 평가자 생성 완료\\n\")\n",
    "\n",
    "\n",
    "            # ========================================\n",
    "            # 평가 실행\n",
    "            # ========================================\n",
    "\n",
    "            print(\"=\"*50)\n",
    "            print(\"📊 평가 실행...\")\n",
    "            print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "            try:\n",
    "                experiment_results = evaluate(\n",
    "                    context_answer_rag_answer,\n",
    "                    data=\"RAG_EVAL_DATASET\",\n",
    "                    evaluators=[\n",
    "                        accuracy_score_evaluator,\n",
    "                        completeness_score_evaluator,\n",
    "                        context_relevance_score_evaluator,\n",
    "                    ],\n",
    "                    experiment_prefix=\"RAG_EVAL_LABELED_SCORE_LOCAL\",\n",
    "                    metadata={\n",
    "                        \"variant\": \"Labeled Score (Accuracy + Completeness + Context Relevance) - Local\",\n",
    "                        \"k\": 10,\n",
    "                        \"chunk_size\": 300,\n",
    "                        \"llm\": \"qwen2.5:14b-instruct\",\n",
    "                        \"eval_llm\": \"qwen2.5:14b-instruct\",\n",
    "                        \"scoring\": \"1-10 scale, normalized to 0-1\",\n",
    "                    },\n",
    "                    max_concurrency=1,\n",
    "                )\n",
    "                \n",
    "                print(\"\\n\" + \"=\"*50)\n",
    "                print(\"✅ 평가 완료!\")\n",
    "                print(\"=\"*50)\n",
    "                print(f\"\\n결과: {experiment_results}\\n\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"\\n❌ 에러 발생: {e}\\n\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de8c2eb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0459f1",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* **`Local LLM`으로 시도**\n",
    "\n",
    "    ```markdown\n",
    "\n",
    "    🚀 Labeled Score 평가 시작...\n",
    "\n",
    "    ✅ PDF 확인: /Users/jay/Projects/20250727-langchain-note/15_Evaluations/data/SPRI_AI_Brief_2023년12월호_F.pdf\n",
    "\n",
    "    ✅ 로컬 LLM 생성 완료: Qwen2.5-Coder-7B \n",
    "\n",
    "    ✅ PDF 로드 완료: 23 페이지\n",
    "    ✅ 청크 분할 완료: 119 청크 (크기=300, 오버랩=50)\n",
    "    ✅ 임베딩 모델 로드 완료: BAAI/bge-m3\n",
    "    ✅ 벡터스토어 생성 완료: FAISS\n",
    "    ✅ 검색기 생성 완료 (k=10, search_type=similarity)\n",
    "    ✅ Context 포함 RAG 체인 생성 완료\n",
    "\n",
    "    ==================================================\n",
    "    🧪 테스트 실행...\n",
    "    ==================================================\n",
    "\n",
    "    질문: 삼성전자가 자체 개발한 생성형 AI의 이름은 무엇인가요?\n",
    "    Context: SPRi AI Brief |  \n",
    "    2023-12월호\n",
    "    10\n",
    "    삼성전자, 자체 개발 생성 AI ‘삼성 가우스’ 공개\n",
    "    n 삼성전자가 온디바이스에서 작동 가능하며 언어, 코드, 이미지의 3개 모델로 구성된 자체 개발 생성 \n",
    "    AI 모델 ‘삼성 가우스’를 공개\n",
    "    n 삼성전자는 삼성 가우스를 다양한 제품에 단계적으로 탑재할 계획으로, 온디바이스 작동이 가능한 \n",
    "    삼성 가우스는...\n",
    "    답변: 삼성전자가 자체 개발한 생성형 AI의 이름은 '삼성 가우스'입니다.\n",
    "\n",
    "    ==================================================\n",
    "    📊 Labeled Score 평가자 생성...\n",
    "    ==================================================\n",
    "\n",
    "    ✅ 평가자 생성 완료\n",
    "\n",
    "    ==================================================\n",
    "    📊 평가 실행...\n",
    "    ==================================================\n",
    "    ```\n",
    "    ```bash\n",
    "    View the evaluation results for experiment: 'RAG_EVAL_LABELED_SCORE_LOCAL-c81ff062' at:\n",
    "    https://smith.langchain.com/o/2c3342d3-1170-4ffa-86fd-f621199e0b9c/datasets/420dd308-2ebd-44c9-8ce8-9aff3886dc8e/compare?selectedSessions=e66a2ac2-bbc0-4073-bec2-7a954dec30db\n",
    "    ```\n",
    "    ```markdown\n",
    "\n",
    "    5it [13:15, 159.12s/it]\n",
    "\n",
    "    ==================================================\n",
    "    ✅ 평가 완료!\n",
    "    ==================================================\n",
    "\n",
    "    결과: <ExperimentResults RAG_EVAL_LABELED_SCORE_LOCAL-c81ff062>\n",
    "\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4912de77",
   "metadata": {},
   "source": [
    "* **`LangSmith`로 결과 확인하기**\n",
    "\n",
    "  * ![`Score_EVAL_1`](../15_Evaluations/assets/Score_EVAL_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa2a070",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff26a34",
   "metadata": {},
   "source": [
    "* next: ***`06. 임베딩 기반 평가 (embedding_distance)`***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719f0c8c",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc_eval_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
