{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "574f6502",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf366bf9",
   "metadata": {},
   "source": [
    "* 출처: LangChain 공식 문서 또는 해당 교재명\n",
    "* 원본 URL: https://smith.langchain.com/hub/teddynote/summary-stuff-documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31f7552",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af770941",
   "metadata": {},
   "source": [
    "### **5. `05. LLM-as-Judge`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4af52a",
   "metadata": {},
   "source": [
    "* `LangSmith`에서 제공되는 **`Off-the-shelf Evaluators`** 활용해보기\n",
    "\n",
    "  * `Off-the-shelf Evaluators` = **`사전에 정의된 프롬프트 기반의 LLM 평가자`**\n",
    "\n",
    "  * 이점: 쉽게 사용 가능\n",
    "\n",
    "  * 더 확장된 기능 사용하기 위해서는 **`직접 평가자를 정의해야 함`**\n",
    "\n",
    "    * **`input` = 질문**: 보통 데이터셋의 **`Question`** 이 사용됨\n",
    "\n",
    "    * **`prediction` = `LLM`이 생성한 답변**: 보통 **`모델의 답변`** 이 사용됨\n",
    "\n",
    "    * **`reference` = `정답 답변`**: **`Context`** 등 `변칙적`으로 `활용` 가능\n",
    "\n",
    "  * *참고: [How to define a code evaluator](https://docs.langchain.com/langsmith/code-evaluator)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f802a885",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb9c7b9",
   "metadata": {},
   "source": [
    "* **`환경 설정`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acf7ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API 키를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API 키 정보 로드\n",
    "load_dotenv()                           # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d33436",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "from langsmith import traceable\n",
    "\n",
    "import os\n",
    "\n",
    "# LangSmith 환경 변수 확인\n",
    "\n",
    "print(\"\\n--- LangSmith 환경 변수 확인 ---\")\n",
    "langchain_tracing_v2 = os.getenv('LANGCHAIN_TRACING_V2')\n",
    "langchain_project = os.getenv('LANGCHAIN_PROJECT')\n",
    "langchain_api_key_status = \"설정됨\" if os.getenv('LANGCHAIN_API_KEY') else \"설정되지 않음\" # API 키 값은 직접 출력하지 않음\n",
    "\n",
    "if langchain_tracing_v2 == \"true\" and os.getenv('LANGCHAIN_API_KEY') and langchain_project:\n",
    "    print(f\"✅ LangSmith 추적 활성화됨 (LANGCHAIN_TRACING_V2='{langchain_tracing_v2}')\")\n",
    "    print(f\"✅ LangSmith 프로젝트: '{langchain_project}'\")\n",
    "    print(f\"✅ LangSmith API Key: {langchain_api_key_status}\")\n",
    "    print(\"  -> 이제 LangSmith 대시보드에서 이 프로젝트를 확인해 보세요.\")\n",
    "else:\n",
    "    print(\"❌ LangSmith 추적이 완전히 활성화되지 않았습니다. 다음을 확인하세요:\")\n",
    "    if langchain_tracing_v2 != \"true\":\n",
    "        print(f\"  - LANGCHAIN_TRACING_V2가 'true'로 설정되어 있지 않습니다 (현재: '{langchain_tracing_v2}').\")\n",
    "    if not os.getenv('LANGCHAIN_API_KEY'):\n",
    "        print(\"  - LANGCHAIN_API_KEY가 설정되어 있지 않습니다.\")\n",
    "    if not langchain_project:\n",
    "        print(\"  - LANGCHAIN_PROJECT가 설정되어 있지 않습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3cee3b",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 셀 출력\n",
    "\n",
    "    ```bash\n",
    "    --- LangSmith 환경 변수 확인 ---\n",
    "    ✅ LangSmith 추적 활성화됨 (LANGCHAIN_TRACING_V2='true')\n",
    "    ✅ LangSmith 프로젝트: 'LangChain-prantice'\n",
    "    ✅ LangSmith API Key: 설정됨\n",
    "    -> 이제 LangSmith 대시보드에서 이 프로젝트를 확인해 보세요.\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f78e3c7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9fee68",
   "metadata": {},
   "source": [
    "#### **1) `RAG 성능 테스트를 위한 함수 정의`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31900cc9",
   "metadata": {},
   "source": [
    "* **`테스트`에 활용할 `RAG` 시스템 생성하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c776f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from myrag import PDFRAG                        # local 임베딩 버전으로 수정한 myrag.py 불러오기\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# API 키 확인\n",
    "from dotenv import load_dotenv\n",
    "if not os.getenv(\"GOOGLE_API_KEY\"):\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = input(\"Enter your Google API key: \")\n",
    "\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    print(\"❌ 경고: GOOGLE_API_KEY 환경 변수가 설정되지 않았습니다. 반드시 설정해야 gemini LLM이 작동합니다.\")\n",
    "\n",
    "\n",
    "# PDFRAG 객체 생성\n",
    "# 이제 myrag.py가 OpenAIEmbeddings 대신 로컬 HuggingFaceEmbeddings를 사용하므로 Pydantic 오류가 사라짐\n",
    "try:\n",
    "    rag = PDFRAG(\n",
    "        \"../15_Evaluations/data/SPRI_AI_Brief_2023년12월호_F.pdf\",\n",
    "        ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\", temperature=0)\n",
    "    )\n",
    "    \n",
    "    print(\"\\n✅ PDFRAG 객체 초기화 성공!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ RAG 실행 중 오류 발생: {e}\")\n",
    "    print(\"OPENAI_API_KEY 설정 또는 PDF 파일 경로('./15_Evaluations/data/SPRI_AI_Brief_2023년12월호_F.pdf')를 확인해주세요.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3680f234",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* `gemini`객체 생성됨\n",
    "\n",
    "    ```bash\n",
    "\n",
    "    WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
    "    E0000 00:00:1760440556.370227 7916400 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
    "\n",
    "    ```\n",
    "\n",
    "* ✅ 문서 로드 완료: 23개 페이지\n",
    "\n",
    "* ✅ 문서 분할 완료: 43개 청크\n",
    "\n",
    "* ✅ 임베딩 모델 로드: all-MiniLM-L6-v2\n",
    "\n",
    "* ✅ 벡터스토어 생성 완료\n",
    "\n",
    "* ✅ PDFRAG 객체 초기화 성공!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132cb5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDFRAG 객체 생성\n",
    "from myrag import PDFRAG\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash-lite\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "rag = PDFRAG(\n",
    "    \"../15_Evaluations/data/SPRI_AI_Brief_2023년12월호_F.pdf\",\n",
    "    llm,\n",
    ")\n",
    "\n",
    "print(\"\\n✅ PDFRAG 객체 생성\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dfdea9",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* PDFRAG 객체 생성해보기\n",
    "\n",
    "    ```bash\n",
    "\n",
    "    E0000 00:00:1760440568.556856 7916400 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
    "\n",
    "    ```\n",
    "\n",
    "  * ✅ 문서 로드 완료: 23개 페이지\n",
    "  * ✅ 문서 분할 완료: 43개 청크\n",
    "  * ✅ 임베딩 모델 로드: all-MiniLM-L6-v2\n",
    "  * ✅ 벡터스토어 생성 완료\n",
    "\n",
    "  * ✅ PDFRAG 객체 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9a1166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검색기(retriever) 생성\n",
    "retriever = rag.create_retriever()                  # 10.5s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5397aa",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* **`✅ 검색기 생성 완료`** (*`k=4`*) - (`10.5s`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bb5ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 체인(chain) 생성\n",
    "chain = rag.create_chain(retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc8f9e1",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* **`✅ RAG 체인 생성 완료`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990958ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문\n",
    "answer = chain.invoke(\"삼성전자가 자체 개발한 생성형 AI의 이름은 무엇인가요?\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e8bf6a",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* **`삼성전자가 자체 개발한 생성형 AI의 이름은 '삼성 가우스'입니다.`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c880f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8b3d9d",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* **`print(chain)`**\n",
    "\n",
    "  ```bash\n",
    "\n",
    "  first={\n",
    "    context: VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x1393f1310>, search_kwargs={'k': 4}),\n",
    "    question: RunnablePassthrough()\n",
    "  } middle=[PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks.\\nUse the following pieces of retrieved context to answer the question.\\nIf you don't know the answer, just say that you don't know.\\n\\n#Context:\\n{context}\\n\\n#Question:\\n{question}\\n\\n#Answer:\"), ChatGoogleGenerativeAI(model='models/gemini-2.5-flash-lite', google_api_key=SecretStr('**********'), temperature=0.0, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x13905b9d0>, default_metadata=(), model_kwargs={})] last=StrOutputParser()\n",
    "\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b9f245",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e58c9d",
   "metadata": {},
   "source": [
    "* **`ask_question()` 함수 생성하기** \n",
    "\n",
    "  * 입력 = `inputs` = `딕셔너리`\n",
    "\n",
    "  * 출력 = `answer` = `딕셔너리` → `반환`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbbd2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문에 대한 답변하는 함수를 생성\n",
    "def ask_question(inputs: dict):\n",
    "    return {\"answer\": chain.invoke(inputs[\"question\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b34d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용자 질문 예시\n",
    "llm_answer = ask_question(\n",
    "    {\"question\": \"삼성전자가 자체 개발한 생성형 AI의 이름은 무엇인가요?\"})\n",
    "\n",
    "llm_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e116014a",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* **`llm_answer`** - (`0.9s`)\n",
    "\n",
    "    ```python\n",
    "\n",
    "        {'answer': \"삼성전자가 자체 개발한 생성형 AI의 이름은 '삼성 가우스'입니다.\"}\n",
    "\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e4c26d",
   "metadata": {},
   "source": [
    "* **`evalutor prompt` 출력 함수 정의하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d15e64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluator prompt 출력을 위한 함수\n",
    "def print_evaluator_prompt(evaluator):\n",
    "    return evaluator.evaluator.prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fd202b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff26a34",
   "metadata": {},
   "source": [
    "* next: ***`05. LLM-as-Judge-2`***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719f0c8c",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
