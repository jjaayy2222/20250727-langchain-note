{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba52bed0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65001001",
   "metadata": {},
   "source": [
    "* ì¶œì²˜: LangChain ê³µì‹ ë¬¸ì„œ ë˜ëŠ” í•´ë‹¹ êµì¬ëª…\n",
    "* ì›ë³¸ URL: https://smith.langchain.com/hub/teddynote/summary-stuff-documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f884510",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb18b49c",
   "metadata": {},
   "source": [
    "### **3. `RunnableLambda`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f936725",
   "metadata": {},
   "source": [
    "#### **1) `RunnableLambda`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad1807f",
   "metadata": {},
   "source": [
    "* **`ì‚¬ìš©ì ì •ì˜ í•¨ìˆ˜ ì‹¤í–‰`** í•  ìˆ˜ ìˆëŠ” ê¸°ëŠ¥\n",
    "\n",
    "  * **`RunnableLambda` â†’ `ìì‹ ë§Œì˜ í•¨ìˆ˜ ì •ì˜, ì‹¤í–‰ ê°€ëŠ¥`**\n",
    "\n",
    "  * ì˜ˆì‹œ: *ë°ì´í„° ì „ì²˜ë¦¬*, *ê³„ì‚°*, *ì™¸ë¶€ APIì™€ì˜ ìƒí˜¸ ì‘ìš©* ë“±"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6638d77",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6bb9c2",
   "metadata": {},
   "source": [
    "#### **2) `ì‚¬ìš©ì ì •ì˜ í•¨ìˆ˜ ì‹¤í–‰í•˜ëŠ” ë°©ë²•`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb5960e",
   "metadata": {},
   "source": [
    "* **`ì£¼ì˜ì‚¬í•­`**\n",
    "\n",
    "  * **`RunnableLambda`** â†’ ì‚¬ìš©ì ì •ì˜ í•¨ìˆ˜ë¥¼ ë˜í•‘í•´ì„œ í™œìš© ê°€ëŠ¥\n",
    "\n",
    "  * **`ì‚¬ìš©ì ì •ì˜ í•¨ìˆ˜ê°€ ë°›ì„ ìˆ˜ ìˆëŠ” ì¸ì` = `only 1`**\n",
    "    * *ë§Œì•½, ì—¬ëŸ¬ ì¸ìˆ˜ë¥¼ ë°›ëŠ” í•¨ìˆ˜ë¡œ êµ¬í˜„í•˜ê³  ì‹¶ë‹¤ë©´?*\n",
    "    * ë‹¨ì¼  ì…ë ¥ì„ ë°›ì•„ë“¤ì´ê³  ì´ë¥¼ ì—¬ëŸ¬ ì¸ìˆ˜ë¡œ í’€ì–´ë‚´ëŠ” ë˜í¼ë¥¼ ì‘ì„±í•´ì•¼ í•¨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d763fc98",
   "metadata": {},
   "source": [
    "* **`í™˜ê²½ì„¤ì •`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ca9302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API í‚¤ë¥¼ í™˜ê²½ë³€ìˆ˜ë¡œ ê´€ë¦¬í•˜ê¸° ìœ„í•œ ì„¤ì • íŒŒì¼\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API í‚¤ ì •ë³´ ë¡œë“œ\n",
    "load_dotenv()                           # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ef86f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "from langsmith import traceable\n",
    "\n",
    "import os\n",
    "\n",
    "# LangSmith í™˜ê²½ ë³€ìˆ˜ í™•ì¸\n",
    "\n",
    "print(\"\\n--- LangSmith í™˜ê²½ ë³€ìˆ˜ í™•ì¸ ---\")\n",
    "langchain_tracing_v2 = os.getenv('LANGCHAIN_TRACING_V2')\n",
    "langchain_project = os.getenv('LANGCHAIN_PROJECT')\n",
    "langchain_api_key_status = \"ì„¤ì •ë¨\" if os.getenv('LANGCHAIN_API_KEY') else \"ì„¤ì •ë˜ì§€ ì•ŠìŒ\" # API í‚¤ ê°’ì€ ì§ì ‘ ì¶œë ¥í•˜ì§€ ì•ŠìŒ\n",
    "\n",
    "if langchain_tracing_v2 == \"true\" and os.getenv('LANGCHAIN_API_KEY') and langchain_project:\n",
    "    print(f\"âœ… LangSmith ì¶”ì  í™œì„±í™”ë¨ (LANGCHAIN_TRACING_V2='{langchain_tracing_v2}')\")\n",
    "    print(f\"âœ… LangSmith í”„ë¡œì íŠ¸: '{langchain_project}'\")\n",
    "    print(f\"âœ… LangSmith API Key: {langchain_api_key_status}\")\n",
    "    print(\"  -> ì´ì œ LangSmith ëŒ€ì‹œë³´ë“œì—ì„œ ì´ í”„ë¡œì íŠ¸ë¥¼ í™•ì¸í•´ ë³´ì„¸ìš”.\")\n",
    "else:\n",
    "    print(\"âŒ LangSmith ì¶”ì ì´ ì™„ì „íˆ í™œì„±í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‹¤ìŒì„ í™•ì¸í•˜ì„¸ìš”:\")\n",
    "    if langchain_tracing_v2 != \"true\":\n",
    "        print(f\"  - LANGCHAIN_TRACING_V2ê°€ 'true'ë¡œ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤ (í˜„ì¬: '{langchain_tracing_v2}').\")\n",
    "    if not os.getenv('LANGCHAIN_API_KEY'):\n",
    "        print(\"  - LANGCHAIN_API_KEYê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
    "    if not langchain_project:\n",
    "        print(\"  - LANGCHAIN_PROJECTê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f14d715",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* ì…€ ì¶œë ¥\n",
    "\n",
    "```bash\n",
    "    --- LangSmith í™˜ê²½ ë³€ìˆ˜ í™•ì¸ ---\n",
    "    âœ… LangSmith ì¶”ì  í™œì„±í™”ë¨ (LANGCHAIN_TRACING_V2='true')\n",
    "    âœ… LangSmith í”„ë¡œì íŠ¸: 'LangChain-prantice'\n",
    "    âœ… LangSmith API Key: ì„¤ì •ë¨\n",
    "    -> ì´ì œ LangSmith ëŒ€ì‹œë³´ë“œì—ì„œ ì´ í”„ë¡œì íŠ¸ë¥¼ í™•ì¸í•´ ë³´ì„¸ìš”.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403df77a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cf7d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os                                       # 3.6s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ìŠ¤íŠ¸ì˜ ê¸¸ì´ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜\n",
    "def length_function(text):  \n",
    "    return len(text)\n",
    "\n",
    "# ë‘ í…ìŠ¤íŠ¸ì˜ ê¸¸ì´ë¥¼ ê³±í•˜ëŠ” í•¨ìˆ˜\n",
    "def _multiple_length_function(text1, text2):  \n",
    "    return len(text1) * len(text2)\n",
    "\n",
    "# 2ê°œ ì¸ìë¥¼ ë°›ëŠ” í•¨ìˆ˜ë¡œ ì—°ê²°í•˜ëŠ” wrapper í•¨ìˆ˜\n",
    "# ë”•ì…”ë„ˆë¦¬ì—ì„œ \"text1\"ê³¼ \"text2\"ì˜ ê¸¸ì´ë¥¼ ê³±í•˜ëŠ” í•¨ìˆ˜\n",
    "def multiple_length_function(  \n",
    "    _dict,\n",
    "): \n",
    "    return _multiple_length_function(_dict[\"text1\"], _dict[\"text2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798428d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"what is {a} + {b}?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0999574d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(prompt))             # <class 'langchain_core.prompts.chat.ChatPromptTemplate'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API í‚¤ í™•ì¸\n",
    "if not os.getenv(\"GOOGLE_API_KEY\"):    \n",
    "    os.environ[\"GOOGLE_API_KEY\"] = input(\"Enter your Google API key: \")\n",
    "    \n",
    "# LLM ì´ˆê¸°í™”\n",
    "gemini_lc = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash-lite\",        \n",
    "    temperature=0,                                              # temperature = 0ìœ¼ë¡œ ì„¤ì •          \n",
    "    max_output_tokens=4096,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4650f18b",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* `LLM` ì´ˆê¸°í™”\n",
    "\n",
    "    ```bash\n",
    "\n",
    "    E0000 00:00:1759973579.086644 1654451 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
    "\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í”„ë¡¬í”„íŠ¸ì™€ ëª¨ë¸ì„ ì—°ê²°í•˜ì—¬ ì²´ì¸ ìƒì„±\n",
    "\n",
    "chain1 = prompt | gemini_lc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì²´ì¸ êµ¬ì„±\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"a\": itemgetter(\"input_1\") | RunnableLambda(length_function),\n",
    "        \"b\": {\"text1\": itemgetter(\"input_1\"), \"text2\": itemgetter(\"input_2\")}\n",
    "        | RunnableLambda(multiple_length_function),\n",
    "    }\n",
    "    | prompt\n",
    "    | gemini_lc\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21778b6d",
   "metadata": {},
   "source": [
    "* **`chain`** ì‹¤í–‰ â†’ ê²°ê³¼ í™•ì¸í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a153d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì£¼ì–´ì§„ ì¸ìë“¤ë¡œ ì²´ì¸ ì‹¤í–‰í•˜ê¸°\n",
    "\n",
    "chain.invoke({\"input_1\": \"bar\", \"input_2\": \"gah\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fb4234",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* ì£¼ì–´ì§„ ì¸ìë“¤ë¡œ ì²´ì¸ ì‹¤í–‰í•˜ê¸°  (`1.3s`)\n",
    "\n",
    "    ```markdown\n",
    "\n",
    "    '3 + 9 = 12'\n",
    "\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd27120",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0aba55",
   "metadata": {},
   "source": [
    "#### **3) `RunnableConfig` ì¸ìë¡œ í™œìš©**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7326b18c",
   "metadata": {},
   "source": [
    "* **`RunnableLambda`** â†’ *(ì„ íƒì ìœ¼ë¡œ)* [**`RunnableConfig`**](https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig) ìˆ˜ìš© ê°€ëŠ¥\n",
    "\n",
    "  * **`ì½œë°±`, `íƒœê·¸`, `ê¸°íƒ€  êµ¬ì„± ì •ë³´` â†’ `ì¤‘ì²©ëœ ì‹¤í–‰ì— ì „ë‹¬ ê°€ëŠ¥`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58dd616",
   "metadata": {},
   "source": [
    "* `gemini` ëª¨ë¸ì€ `ì‘ë‹µ`ì— `í† í° ì •ë³´`ë¥¼ í¬í•¨\n",
    "\n",
    "  * *`get_openai_callback()` = **`OpenAI`** ì „ìš© ë°©ë²•*\n",
    "\n",
    "  * **`geimini`** ë°©ë²•\n",
    "    * a. `usage_metadata` ì§ì ‘ ì‚¬ìš©í•´ë³´ê¸°\n",
    "    * b. `Custom Callback Handler` ì‚¬ìš©í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8784e972",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda, RunnableConfig\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadc4d69",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* **`a`** ì‹œë„ - ***`usage_metadata()`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb935ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a. ì§ì ‘ í˜¸ì¶œ + usage_metadata\n",
    "\n",
    "def parse_or_fix_with_tracking(text: str, config: RunnableConfig):\n",
    "    fixing_prompt = ChatPromptTemplate.from_template(\n",
    "        \"Fix the following text:\\n\\ntext\\n{input}\\n\\nError: {error}\"\n",
    "        \" Don't narrate, just respond with the fixed data.\"\n",
    "    )\n",
    "    \n",
    "    # í† í° ì‚¬ìš©ëŸ‰ ëˆ„ì \n",
    "    total_input_tokens = 0\n",
    "    total_output_tokens = 0\n",
    "    \n",
    "    # ìµœëŒ€ 3ë²ˆ ì‹œë„\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            return json.loads(text)\n",
    "        \n",
    "        except Exception as e:\n",
    "            # í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
    "            messages = fixing_prompt.invoke({\"input\": text, \"error\": e})\n",
    "            \n",
    "            # âœ¨ LLM ì§ì ‘ í˜¸ì¶œ (AIMessage ë°˜í™˜)\n",
    "            ai_message = gemini_lc.invoke(messages, config=config)\n",
    "            \n",
    "            # âœ¨ í† í° ì‚¬ìš©ëŸ‰ ì¶”ì¶œ\n",
    "            usage = ai_message.usage_metadata\n",
    "            total_input_tokens += usage['input_tokens']\n",
    "            total_output_tokens += usage['output_tokens']\n",
    "            \n",
    "            print(f\"\\n--- Attempt {attempt + 1} ---\")\n",
    "            print(f\"Input tokens: {usage['input_tokens']}\")\n",
    "            print(f\"Output tokens: {usage['output_tokens']}\")\n",
    "            print(f\"Total tokens: {usage['total_tokens']}\")\n",
    "            \n",
    "            # ìˆ˜ì •ëœ í…ìŠ¤íŠ¸\n",
    "            text = ai_message.content\n",
    "            \n",
    "    # ìµœì¢… í† í° ì‚¬ìš©ëŸ‰ ì¶œë ¥\n",
    "    print(f\"\\n=== Final Token Usage ===\")\n",
    "    print(f\"Total Input tokens: {total_input_tokens}\")\n",
    "    print(f\"Total Output tokens: {total_output_tokens}\")\n",
    "    print(f\"Total tokens: {total_input_tokens + total_output_tokens}\")\n",
    "    \n",
    "    # íŒŒì‹± ì‹¤íŒ¨ ì‹œ â†’ \"Failed to parse\" ë¬¸ìì—´ ë°˜í™˜\n",
    "    return \"Failed to parse\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ca75c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a. ì‹¤í–‰í•´ë³´ê¸°\n",
    "\n",
    "output = RunnableLambda(parse_or_fix_with_tracking).invoke(\n",
    "    input=\"{foo:: bar}\",\n",
    "    config={\"tags\": [\"my-tag\"]},\n",
    ")\n",
    "\n",
    "print(f\"\\n\\nìˆ˜ì •í•œ ê²°ê³¼:\\n{output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0119b7",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* aë°©ë²• ì‹¤í–‰ ê²°ê³¼ (`0.8s`)\n",
    "\n",
    "    ```markdown\n",
    "    --- Attempt 1 ---\n",
    "    Input tokens: 50\n",
    "    Output tokens: 6\n",
    "    Total tokens: 56\n",
    "\n",
    "\n",
    "    ìˆ˜ì •í•œ ê²°ê³¼:\n",
    "    ```\n",
    "    ```python\n",
    "        {'foo': 'bar'}\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00eb3d1c",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* **`b`** ì‹œë„ - ***`Custom Callback Handler`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a60980",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda, RunnableConfig\n",
    "from langchain_core.callbacks import BaseCallbackHandler\n",
    "from typing import Any\n",
    "import json\n",
    "\n",
    "# Custom Callback Handler = í† í° ì‚¬ìš©ëŸ‰ ì¶”ì  í•¸ë“¤ëŸ¬\n",
    "\n",
    "class TokenTrackingHandler(BaseCallbackHandler):\n",
    "    \"\"\"Gemini í† í° ì‚¬ìš©ëŸ‰ ì¶”ì  í•¸ë“¤ëŸ¬\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.total_input_tokens = 0\n",
    "        self.total_output_tokens = 0\n",
    "        self.total_tokens = 0\n",
    "        self.call_count = 0\n",
    "    \n",
    "    def on_llm_end(self, response: Any, **kwargs: Any) -> None:\n",
    "        \"\"\"LLM í˜¸ì¶œ ì™„ë£Œ ì‹œ í˜¸ì¶œ\"\"\"\n",
    "        # Geminiì˜ usage_metadata ì¶”ì¶œ\n",
    "        if hasattr(response, 'generations') and response.generations:\n",
    "            generation = response.generations[0][0]\n",
    "            if hasattr(generation, 'message') and hasattr(generation.message, 'usage_metadata'):\n",
    "                usage = generation.message.usage_metadata\n",
    "                \n",
    "                self.total_input_tokens += usage.get('input_tokens', 0)\n",
    "                self.total_output_tokens += usage.get('output_tokens', 0)\n",
    "                self.total_tokens += usage.get('total_tokens', 0)\n",
    "                self.call_count += 1\n",
    "                \n",
    "                print(f\"\\n[Call {self.call_count}] Tokens:\")\n",
    "                print(f\"  Input: {usage.get('input_tokens', 0)}\")\n",
    "                print(f\"  Output: {usage.get('output_tokens', 0)}\")\n",
    "                print(f\"  Total: {usage.get('total_tokens', 0)}\")\n",
    "    \n",
    "    def get_summary(self):\n",
    "        \"\"\"ìµœì¢… ìš”ì•½\"\"\"\n",
    "        return {\n",
    "            \"total_input_tokens\": self.total_input_tokens,\n",
    "            \"total_output_tokens\": self.total_output_tokens,\n",
    "            \"total_tokens\": self.total_tokens,\n",
    "            \"total_calls\": self.call_count,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse_or_fix í•¨ìˆ˜\n",
    "\n",
    "def parse_or_fix(text: str, config: RunnableConfig):\n",
    "    fixing_chain = (\n",
    "        ChatPromptTemplate.from_template(\n",
    "            \"Fix the following text:\\n\\ntext\\n{input}\\n\\nError: {error}\"\n",
    "            \" Don't narrate, just respond with the fixed data.\"\n",
    "        )\n",
    "        | gemini_lc\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    # ìµœëŒ€ 3ë²ˆ ì‹œë„\n",
    "    for _ in range(3):\n",
    "        try:\n",
    "            return json.loads(text)\n",
    "        \n",
    "        except Exception as e:\n",
    "            # ìˆ˜ì • ì²´ì¸ í˜¸ì¶œ\n",
    "            text = fixing_chain.invoke({\"input\": text, \"error\": e}, config)\n",
    "    \n",
    "    return \"Failed to parse\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b. ì‹¤í–‰í•´ë³´ê¸° - Callback Handler ì‚¬ìš©í•˜ê¸°\n",
    "\n",
    "token_handler = TokenTrackingHandler()\n",
    "\n",
    "output = RunnableLambda(parse_or_fix).invoke(\n",
    "    input=\"{foo:: bar}\",\n",
    "    config={\n",
    "        \"tags\": [\"my-tag\"],\n",
    "        \"callbacks\": [token_handler],  # âœ¨ Custom Handler\n",
    "    },\n",
    ")\n",
    "\n",
    "# ìµœì¢… ê²°ê³¼\n",
    "print(f\"\\n\\nìˆ˜ì •í•œ ê²°ê³¼:\\n{output}\")\n",
    "\n",
    "# í† í° ì‚¬ìš©ëŸ‰ ìš”ì•½\n",
    "print(\"\\n=== Token Usage Summary ===\")\n",
    "summary = token_handler.get_summary()\n",
    "for key, value in summary.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94f4c5e",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* b ì‹¤í–‰ ê²°ê³¼ (`1.0s`)\n",
    "\n",
    "    ```markdown\n",
    "    [Call 1] Tokens:\n",
    "      Input: 50\n",
    "      Output: 6\n",
    "      Total: 56\n",
    "\n",
    "\n",
    "    ìˆ˜ì •í•œ ê²°ê³¼:\n",
    "    ```\n",
    "    ```python\n",
    "        {'foo': 'bar'}\n",
    "    ```\n",
    "\n",
    "    ```markdown\n",
    "    === Token Usage Summary ===\n",
    "    total_input_tokens: 50\n",
    "    total_output_tokens: 6\n",
    "    total_tokens: 56\n",
    "    total_calls: 1\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2d664c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a08b3cd",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* ğŸ“Š  `a` ë°©ë²• vs `b` ë°©ë²• \n",
    "\n",
    "  | ë°©ë²• | ì¥ì  | ë‹¨ì  | ì¶”ì²œ |\n",
    "  |------|------|------|------|\n",
    "  | **`a` (ì§ì ‘ usage_metadata)** | ê°„ë‹¨, ì§ê´€ì  | Chain ë‚´ë¶€ í˜¸ì¶œ ì¶”ì  ì–´ë ¤ì›€ | â­â­â­â­â­ |\n",
    "  | **`b` (Custom Callback)** | ì •êµí•œ ì¶”ì , ëª¨ë“  í˜¸ì¶œ ìºì¹˜ | êµ¬í˜„ ë³µì¡ | â­â­â­â­ |\n",
    "\n",
    "<br>\n",
    "\n",
    "* ğŸ“Š `OpenAI callback` vs `gemini callback`\n",
    "\n",
    "  * `OpenAI ì „ìš© ì½œë°± í•¨ìˆ˜` ì¡´ì¬\n",
    "  ```python\n",
    "\n",
    "      from langchain.callbacks import get_openai_callback     # âœ…\n",
    "\n",
    "  ```\n",
    "\n",
    "  * `gemini` **ì „ìš© ì½œë°± í•¨ìˆ˜ ì¡´ì¬í•˜ì§€ ì•ŠìŒ**\n",
    "  ```python\n",
    "\n",
    "      from langchain.callbacks import get_gemini_callback     # âŒ ì¡´ì¬í•˜ì§€ ì•ŠìŒ\n",
    "\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b3b031",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7626d2",
   "metadata": {},
   "source": [
    "* next: ***`04. LLM ì²´ì¸ ë¼ìš°íŒ… (RunnableLambda, RunnableBranch)`***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f4de16",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
