{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf989cca",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5587735",
   "metadata": {},
   "source": [
    "* 출처: LangChain 공식 문서 또는 해당 교재명\n",
    "* 원본 URL: https://smith.langchain.com/hub/teddynote/summary-stuff-documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fe9ecc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e46434a",
   "metadata": {},
   "source": [
    "#### **2. `ContextualCompressionRetriever`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2724843",
   "metadata": {},
   "source": [
    "#### **1) `문맥 압축 검색기`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480d76a2",
   "metadata": {},
   "source": [
    "* **`검색 시스템의 어려움과 문제점`**\n",
    "\n",
    "  * 사용자가 **`어떤 질문`(`질의`)을 할지 `미리 알 수 없다`는 점**\n",
    "\n",
    "  * **`문제점`**: **`질문에 가장 관련 있는 정보`가 *`매우 많은 불필요한 텍스트가 담긴 문서`* 속에 섞여 있을 수 있음**\n",
    "\n",
    "  * **`결과`**: **`전체 문서`를 `언어 모델`(`LLM`)에 그대로 전달 시 → `비용이 많이 들고`, `답변의 품질이 낮아짐`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726b8d99",
   "metadata": {},
   "source": [
    "* ![문맥 압축 검색기](../10_Retriever/images/01-Contextual-Compression.jpeg)\n",
    "  * *[출처](https://drive.google.com/uc?id=1CtNgWODXZudxAWSRiWgSGEoTNrUFT98v):* *https://drive.google.com/uc?id=1CtNgWODXZudxAWSRiWgSGEoTNrUFT98v*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cb0322",
   "metadata": {},
   "source": [
    "* **`ContextualCompressionRetriever` = 해결책**\n",
    "\n",
    "  * **`기본 아이디어`**: 검색된 문서를 바로 반환 X → **`사용자의 질문(질의)의 맥락`을 사용** → 문서의 내용을 **`압축`**\n",
    "\n",
    "  * **`압축`의 의미**:\n",
    "\n",
    "    * 개별 문서에서 `관련 없는 내용`을 `제거` → 내용을 줄임\n",
    "\n",
    "    * `불필요한 문서`를 `목록`에서 `제외`하는 것을 모두 포함\n",
    "\n",
    "  * **`작동 방식`**:\n",
    "\n",
    "    * 질의를 **`기본 검색기`(`base retriever`)** 에 전달\n",
    "\n",
    "    * 기본 검색기에서 초기 문서를 가져옴\n",
    "\n",
    "    * 이 문서를 `Document Compressor`에 통과시켜 가장 관련 있는 정보만 남도록 내용을 줄이거나 문서를 제거\n",
    "\n",
    "  * **`목표`**: 관련 있는 정보만 응용 프로그램에 전달되게 하여, LLM 호출 비용을 줄이고 답변 품질을 높이는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d5a301",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8748488",
   "metadata": {},
   "source": [
    "#### **2) `설정`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b37773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API 키를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API 키 정보 로드\n",
    "load_dotenv()                               # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b64773",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "from langsmith import traceable\n",
    "\n",
    "import os\n",
    "\n",
    "# LangSmith 환경 변수 확인\n",
    "\n",
    "print(\"\\n--- LangSmith 환경 변수 확인 ---\")\n",
    "langchain_tracing_v2 = os.getenv('LANGCHAIN_TRACING_V2')\n",
    "langchain_project = os.getenv('LANGCHAIN_PROJECT')\n",
    "langchain_api_key_status = \"설정됨\" if os.getenv('LANGCHAIN_API_KEY') else \"설정되지 않음\" # API 키 값은 직접 출력하지 않음\n",
    "\n",
    "if langchain_tracing_v2 == \"true\" and os.getenv('LANGCHAIN_API_KEY') and langchain_project:\n",
    "    print(f\"✅ LangSmith 추적 활성화됨 (LANGCHAIN_TRACING_V2='{langchain_tracing_v2}')\")\n",
    "    print(f\"✅ LangSmith 프로젝트: '{langchain_project}'\")\n",
    "    print(f\"✅ LangSmith API Key: {langchain_api_key_status}\")\n",
    "    print(\"  -> 이제 LangSmith 대시보드에서 이 프로젝트를 확인해 보세요.\")\n",
    "else:\n",
    "    print(\"❌ LangSmith 추적이 완전히 활성화되지 않았습니다. 다음을 확인하세요:\")\n",
    "    if langchain_tracing_v2 != \"true\":\n",
    "        print(f\"  - LANGCHAIN_TRACING_V2가 'true'로 설정되어 있지 않습니다 (현재: '{langchain_tracing_v2}').\")\n",
    "    if not os.getenv('LANGCHAIN_API_KEY'):\n",
    "        print(\"  - LANGCHAIN_API_KEY가 설정되어 있지 않습니다.\")\n",
    "    if not langchain_project:\n",
    "        print(\"  - LANGCHAIN_PROJECT가 설정되어 있지 않습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecadfe5",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 셀 출력\n",
    "\n",
    "    ```markdown\n",
    "    --- LangSmith 환경 변수 확인 ---\n",
    "    ✅ LangSmith 추적 활성화됨 (LANGCHAIN_TRACING_V2='true')\n",
    "    ✅ LangSmith 프로젝트: 'LangChain-prantice'\n",
    "    ✅ LangSmith API Key: 설정됨\n",
    "    -> 이제 LangSmith 대시보드에서 이 프로젝트를 확인해 보세요.\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff86048e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서를 예쁘게 출력하기 위한 도우미 함수\n",
    "def pretty_print_docs(docs):\n",
    "    print(\n",
    "        f\"\\n{'-' * 100}\\n\".join(\n",
    "            [f\"문서 {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf86c0a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9c8342",
   "metadata": {},
   "source": [
    "#### **3) `기본 Retriever 설정`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b47e92e",
   "metadata": {},
   "source": [
    "* 간단한 벡터 스토어 `retriever` 초기화 → 텍스트 문서를 청크 단위로 저장하는 것부터 시작\n",
    "\n",
    "* 예시 질문: `retriever`는 관련 있는 문서 `1~2`개와 관련 없는 문서 몇 개를 반환하는 것을 확인할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf3a08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "import warnings\n",
    "\n",
    "# 경고 무시\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    "    )\n",
    "\n",
    "# 1단계: Fake Embeddings 사용\n",
    "embeddings = embeddings\n",
    "\n",
    "# 임베딩 차원 크기를 계산\n",
    "dimension_size = len(embeddings.embed_query(\"hello world\"))\n",
    "print(dimension_size)                                                     # 384\n",
    "print(\"✅ HuggingFaceEmbeddings 초기화 완료!\")                               # ✅ HuggingFaceEmbeddings 초기화 완료!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29107ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2단계: 문서 로더 및 분할\n",
    "\n",
    "loader = TextLoader(\"../10_Retriever/data/appendix-keywords.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "split_docs = text_splitter.split_documents(documents)            # split_documents() 사용!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8242992b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3단계: 벡터스토어 생성\n",
    "\n",
    "db = FAISS.from_documents(split_docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f5864aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4단계: 검색기(Retriever) 생성\n",
    "\n",
    "retriever = db.as_retriever()                      # 벡터스토어에서 as_retriever() 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "156036c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5단계: 검색 실행\n",
    "\n",
    "docs = retriever.invoke(\"Semantic Search 에 대해서 알려줘.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78c75d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6단계: 결과 출력\n",
    "def pretty_print_docs(docs):\n",
    "    for i, doc in enumerate(docs):\n",
    "        print(f\"Document {i+1}:\")\n",
    "        print(doc.page_content)\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "pretty_print_docs(docs)\n",
    "print(\"🎉 ✅ HuggingFaceEmbeddings로 완벽 실행!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00966f8",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 셀 출력\n",
    "\n",
    "    ```markdown\n",
    "    Document 1:\n",
    "    Semantic Search\n",
    "\n",
    "    정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
    "    예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
    "    연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
    "\n",
    "    Embedding\n",
    "    ==================================================\n",
    "    Document 2:\n",
    "    정의: 토크나이저는 텍스트 데이터를 토큰으로 분할하는 도구입니다. 이는 자연어 처리에서 데이터를 전처리하는 데 사용됩니다.\n",
    "    예시: \"I love programming.\"이라는 문장을 [\"I\", \"love\", \"programming\", \".\"]으로 분할합니다.\n",
    "    연관키워드: 토큰화, 자연어 처리, 구문 분석\n",
    "\n",
    "    VectorStore\n",
    "    ==================================================\n",
    "    Document 3:\n",
    "    정의: JSON(JavaScript Object Notation)은 경량의 데이터 교환 형식으로, 사람과 기계 모두에게 읽기 쉬운 텍스트를 사용하여 데이터 객체를 표현합니다.\n",
    "    예시: {\"이름\": \"홍길동\", \"나이\": 30, \"직업\": \"개발자\"}는 JSON 형식의 데이터입니다.\n",
    "    연관키워드: 데이터 교환, 웹 개발, API\n",
    "\n",
    "    Transformer\n",
    "    ==================================================\n",
    "    Document 4:\n",
    "    정의: SQL(Structured Query Language)은 데이터베이스에서 데이터를 관리하기 위한 프로그래밍 언어입니다. 데이터 조회, 수정, 삽입, 삭제 등 다양한 작업을 수행할 수 있습니다.\n",
    "    예시: SELECT * FROM users WHERE age > 18;은 18세 이상의 사용자 정보를 조회합니다.\n",
    "    연관키워드: 데이터베이스, 쿼리, 데이터 관리\n",
    "\n",
    "    CSV\n",
    "    ==================================================\n",
    "    🎉 ✅ HuggingFaceEmbeddings로 완벽 실행!\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635048b7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4047d45c",
   "metadata": {},
   "source": [
    "#### **4) `맥락적 압축`** (`Contextual Compression`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23f74cd",
   "metadata": {},
   "source": [
    "* **`LLM Chain Extractor` → 생성한 `DocumentCompressor`를 `retriever`를 적용한 것 = `ContextualCompressionaRetriever`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e7d558",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# LLM 초기화\n",
    "gemini_lc = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-2.5-flash-lite\",\n",
    "        temperature=0,                                    \n",
    "        max_output_tokens=4096,\n",
    "    )\n",
    "\n",
    "# LLM을 사용하여 문서 압축기 생성\n",
    "compressor = LLMChainExtractor.from_llm(gemini_lc)\n",
    "\n",
    "# Contextual Compression Retriever 생성\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    # 문서 압축기와 리트리버를 사용하여 컨텍스트 압축 리트리버 생성\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=retriever,\n",
    ")\n",
    "\n",
    "# 비교 실행\n",
    "def pretty_print_docs(docs):\n",
    "    for i, doc in enumerate(docs):\n",
    "        print(f\"Document {i+1}:\")\n",
    "        print(doc.page_content)\n",
    "        print(\"=\" * 50)\n",
    "        print()\n",
    "\n",
    "print(\"🔍 기본 Retriever 결과:\")\n",
    "basic_docs = retriever.invoke(\"Semantic Search 에 대해서 알려줘.\")\n",
    "pretty_print_docs(basic_docs)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"🧠 LLMChainExtractor 압축 후 결과:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "compressed_docs = compression_retriever.invoke(\"Semantic Search 에 대해서 알려줘.\")\n",
    "pretty_print_docs(compressed_docs)\n",
    "\n",
    "print(\"✅ Contextual Compression 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f432765",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 셀 출력 (3.4s)\n",
    "\n",
    "    ```bash\n",
    "    E0000 00:00:1759054106.473395 2024367 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
    "    ```\n",
    "    ```markdown\n",
    "    🔍 기본 Retriever 결과:\n",
    "    Document 1:\n",
    "    정의: InstructGPT는 사용자의 지시에 따라 특정한 작업을 수행하기 위해 최적화된 GPT 모델입니다. 이 모델은 보다 정확하고 관련성 높은 결과를 생성하도록 설계되었습니다.\n",
    "    예시: 사용자가 \"이메일 초안 작성\"과 같은 특정 지시를 제공하면, InstructGPT는 관련 내용을 기반으로 이메일을 작성합니다.\n",
    "    연관키워드: 인공지능, 자연어 이해, 명령 기반 처리\n",
    "\n",
    "    Keyword Search\n",
    "    ==================================================\n",
    "\n",
    "    Document 2:\n",
    "    정의: 구조화된 데이터는 정해진 형식이나 스키마에 따라 조직된 데이터입니다. 이는 데이터베이스, 스프레드시트 등에서 쉽게 검색하고 분석할 수 있습니다.\n",
    "    예시: 관계형 데이터베이스에 저장된 고객 정보 테이블은 구조화된 데이터의 예입니다.\n",
    "    연관키워드: 데이터베이스, 데이터 분석, 데이터 모델링\n",
    "\n",
    "    Parser\n",
    "    ==================================================\n",
    "\n",
    "    Document 3:\n",
    "    정의: 페이지 랭크는 웹 페이지의 중요도를 평가하는 알고리즘으로, 주로 검색 엔진 결과의 순위를 결정하는 데 사용됩니다. 이는 웹 페이지 간의 링크 구조를 분석하여 평가합니다.\n",
    "    예시: 구글 검색 엔진은 페이지 랭크 알고리즘을 사용하여 검색 결과의 순위를 정합니다.\n",
    "    연관키워드: 검색 엔진 최적화, 웹 분석, 링크 분석\n",
    "\n",
    "    데이터 마이닝\n",
    "    ==================================================\n",
    "\n",
    "    Document 4:\n",
    "    정의: 크롤링은 자동화된 방식으로 웹 페이지를 방문하여 데이터를 수집하는 과정입니다. 이는 검색 엔진 최적화나 데이터 분석에 자주 사용됩니다.\n",
    "    예시: 구글 검색 엔진이 인터넷 상의 웹사이트를 방문하여 콘텐츠를 수집하고 인덱싱하는 것이 크롤링입니다.\n",
    "    연관키워드: 데이터 수집, 웹 스크래핑, 검색 엔진\n",
    "\n",
    "    Word2Vec\n",
    "    ==================================================\n",
    "\n",
    "    ============================================================\n",
    "    🧠 LLMChainExtractor 압축 후 결과:\n",
    "    ============================================================\n",
    "    ✅ Contextual Compression 완료!\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d923c31",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4f1924",
   "metadata": {},
   "source": [
    "#### **5) `LLM을 활용한 문서 필터링`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02023431",
   "metadata": {},
   "source": [
    "* **`LLM Chain Filter`**\n",
    "\n",
    "  *  초기에 검색된 문서 중 어떤 문서를 필터링하고, 어떤 문서를 반환할지 결정하기 위해 **`LLM 체인` 을 사용**\n",
    "\n",
    "  *  보다 단순하지만 강력한 압축기\n",
    "\n",
    "  *  문서 내용을 **`변경` or `압축`하지 않고 문서를 `선택적으로 반환`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b8f413",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainFilter\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# LLM 사용해 LLM Chain Filter 객체 생성하기\n",
    "_filter =  LLMChainFilter.from_llm(gemini_lc)\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    # LLMChainFilter와 retriever를 사용하여 ContextualCompressionRetriever 객체 생성하기\n",
    "    base_compressor=_filter,\n",
    "    base_retriever=retriever,\n",
    ")\n",
    "\n",
    "compressed_docs = compression_retriever.invoke(\n",
    "    # 쿼리\n",
    "    \"Semantic Search 에 대해서 알려줘.\"\n",
    ")\n",
    "\n",
    "# 압축된 문서를 예쁘게 출력해보기\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0121b8",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 셀 출력 (1.4s)\n",
    "\n",
    "    ```markdown\n",
    "    Document 1:\n",
    "    정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
    "    예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n",
    "    연관키워드: 자연어 처리, 벡터화, 딥러닝\n",
    "\n",
    "    Token\n",
    "    ==================================================\n",
    "\n",
    "    Document 2:\n",
    "    정의: FAISS는 페이스북에서 개발한 고속 유사성 검색 라이브러리로, 특히 대규모 벡터 집합에서 유사 벡터를 효과적으로 검색할 수 있도록 설계되었습니다.\n",
    "    예시: 수백만 개의 이미지 벡터 중에서 비슷한 이미지를 빠르게 찾는 데 FAISS가 사용될 수 있습니다.\n",
    "    연관키워드: 벡터 검색, 머신러닝, 데이터베이스 최적화\n",
    "\n",
    "    Open Source\n",
    "    ==================================================\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108e74f0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55d5e89",
   "metadata": {},
   "source": [
    "* **`EmbeddingFilter`**\n",
    "\n",
    "  * `EmbeddingFilter`: 문서, 쿼리 임베딩 → 쿼리와 충분히 유사한 임베딩을 가진 문서만 반환 \n",
    "    * `더 저렴, 빠른 옵션 제공`\n",
    "    * `검색 결과의 관련성을 유지하면서도 계산 비용과 시간을 절약할 수 있음`\n",
    "\n",
    "  * **`EmbeddingsFilter` + `ContextualCompressionRetriever`**\n",
    "    * `EmbeddingsFilter` 사용, **`유사도 임계값 (0.86) 이상인 문서 필터링`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac04244",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.document_compressors import EmbeddingsFilter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# 1단계: 임베딩 초기화\n",
    "embeddings = embeddings\n",
    "\n",
    "print(\"✅ HuggingFaceEmbeddings 초기화 완료\")                          # ✅ HuggingFaceEmbeddings 초기화 완료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8139a00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2단계: EmbeddingsFilter 객체 생성하기\n",
    "embeddings_filter = EmbeddingsFilter(\n",
    "    embeddings=embeddings,                                          # 임베딩 모델\n",
    "    similarity_threshold=0.86                                       # 유사도 임계값 = 0.86\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "947a33d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3단계: ContextualCompressionRetriever 객체 생성하기\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=embeddings_filter,                              # 기본 압축기 = embedding_filter\n",
    "    base_retriever=retriever                                        # 기본 검색기 = retrirever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3dcf7755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4단계: ContextualCompressionRetriever 객체를 사용하여 관련 문서 검색하기\n",
    "compressed_docs = compression_retriever.invoke(\n",
    "    # 쿼리\n",
    "    \"Semantic Search 에 대해서 알려줘.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "55adb5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5단계: 검색된 문서를 예쁘게 출력하기\n",
    "\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0872919",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 차원이 낮은 임베딩 모델, 높은 유사도 임계값 → 값이 출력되지 않음 \n",
    "\n",
    "* 유사도 임계값을 낮게 설정해서 다시 시도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab1e3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.document_compressors import EmbeddingsFilter\n",
    "from langchain_core.embeddings import FakeEmbeddings\n",
    "\n",
    "# 1단계: 허깅페이스 Embeddings 사용\n",
    "embeddings = embeddings\n",
    "\n",
    "# 2단계: EmbeddingsFilter 객체 생성하기\n",
    "embeddings_filter = EmbeddingsFilter(\n",
    "    embeddings=embeddings,                                          # 임베딩 모델\n",
    "    similarity_threshold=0.01                                       # 유사도 임계값 = 0.01\n",
    "    )\n",
    "\n",
    "# 3단계: ContextualCompressionRetriever 객체 생성하기\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=embeddings_filter,                              # 기본 압축기 = embedding_filter\n",
    "    base_retriever=retriever                                        # 기본 검색기 = retrirever\n",
    ")\n",
    "\n",
    "# 4단계: ContextualCompressionRetriever 객체를 사용하여 관련 문서 검색하기\n",
    "compressed_docs = compression_retriever.invoke(\n",
    "    \"Semantic Search 에 대해서 알려줘.\"                                 # 쿼리\n",
    ")\n",
    "\n",
    "# 5단계: 검색된 문서를 예쁘게 출력하기\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d9fea9",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 셀 출력\n",
    "\n",
    "    ```markdown\n",
    "    Document 1:\n",
    "    정의: SQL(Structured Query Language)은 데이터베이스에서 데이터를 관리하기 위한 프로그래밍 언어입니다. 데이터 조회, 수정, 삽입, 삭제 등 다양한 작업을 수행할 수 있습니다.\n",
    "    예시: SELECT * FROM users WHERE age > 18;은 18세 이상의 사용자 정보를 조회합니다.\n",
    "    연관키워드: 데이터베이스, 쿼리, 데이터 관리\n",
    "\n",
    "    CSV\n",
    "    ==================================================\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c864b8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.document_compressors import EmbeddingsFilter\n",
    "from langchain_core.embeddings import FakeEmbeddings\n",
    "\n",
    "# 1단계: Fake Embeddings 사용\n",
    "embeddings = FakeEmbeddings(size=384)                               # 384차원 가짜 임베딩\n",
    "\n",
    "# 2단계: EmbeddingsFilter 객체 생성하기\n",
    "embeddings_filter = EmbeddingsFilter(\n",
    "    embeddings=embeddings,                                          # 임베딩 모델\n",
    "    similarity_threshold=0.01                                       # 유사도 임계값 = 0.01\n",
    "    )\n",
    "\n",
    "# 3단계: ContextualCompressionRetriever 객체 생성하기\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=embeddings_filter,                              # 기본 압축기 = embedding_filter\n",
    "    base_retriever=retriever                                        # 기본 검색기 = retrirever\n",
    ")\n",
    "\n",
    "# 4단계: ContextualCompressionRetriever 객체를 사용하여 관련 문서 검색하기\n",
    "compressed_docs = compression_retriever.invoke(\n",
    "    \"Semantic Search 에 대해서 알려줘.\"                                 # 쿼리\n",
    ")\n",
    "\n",
    "# 5단계: 검색된 문서를 예쁘게 출력하기\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50378826",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 셀 출력 (**`최대 유사도 임계값 = 0.08`**)\n",
    "\n",
    "    ```markdown\n",
    "    Document 1:\n",
    "    정의: JSON(JavaScript Object Notation)은 경량의 데이터 교환 형식으로, 사람과 기계 모두에게 읽기 쉬운 텍스트를 사용하여 데이터 객체를 표현합니다.\n",
    "    예시: {\"이름\": \"홍길동\", \"나이\": 30, \"직업\": \"개발자\"}는 JSON 형식의 데이터입니다.\n",
    "    연관키워드: 데이터 교환, 웹 개발, API\n",
    "\n",
    "    Transformer\n",
    "    ==================================================\n",
    "    Document 2:\n",
    "    정의: 토크나이저는 텍스트 데이터를 토큰으로 분할하는 도구입니다. 이는 자연어 처리에서 데이터를 전처리하는 데 사용됩니다.\n",
    "    예시: \"I love programming.\"이라는 문장을 [\"I\", \"love\", \"programming\", \".\"]으로 분할합니다.\n",
    "    연관키워드: 토큰화, 자연어 처리, 구문 분석\n",
    "\n",
    "    VectorStore\n",
    "    ==================================================\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d599eb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1e7b26",
   "metadata": {},
   "source": [
    "#### **6) `파이프라인 생성`** (`압축기 + 문서 변환기`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea20f8db",
   "metadata": {},
   "source": [
    "* **`DocumentCompressionPipeline`** → 여러 `compressor`를 순차적으로 결합 가능\n",
    "\n",
    "  * `Compressor`와 함께 **`BaseDocumentTransformer`** 를 파이프라인에 추가할 수 있음\n",
    "  * **`맥락적 압축을 수행하지 않고 단순한 문서 집합에 대한 변환 수행`**\n",
    "\n",
    "  <br>\n",
    "\n",
    "  * **`TextSplitter`** = 더 작은 조각으로 분할하기 위해 `document transformer`로 사용 가능\n",
    "  * **`EmbeddingsRedundantFilter`** = **`문서 간 임베딩 유사성 (기본값 = 0.95 유사도 이상을 중복 문서로 간주) 을 기반으로 중복 문서를 필터링하는 데 사용`**\n",
    "\n",
    "  <br>\n",
    "\n",
    "* 순서: 문서를 더 작은 청크로 분할 → 중복 문서 제거 → 쿼리와의 관련성 기준으로 필터링 → `compressor pipeline` 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1addb007",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
    "from langchain_community.document_transformers import EmbeddingsRedundantFilter\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# 1단계: 문자 기반 텍스트 분할기 생성하기\n",
    "splitter = CharacterTextSplitter(\n",
    "    chunk_size=300,                             # 청크 크기 = 300\n",
    "    chunk_overlap=0                             # 청크 간 중복 X \n",
    "    )\n",
    "\n",
    "# 2단계: 중복 필터 생성하기\n",
    "redundant_filter = EmbeddingsRedundantFilter(embeddings=embeddings)     # 임베딩 사용\n",
    "\n",
    "# 3단계: 관련성 필터 생성하기\n",
    "relevant_filter = EmbeddingsFilter(\n",
    "    embeddings=embeddings,                      # 임베딩 사용\n",
    "    similarity_threshold=0.86                   # 유사도 임계값을 0.86으로 설정\n",
    ")\n",
    "\n",
    "# 4단계: 문서 압축 파이프라인 생성하기\n",
    "pipeline_compressor = DocumentCompressorPipeline(\n",
    "    # 변환기로 설정하기\n",
    "    transformers = [\n",
    "        splitter,                               # 텍스트 분할기\n",
    "        redundant_filter,                       # 중복 필터\n",
    "        relevant_filter,                        # 관련성 필터\n",
    "        LLMChainExtractor.from_llm(gemini_lc)   # LLM\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644c8ab5",
   "metadata": {},
   "source": [
    "* **`ContextualCompressionRetriever`** 초기화\n",
    "\n",
    "* **`base_compressor`** = **`pipeline_compressor`**\n",
    "\n",
    "* **`base_retriever`** = **`retriever`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6150cc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5단계: ContextualCompressionRetriever 객체 생성하기\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=pipeline_compressor,\n",
    "    base_retriever=retriever,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "aec3b584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6단계: ContextualCompressionRetriever 객체를 사용하여 관련 문서 검색하기\n",
    "\n",
    "compressed_docs = compression_retriever.invoke(\n",
    "    \"Semantic Search 에 대해서 알려줘.\"                                 # 쿼리\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a9af9a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7단계: 검색된 문서를 예쁘게 출력하기\n",
    "\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c79d71",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 유사도 임계값의 차이로 인해 결과값이 나오지 않음\n",
    "\n",
    "* 임베딩 모델들의 유사도 범위\n",
    "  * `OpenAI_embeddings`의 유사도: **`0.3 ~ 0.95`**\n",
    "  * `HuggingFace_embeddings`의 유사도: **`0.2 ~ 0.90`**\n",
    "  * `Google_Gemini`의 유사도: **`0.4 ~ 0.85`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac244126",
   "metadata": {},
   "source": [
    "* 시도_1 : 허깅페이스 임베딩 모델 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba30ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
    "from langchain_community.document_transformers import EmbeddingsRedundantFilter\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# 1단계: 문자 기반 텍스트 분할기 생성하기\n",
    "splitter = CharacterTextSplitter(\n",
    "    chunk_size=300,                             # 청크 크기 = 300\n",
    "    chunk_overlap=0                             # 청크 간 중복 X \n",
    "    )\n",
    "\n",
    "# 2단계: 중복 필터 생성하기\n",
    "redundant_filter = EmbeddingsRedundantFilter(embeddings=embeddings)     # 임베딩 사용\n",
    "\n",
    "# 3단계: 관련성 필터 생성하기\n",
    "relevant_filter = EmbeddingsFilter(\n",
    "    embeddings=embeddings,                       # 임베딩 사용\n",
    "    similarity_threshold=0.009                   # 유사도 임계값을 0.009로 설정\n",
    ")\n",
    "\n",
    "# 4단계: 문서 압축 파이프라인 생성하기\n",
    "pipeline_compressor = DocumentCompressorPipeline(\n",
    "    # 변환기로 설정하기\n",
    "    transformers = [\n",
    "        splitter,                               # 텍스트 분할기\n",
    "        redundant_filter,                       # 중복 필터\n",
    "        relevant_filter,                        # 관련성 필터\n",
    "        LLMChainExtractor.from_llm(gemini_lc)   # LLM\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 5단계: ContextualCompressionRetriever 객체 생성하기\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=pipeline_compressor,\n",
    "    base_retriever=retriever,\n",
    ")\n",
    "\n",
    "# 6단계: ContextualCompressionRetriever 객체를 사용하여 관련 문서 검색하기\n",
    "compressed_docs = compression_retriever.invoke(\n",
    "    \"Semantic Search 에 대해서 알려줘.\"                                 # 쿼리\n",
    ")\n",
    "\n",
    "# 7단계: 검색된 문서를 예쁘게 출력하기\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae7715f",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 셀 출력 (2.6s)\n",
    "\n",
    "    ```markdown\n",
    "    Document 1:\n",
    "    Semantic Search\n",
    "\n",
    "    정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
    "    예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
    "    연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
    "    ==================================================\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219b5b2e",
   "metadata": {},
   "source": [
    "* 시도_2 : 허깅페이스 fake 임베딩 모델 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c719109",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
    "from langchain_community.document_transformers import EmbeddingsRedundantFilter\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_core.embeddings import FakeEmbeddings\n",
    "\n",
    "# 1단계: 문자 기반 텍스트 분할기 생성하기\n",
    "splitter = CharacterTextSplitter(\n",
    "    chunk_size=300,                             # 청크 크기 = 300\n",
    "    chunk_overlap=0                             # 청크 간 중복 X \n",
    "    )\n",
    "\n",
    "# 2단계: 중복 필터 생성하기\n",
    "redundant_filter = EmbeddingsRedundantFilter(embeddings=embeddings)     # 임베딩 사용\n",
    "\n",
    "# 3단계: 관련성 필터 생성하기\n",
    "relevant_filter = EmbeddingsFilter(\n",
    "    embeddings=FakeEmbeddings(size=384),         # fake 임베딩 사용\n",
    "    similarity_threshold=0.009                   # 유사도 임계값을 0.009로 설정\n",
    ")\n",
    "\n",
    "# 4단계: 문서 압축 파이프라인 생성하기\n",
    "pipeline_compressor = DocumentCompressorPipeline(\n",
    "    # 변환기로 설정하기\n",
    "    transformers = [\n",
    "        splitter,                               # 텍스트 분할기\n",
    "        redundant_filter,                       # 중복 필터\n",
    "        relevant_filter,                        # 관련성 필터\n",
    "        LLMChainExtractor.from_llm(gemini_lc)   # LLM\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 5단계: ContextualCompressionRetriever 객체 생성하기\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=pipeline_compressor,\n",
    "    base_retriever=retriever,\n",
    ")\n",
    "\n",
    "# 6단계: ContextualCompressionRetriever 객체를 사용하여 관련 문서 검색하기\n",
    "compressed_docs = compression_retriever.invoke(\n",
    "    \"Semantic Search 에 대해서 알려줘.\"                                 # 쿼리\n",
    ")\n",
    "\n",
    "# 7단계: 검색된 문서를 예쁘게 출력하기\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6e6bde",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 셀 출력 (1.3s)\n",
    "\n",
    "    ```markdown\n",
    "    Document 1:\n",
    "    Semantic Search\n",
    "\n",
    "    정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
    "    예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
    "    연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
    "    ==================================================\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1dd9cd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720a7bf8",
   "metadata": {},
   "source": [
    "#### **7) `유동적 유사도 분포 분석`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559236e7",
   "metadata": {},
   "source": [
    "* 실시간 유사도 분포 분석해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ead855",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.retrievers.document_compressors import EmbeddingsFilter\n",
    "from typing import List, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class SimilarityThresholdCalculator:\n",
    "    \"\"\"유사도 임계값 자동 계산기\"\"\"\n",
    "    \n",
    "    def __init__(self, embeddings, retriever):\n",
    "        self.embeddings = embeddings\n",
    "        self.retriever = retriever\n",
    "        \n",
    "    def analyze_similarity_distribution(self, query: str, top_k: int = 10) -> dict:\n",
    "        \"\"\"쿼리에 대한 유사도 분포 분석\"\"\"\n",
    "        \n",
    "        print(f\"🔍 쿼리 분석: '{query}'\")\n",
    "        \n",
    "        # 기본 검색 결과\n",
    "        docs = self.retriever.invoke(query)\n",
    "        if not docs:\n",
    "            print(\"❌ 검색된 문서가 없습니다.\")\n",
    "            return {}\n",
    "        \n",
    "        # 쿼리 임베딩\n",
    "        query_embedding = self.embeddings.embed_query(query)\n",
    "        \n",
    "        # 각 문서의 유사도 계산\n",
    "        similarities = []\n",
    "        for doc in docs:\n",
    "            doc_embedding = self.embeddings.embed_documents([doc.page_content])[0]\n",
    "            \n",
    "            # 코사인 유사도 계산\n",
    "            similarity = self._cosine_similarity(query_embedding, doc_embedding)\n",
    "            similarities.append(similarity)\n",
    "            \n",
    "            print(f\"  📄 유사도: {similarity:.4f} | {doc.page_content[:50]}...\")\n",
    "        \n",
    "        # 통계 계산\n",
    "        similarities = np.array(similarities)\n",
    "        stats = {\n",
    "            'similarities': similarities,\n",
    "            'mean': np.mean(similarities),\n",
    "            'std': np.std(similarities),\n",
    "            'min': np.min(similarities),\n",
    "            'max': np.max(similarities),\n",
    "            'median': np.median(similarities),\n",
    "            'q25': np.percentile(similarities, 25),\n",
    "            'q75': np.percentile(similarities, 75)\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n📊 유사도 통계:\")\n",
    "        print(f\"  평균: {stats['mean']:.4f}\")\n",
    "        print(f\"  표준편차: {stats['std']:.4f}\")\n",
    "        print(f\"  최소값: {stats['min']:.4f}\")\n",
    "        print(f\"  최대값: {stats['max']:.4f}\")\n",
    "        print(f\"  중간값: {stats['median']:.4f}\")\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def _cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:\n",
    "        \"\"\"코사인 유사도 계산\"\"\"\n",
    "        vec1, vec2 = np.array(vec1), np.array(vec2)\n",
    "        dot_product = np.dot(vec1, vec2)\n",
    "        norms = np.linalg.norm(vec1) * np.linalg.norm(vec2)\n",
    "        return dot_product / norms if norms != 0 else 0\n",
    "    \n",
    "    def suggest_optimal_threshold(self, query: str, target_retention: float = 0.5) -> float:\n",
    "        \"\"\"최적 임계값 제안 (target_retention: 유지하고 싶은 문서 비율)\"\"\"\n",
    "        \n",
    "        stats = self.analyze_similarity_distribution(query)\n",
    "        if not stats:\n",
    "            return 0.01\n",
    "        \n",
    "        similarities = stats['similarities']\n",
    "        \n",
    "        # 방법 1: 분위수 기반\n",
    "        threshold_percentile = (1 - target_retention) * 100\n",
    "        threshold_quantile = np.percentile(similarities, threshold_percentile)\n",
    "        \n",
    "        # 방법 2: 평균 - n*표준편차 기반  \n",
    "        threshold_std = stats['mean'] - 0.5 * stats['std']\n",
    "        \n",
    "        # 방법 3: 안전 임계값 (최소값의 80%)\n",
    "        threshold_safe = stats['min'] * 0.8\n",
    "        \n",
    "        # 세 방법 중 중간값 선택\n",
    "        candidates = [threshold_quantile, threshold_std, threshold_safe]\n",
    "        optimal_threshold = np.median(candidates)\n",
    "        \n",
    "        # 최소 0.001, 최대 0.95 제한\n",
    "        optimal_threshold = max(0.001, min(0.95, optimal_threshold))\n",
    "        \n",
    "        print(f\"\\n🎯 추천 임계값:\")\n",
    "        print(f\"  분위수 기반: {threshold_quantile:.4f}\")\n",
    "        print(f\"  표준편차 기반: {threshold_std:.4f}\")\n",
    "        print(f\"  안전 기반: {threshold_safe:.4f}\")\n",
    "        print(f\"  ✅ 최종 추천: {optimal_threshold:.4f}\")\n",
    "        \n",
    "        return optimal_threshold\n",
    "    \n",
    "    def test_threshold_performance(self, query: str, thresholds: List[float]) -> dict:\n",
    "        \"\"\"다양한 임계값 성능 테스트\"\"\"\n",
    "        \n",
    "        base_docs = self.retriever.invoke(query)\n",
    "        base_count = len(base_docs)\n",
    "        \n",
    "        print(f\"\\n🧪 임계값 성능 테스트 (기준: {base_count}개 문서)\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for threshold in thresholds:\n",
    "            try:\n",
    "                # EmbeddingsFilter 생성\n",
    "                embeddings_filter = EmbeddingsFilter(\n",
    "                    embeddings=self.embeddings,\n",
    "                    similarity_threshold=threshold\n",
    "                )\n",
    "                \n",
    "                # 필터링 테스트\n",
    "                filtered_docs = embeddings_filter.compress_documents(base_docs, query)\n",
    "                filtered_count = len(filtered_docs)\n",
    "                retention_rate = filtered_count / base_count if base_count > 0 else 0\n",
    "                \n",
    "                results[threshold] = {\n",
    "                    'filtered_count': filtered_count,\n",
    "                    'retention_rate': retention_rate\n",
    "                }\n",
    "                \n",
    "                print(f\"임계값 {threshold:6.3f}: {filtered_count:2d}개 ({retention_rate:5.1%})\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"임계값 {threshold:6.3f}: ❌ 오류 - {e}\")\n",
    "                results[threshold] = {'filtered_count': 0, 'retention_rate': 0}\n",
    "        \n",
    "        return results\n",
    "\n",
    "# 🎬 사용 예시\n",
    "def auto_threshold_pipeline():\n",
    "    \"\"\"자동 임계값 계산 파이프라인\"\"\"\n",
    "    \n",
    "    # 설정\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        model_kwargs={'device': 'cpu'}\n",
    "    )\n",
    "    \n",
    "    # 기존 retriever 사용 (이미 생성된 것)\n",
    "    # retriever = your_existing_retriever\n",
    "    \n",
    "    # 계산기 생성\n",
    "    calc = SimilarityThresholdCalculator(embeddings, retriever)\n",
    "    \n",
    "    query = \"Semantic Search 에 대해서 알려줘.\"\n",
    "    \n",
    "    # 1단계: 유사도 분포 분석\n",
    "    stats = calc.analyze_similarity_distribution(query)\n",
    "    \n",
    "    # 2단계: 최적 임계값 제안\n",
    "    optimal_threshold = calc.suggest_optimal_threshold(query, target_retention=0.6)\n",
    "    \n",
    "    # 3단계: 다양한 임계값 테스트\n",
    "    test_thresholds = [0.001, 0.01, 0.05, 0.1, 0.2, 0.3, 0.5, 0.7, optimal_threshold]\n",
    "    results = calc.test_threshold_performance(query, sorted(set(test_thresholds)))\n",
    "    \n",
    "    return optimal_threshold\n",
    "\n",
    "# 실행\n",
    "optimal_threshold = auto_threshold_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf68fcb7",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 셀 출력 (5.1s)\n",
    "\n",
    "    ```markdown\n",
    "    🔍 쿼리 분석: 'Semantic Search 에 대해서 알려줘.'\n",
    "    📄 유사도: 0.4923 | Semantic Search\n",
    "\n",
    "    정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을...\n",
    "    📄 유사도: 0.3349 | 정의: 토크나이저는 텍스트 데이터를 토큰으로 분할하는 도구입니다. 이는 자연어 처리에서 데...\n",
    "    📄 유사도: 0.3063 | 정의: JSON(JavaScript Object Notation)은 경량의 데이터 교환 형...\n",
    "    📄 유사도: 0.2875 | 정의: SQL(Structured Query Language)은 데이터베이스에서 데이터를 ...\n",
    "\n",
    "    📊 유사도 통계:\n",
    "    평균: 0.3553\n",
    "    표준편차: 0.0809\n",
    "    최소값: 0.2875\n",
    "    최대값: 0.4923\n",
    "    중간값: 0.3206\n",
    "    🔍 쿼리 분석: 'Semantic Search 에 대해서 알려줘.'\n",
    "    📄 유사도: 0.4923 | Semantic Search\n",
    "\n",
    "    정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을...\n",
    "    📄 유사도: 0.3349 | 정의: 토크나이저는 텍스트 데이터를 토큰으로 분할하는 도구입니다. 이는 자연어 처리에서 데...\n",
    "    📄 유사도: 0.3063 | 정의: JSON(JavaScript Object Notation)은 경량의 데이터 교환 형...\n",
    "    📄 유사도: 0.2875 | 정의: SQL(Structured Query Language)은 데이터베이스에서 데이터를 ...\n",
    "\n",
    "    📊 유사도 통계:\n",
    "    평균: 0.3553\n",
    "    표준편차: 0.0809\n",
    "    최소값: 0.2875\n",
    "    최대값: 0.4923\n",
    "    중간값: 0.3206\n",
    "\n",
    "    🎯 추천 임계값:\n",
    "    분위수 기반: 0.3120\n",
    "    표준편차 기반: 0.3148\n",
    "    안전 기반: 0.2300\n",
    "    ✅ 최종 추천: 0.3120\n",
    "\n",
    "    🧪 임계값 성능 테스트 (기준: 4개 문서)\n",
    "    ============================================================\n",
    "    임계값  0.001:  4개 (100.0%)\n",
    "    임계값  0.010:  4개 (100.0%)\n",
    "    임계값  0.050:  4개 (100.0%)\n",
    "    임계값  0.100:  4개 (100.0%)\n",
    "    임계값  0.200:  4개 (100.0%)\n",
    "    임계값  0.300:  3개 (75.0%)\n",
    "    임계값  0.312:  2개 (50.0%)\n",
    "    임계값  0.500:  0개 ( 0.0%)\n",
    "    임계값  0.700:  0개 ( 0.0%)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c21c58",
   "metadata": {},
   "source": [
    "* 임계값 자동 설정 함수 만들어보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3233e1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.document_compressors import DocumentCompressorPipeline, EmbeddingsFilter\n",
    "from langchain_community.document_transformers import EmbeddingsRedundantFilter\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "\n",
    "def create_auto_threshold_pipeline(embeddings, retriever, gemini_lc, query_sample=\"Semantic Search\"):\n",
    "    \"\"\"자동 임계값 계산으로 파이프라인 생성\"\"\"\n",
    "    \n",
    "    print(\"🤖 자동 임계값 계산 시작...\")\n",
    "    \n",
    "    # 1단계: 임계값 자동 계산\n",
    "    calc = SimilarityThresholdCalculator(embeddings, retriever)\n",
    "    optimal_threshold = calc.suggest_optimal_threshold(query_sample, target_retention=0.4)\n",
    "    \n",
    "    print(f\"✅ 계산된 최적 임계값: {optimal_threshold:.4f}\")\n",
    "    \n",
    "    # 2단계: 컴포넌트 생성\n",
    "    splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0)\n",
    "    redundant_filter = EmbeddingsRedundantFilter(embeddings=embeddings)\n",
    "    relevant_filter = EmbeddingsFilter(\n",
    "        embeddings=embeddings,\n",
    "        similarity_threshold=optimal_threshold  # 자동 계산된 값 사용!\n",
    "    )\n",
    "    \n",
    "    # 3단계: 파이프라인 생성\n",
    "    try:\n",
    "        from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "        llm_extractor = LLMChainExtractor.from_llm(gemini_lc)\n",
    "        \n",
    "        pipeline_compressor = DocumentCompressorPipeline(\n",
    "            transformers=[\n",
    "                splitter,\n",
    "                redundant_filter,\n",
    "                relevant_filter,\n",
    "                llm_extractor\n",
    "            ]\n",
    "        )\n",
    "    except ImportError:\n",
    "        # LLMChainExtractor가 없으면 제외\n",
    "        pipeline_compressor = DocumentCompressorPipeline(\n",
    "            transformers=[\n",
    "                splitter,\n",
    "                redundant_filter,\n",
    "                relevant_filter,\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    # 4단계: 최종 검색기 생성\n",
    "    compression_retriever = ContextualCompressionRetriever(\n",
    "        base_compressor=pipeline_compressor,\n",
    "        base_retriever=retriever,\n",
    "    )\n",
    "    \n",
    "    print(\"🎉 자동 최적화 파이프라인 생성 완료!\")\n",
    "    return compression_retriever, optimal_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c229eb",
   "metadata": {},
   "source": [
    "* 적응형 임계값 필터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0098ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveThresholdFilter:\n",
    "    \"\"\"임베딩 모델별 적응형 임계값 필터\"\"\"\n",
    "    \n",
    "    # 임베딩 모델별 기본 임계값 데이터베이스\n",
    "    MODEL_THRESHOLDS = {\n",
    "        'sentence-transformers/all-MiniLM-L6-v2': 0.15,\n",
    "        'sentence-transformers/all-MiniLM-L12-v2': 0.20,\n",
    "        'sentence-transformers/paraphrase-MiniLM-L3-v2': 0.12,\n",
    "        'text-embedding-3-small': 0.75,                 # OpenAI\n",
    "        'text-embedding-3-large': 0.80,                 # OpenAI  \n",
    "        'models/embedding-001': 0.70,                   # Google\n",
    "        'fake': 0.01,                                   # FakeEmbeddings\n",
    "    }\n",
    "    \n",
    "    def __init__(self, embeddings):\n",
    "        self.embeddings = embeddings\n",
    "        self.model_name = self._detect_model_name()\n",
    "        \n",
    "    def _detect_model_name(self) -> str:\n",
    "        \"\"\"임베딩 모델명 감지\"\"\"\n",
    "        embedding_type = str(type(self.embeddings))\n",
    "        \n",
    "        if 'Fake' in embedding_type:\n",
    "            return 'fake'\n",
    "        elif hasattr(self.embeddings, 'model_name'):\n",
    "            return self.embeddings.model_name\n",
    "        elif 'OpenAI' in embedding_type:\n",
    "            return 'text-embedding-3-small'\n",
    "        elif 'Google' in embedding_type:\n",
    "            return 'models/embedding-001'\n",
    "        else:\n",
    "            return 'sentence-transformers/all-MiniLM-L6-v2'  # 기본값\n",
    "    \n",
    "    def get_recommended_threshold(self) -> float:\n",
    "        \"\"\"모델별 추천 임계값 반환\"\"\"\n",
    "        threshold = self.MODEL_THRESHOLDS.get(self.model_name, 0.15)\n",
    "        print(f\"🎯 모델 '{self.model_name}' 추천 임계값: {threshold}\")\n",
    "        return threshold\n",
    "    \n",
    "    def create_filter(self, custom_threshold: float = None) -> EmbeddingsFilter:\n",
    "        \"\"\"최적화된 EmbeddingsFilter 생성\"\"\"\n",
    "        threshold = custom_threshold if custom_threshold else self.get_recommended_threshold()\n",
    "        \n",
    "        return EmbeddingsFilter(\n",
    "            embeddings=self.embeddings,\n",
    "            similarity_threshold=threshold\n",
    "        )\n",
    "\n",
    "# 🎬 Jay를 위한 완벽한 사용법\n",
    "def jay_optimized_pipeline(embeddings, retriever, gemini_lc):\n",
    "    \"\"\"Jay 전용 최적화 파이프라인\"\"\"\n",
    "    \n",
    "    print(\"🎯 Jay 전용 최적화 파이프라인 시작...\")\n",
    "    \n",
    "    # 1단계: 적응형 임계값 계산\n",
    "    adaptive_filter = AdaptiveThresholdFilter(embeddings)\n",
    "    optimal_threshold = adaptive_filter.get_recommended_threshold()\n",
    "    \n",
    "    # 2단계: 실시간 검증 (선택사항)\n",
    "    query_test = \"Semantic Search 에 대해서 알려줘.\"\n",
    "    calc = SimilarityThresholdCalculator(embeddings, retriever)\n",
    "    \n",
    "    # 빠른 검증\n",
    "    test_results = calc.test_threshold_performance(query_test, [optimal_threshold])\n",
    "    retention_rate = test_results[optimal_threshold]['retention_rate']\n",
    "    \n",
    "    # 임계값 조정 (보정)\n",
    "    if retention_rate < 0.1:                                        # 너무 적게 통과\n",
    "        optimal_threshold *= 0.5\n",
    "        print(f\"⚠️ 임계값 너무 높음. 조정: {optimal_threshold:.4f}\")\n",
    "    elif retention_rate > 0.9:                                      # 너무 많이 통과\n",
    "        optimal_threshold *= 1.5\n",
    "        print(f\"⚠️ 임계값 너무 낮음. 조정: {optimal_threshold:.4f}\")\n",
    "    \n",
    "    print(f\"✅ 최종 최적화된 임계값: {optimal_threshold:.4f}\")\n",
    "    \n",
    "    # 3단계: 파이프라인 구성\n",
    "    splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0)\n",
    "    redundant_filter = EmbeddingsRedundantFilter(embeddings=embeddings)\n",
    "    relevant_filter = EmbeddingsFilter(\n",
    "        embeddings=embeddings,\n",
    "        similarity_threshold=optimal_threshold\n",
    "    )\n",
    "    \n",
    "    # LLM Extractor 추가 (가능하면)\n",
    "    transformers = [splitter, redundant_filter, relevant_filter]\n",
    "    \n",
    "    try:\n",
    "        from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "        llm_extractor = LLMChainExtractor.from_llm(gemini_lc)\n",
    "        transformers.append(llm_extractor)\n",
    "        print(\"✅ LLM 압축기 추가됨\")\n",
    "    except:\n",
    "        print(\"⚠️ LLM 압축기 제외 (기본 필터만 사용)\")\n",
    "    \n",
    "    # 4단계: 최종 파이프라인\n",
    "    pipeline_compressor = DocumentCompressorPipeline(transformers=transformers)\n",
    "    \n",
    "    compression_retriever = ContextualCompressionRetriever(\n",
    "        base_compressor=pipeline_compressor,\n",
    "        base_retriever=retriever,\n",
    "    )\n",
    "    \n",
    "    return compression_retriever, optimal_threshold\n",
    "\n",
    "# 🎬 실행 예시\n",
    "compression_retriever, threshold = jay_optimized_pipeline(embeddings, retriever, gemini_lc)\n",
    "print(f\"🎉 최적화 완료! 사용된 임계값: {threshold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35dae56",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 셀 출력\n",
    "\n",
    "    ```markdown\n",
    "    🎯 Jay 전용 최적화 파이프라인 시작...\n",
    "    🎯 모델 'fake' 추천 임계값: 0.01\n",
    "\n",
    "    🧪 임계값 성능 테스트 (기준: 4개 문서)\n",
    "    ============================================================\n",
    "    임계값  0.010:  2개 (50.0%)\n",
    "    ✅ 최종 최적화된 임계값: 0.0100\n",
    "    ✅ LLM 압축기 추가됨\n",
    "    🎉 최적화 완료! 사용된 임계값: 0.01\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fa25f4",
   "metadata": {},
   "source": [
    "* 유사도 임계값 계산 자동화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "912da475",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_click_contextual_compression(embeddings, retriever, gemini_lc, query=\"Semantic Search\"):\n",
    "    \"\"\"원클릭 완전 자동화 시스템\"\"\"\n",
    "    \n",
    "    print(\"🚀 원클릭 자동 최적화 시작!\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # 1단계: 임베딩 모델 분석\n",
    "    model_type = str(type(embeddings))\n",
    "    print(f\"🔍 감지된 모델: {model_type}\")\n",
    "    \n",
    "    # 2단계: 기본 임계값 추정\n",
    "    if 'Fake' in model_type:\n",
    "        base_threshold = 0.01\n",
    "        print(\"  📝 FakeEmbeddings 감지 → 초저 임계값 모드\")\n",
    "    elif 'HuggingFace' in model_type:\n",
    "        base_threshold = 0.15\n",
    "        print(\"  🤗 HuggingFace 모델 감지 → 중간 임계값 모드\") \n",
    "    elif 'OpenAI' in model_type:\n",
    "        base_threshold = 0.75\n",
    "        print(\"  🤖 OpenAI 모델 감지 → 고 임계값 모드\")\n",
    "    elif 'Google' in model_type:\n",
    "        base_threshold = 0.70\n",
    "        print(\"  🌟 Google 모델 감지 → 고 임계값 모드\")\n",
    "    else:\n",
    "        base_threshold = 0.20\n",
    "        print(\"  ❓ 알 수 없는 모델 → 기본 임계값 모드\")\n",
    "    \n",
    "    # 3단계: 실시간 검증 및 조정\n",
    "    print(f\"\\n🧪 임계값 {base_threshold} 검증 중...\")\n",
    "    \n",
    "    # 테스트 쿼리로 성능 확인\n",
    "    base_docs = retriever.invoke(query)\n",
    "    if not base_docs:\n",
    "        print(\"❌ 검색 결과 없음 - 기본 설정 사용\")\n",
    "        final_threshold = base_threshold\n",
    "    else:\n",
    "        # 임계값 테스트\n",
    "        test_filter = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=base_threshold)\n",
    "        filtered_docs = test_filter.compress_documents(base_docs, query)\n",
    "        retention_rate = len(filtered_docs) / len(base_docs)\n",
    "        \n",
    "        print(f\"  📊 테스트 결과: {len(base_docs)}개 → {len(filtered_docs)}개 ({retention_rate:.1%})\")\n",
    "        \n",
    "        # 동적 조정\n",
    "        if retention_rate < 0.1:\n",
    "            final_threshold = base_threshold * 0.3\n",
    "            print(f\"  ⬇️ 임계값 낮춤: {final_threshold:.4f}\")\n",
    "        elif retention_rate > 0.9:\n",
    "            final_threshold = base_threshold * 1.5\n",
    "            print(f\"  ⬆️ 임계값 높임: {final_threshold:.4f}\")\n",
    "        else:\n",
    "            final_threshold = base_threshold\n",
    "            print(f\"  ✅ 임계값 적정: {final_threshold:.4f}\")\n",
    "    \n",
    "    # 4단계: 파이프라인 자동 구성\n",
    "    print(f\"\\n🔧 파이프라인 구성 중...\")\n",
    "    \n",
    "    transformers = [\n",
    "        CharacterTextSplitter(chunk_size=300, chunk_overlap=0),\n",
    "        EmbeddingsRedundantFilter(embeddings=embeddings),\n",
    "        EmbeddingsFilter(embeddings=embeddings, similarity_threshold=final_threshold)\n",
    "    ]\n",
    "    \n",
    "    # LLM 압축기 추가 시도\n",
    "    try:\n",
    "        from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "        transformers.append(LLMChainExtractor.from_llm(gemini_lc))\n",
    "        print(\"  ✅ LLM 압축기 추가\")\n",
    "    except:\n",
    "        print(\"  ⚠️ LLM 압축기 생략\")\n",
    "    \n",
    "    # 5단계: 최종 검색기 생성\n",
    "    pipeline = DocumentCompressorPipeline(transformers=transformers)\n",
    "    final_retriever = ContextualCompressionRetriever(\n",
    "        base_compressor=pipeline,\n",
    "        base_retriever=retriever\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n🎉 자동 최적화 완료!\")\n",
    "    print(f\"📊 최종 설정:\")\n",
    "    print(f\"  - 임계값: {final_threshold:.4f}\")\n",
    "    print(f\"  - 파이프라인 단계: {len(transformers)}개\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    return final_retriever, final_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a227f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎯 최종 사용법\n",
    "def pretty_print_docs(docs):\n",
    "    if docs:\n",
    "        for i, doc in enumerate(docs):\n",
    "            print(f\"Document {i+1}:\")\n",
    "            print(doc.page_content)\n",
    "            print(\"=\"*50)\n",
    "    else:\n",
    "        print(\"📭 필터링된 문서가 없습니다.\")\n",
    "\n",
    "# 실행 (기존 변수들 사용)\n",
    "compression_retriever, used_threshold = one_click_contextual_compression(\n",
    "    embeddings=embeddings,                          # 기존 embeddings\n",
    "    retriever=retriever,                            # 기존 retriever  \n",
    "    gemini_lc=gemini_lc,                            # 기존 gemini_lc\n",
    "    query=\"Semantic Search 에 대해서 알려줘.\"\n",
    ")\n",
    "\n",
    "# 테스트\n",
    "print(\"\\n🔍 최적화된 파이프라인 테스트: \\n\")\n",
    "compressed_docs = compression_retriever.invoke(\"Semantic Search 에 대해서 알려줘.\")\n",
    "pretty_print_docs(compressed_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c82f0b",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 셀 출력 (1.0s)\n",
    "\n",
    "    ```markdown\n",
    "    🚀 원클릭 자동 최적화 시작!\n",
    "    ==================================================\n",
    "    🔍 감지된 모델: <class 'langchain_core.embeddings.fake.FakeEmbeddings'>\n",
    "    📝 FakeEmbeddings 감지 → 초저 임계값 모드\n",
    "\n",
    "    🧪 임계값 0.01 검증 중...\n",
    "    📊 테스트 결과: 4개 → 2개 (50.0%)\n",
    "    ✅ 임계값 적정: 0.0100\n",
    "\n",
    "    🔧 파이프라인 구성 중...\n",
    "    ✅ LLM 압축기 추가\n",
    "\n",
    "    🎉 자동 최적화 완료!\n",
    "    📊 최종 설정:\n",
    "    - 임계값: 0.0100\n",
    "    - 파이프라인 단계: 4개\n",
    "    ==================================================\n",
    "\n",
    "    🔍 최적화된 파이프라인 테스트: \n",
    "\n",
    "    Document 1:\n",
    "    Semantic Search\n",
    "\n",
    "    정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
    "    예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
    "    연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
    "    ==================================================\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfa68ee",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9480ef",
   "metadata": {},
   "source": [
    "* *next: **`앙상블 검색기 (EnsembleRetriever)`***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c888d3dc",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
