# 10_Retriever/utils/korean_analyzer.py

# ========================================
# ğŸ† Korean Morphological Analyzer Class
# í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ í†µí•© ëª¨ë¸
# ========================================

from kiwipiepy import Kiwi
import math
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass

@dataclass
class TokenInfo:
    """í† í° ì •ë³´ë¥¼ ë‹´ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    form: str              # í˜•íƒœì†Œ
    tag: str               # í’ˆì‚¬ íƒœê·¸
    score: float           # ë¡œê·¸ í™•ë¥ 
    probability: float     # í™•ë¥  (0~1)
    start: int            # ì‹œì‘ ìœ„ì¹˜
    length: int           # ê¸¸ì´
    tagged_form: str      # í˜•íƒœì†Œ/í’ˆì‚¬

class KoreanMorphologicalAnalyzer:
    """
    í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ í†µí•© í´ë˜ìŠ¤
    
    Features:
    - í˜•íƒœì†Œ ë¶„ì„
    - ì‹ ë¢°ë„ ê¸°ë°˜ í•„í„°ë§
    - í’ˆì‚¬ë³„ í† í° ì¶”ì¶œ
    - ìƒì„¸ ì •ë³´ ì¡°íšŒ
    
    Examples:
        >>> analyzer = KoreanMorphologicalAnalyzer()
        >>> result = analyzer.analyze("ì•ˆë…•í•˜ì„¸ìš”")
        >>> keywords = analyzer.extract_keywords("Pythonì€ ì¢‹ì€ ì–¸ì–´ì…ë‹ˆë‹¤", confidence=-10.0)
    """
    
    # í’ˆì‚¬ íƒœê·¸ ì„¤ëª…
    POS_TAGS = {
        'NNG': 'ì¼ë°˜ ëª…ì‚¬',
        'NNP': 'ê³ ìœ  ëª…ì‚¬',
        'NNB': 'ì˜ì¡´ ëª…ì‚¬',
        'VV': 'ë™ì‚¬',
        'VA': 'í˜•ìš©ì‚¬',
        'MM': 'ê´€í˜•ì‚¬',
        'MAG': 'ì¼ë°˜ ë¶€ì‚¬',
        'XR': 'ì–´ê·¼',
        'SL': 'ì™¸êµ­ì–´',
        'SH': 'í•œì',
        'JKS': 'ì£¼ê²© ì¡°ì‚¬',
        'JKO': 'ëª©ì ê²© ì¡°ì‚¬',
        'JKB': 'ë¶€ì‚¬ê²© ì¡°ì‚¬',
        'EP': 'ì„ ì–´ë§ ì–´ë¯¸',
        'EF': 'ì¢…ê²° ì–´ë¯¸',
        'EC': 'ì—°ê²° ì–´ë¯¸',
        'XSV': 'ë™ì‚¬ íŒŒìƒ ì ‘ë¯¸ì‚¬',
        'XSA': 'í˜•ìš©ì‚¬ íŒŒìƒ ì ‘ë¯¸ì‚¬',
        'SP': 'ì‰¼í‘œ, ê°€ìš´ëƒì , ì½œë¡ , ë¹—ê¸ˆ',
        'SF': 'ë§ˆì¹¨í‘œ, ë¬¼ìŒí‘œ, ëŠë‚Œí‘œ',
    }
    
    # ì˜ë¯¸ìˆëŠ” í’ˆì‚¬ (í‚¤ì›Œë“œ ì¶”ì¶œìš©)
    MEANINGFUL_TAGS = ['NNG', 'NNP', 'VV', 'VA', 'SL', 'SH', 'MM', 'MAG']
    
    def __init__(
        self, 
        model_type: str = 'sbg',
        typos: str = 'basic',
        load_default_dict: bool = True
    ):
        """
        ì´ˆê¸°í™”
        
        Args:
            model_type: ëª¨ë¸ íƒ€ì… ('sbg', 'knlm')
            typos: ì˜¤íƒ€ êµì • ìˆ˜ì¤€ ('basic', 'auto', 'disabled')
            load_default_dict: ê¸°ë³¸ ì‚¬ì „ ë¡œë“œ ì—¬ë¶€
        """
        self.kiwi = Kiwi(
            model_type=model_type,
            typos=typos,
            load_default_dict=load_default_dict
        )
        self.last_analysis = None
    
    def analyze(self, text: str) -> List[TokenInfo]:
        """
        í…ìŠ¤íŠ¸ í˜•íƒœì†Œ ë¶„ì„
        
        Args:
            text: ë¶„ì„í•  í…ìŠ¤íŠ¸
            
        Returns:
            TokenInfo ê°ì²´ ë¦¬ìŠ¤íŠ¸
        """
        tokens = self.kiwi.tokenize(text)
        
        result = []
        for token in tokens:
            result.append(TokenInfo(
                form=token.form,
                tag=token.tag,
                score=token.score,
                probability=math.exp(token.score),
                start=token.start,
                length=token.len,
                tagged_form=token.tagged_form
            ))
        
        self.last_analysis = result
        return result
    
    def extract_keywords(
        self, 
        text: str, 
        confidence_threshold: float = -10.0,
        pos_tags: Optional[List[str]] = None
    ) -> List[str]:
        """
        ì‹ ë¢°ë„ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ
        
        Args:
            text: ë¶„ì„í•  í…ìŠ¤íŠ¸
            confidence_threshold: ì‹ ë¢°ë„ ì„ê³„ê°’ (ë‚®ì„ìˆ˜ë¡ ì—„ê²©)
            pos_tags: ì¶”ì¶œí•  í’ˆì‚¬ íƒœê·¸ (ê¸°ë³¸: MEANINGFUL_TAGS)
            
        Returns:
            ì¶”ì¶œëœ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        """
        if pos_tags is None:
            pos_tags = self.MEANINGFUL_TAGS
        
        tokens = self.analyze(text)
        
        keywords = [
            token.form for token in tokens
            if token.score > confidence_threshold and token.tag in pos_tags
        ]
        
        return keywords
    
    def extract_by_pos(self, text: str, pos_tag: str) -> List[str]:
        """
        íŠ¹ì • í’ˆì‚¬ë§Œ ì¶”ì¶œ
        
        Args:
            text: ë¶„ì„í•  í…ìŠ¤íŠ¸
            pos_tag: í’ˆì‚¬ íƒœê·¸ ('NNG', 'VV' ë“±)
            
        Returns:
            í•´ë‹¹ í’ˆì‚¬ì˜ í˜•íƒœì†Œ ë¦¬ìŠ¤íŠ¸
        """
        tokens = self.analyze(text)
        return [token.form for token in tokens if token.tag == pos_tag]
    
    def extract_nouns(self, text: str) -> List[str]:
        """ëª…ì‚¬ë§Œ ì¶”ì¶œ (ì¼ë°˜ëª…ì‚¬ + ê³ ìœ ëª…ì‚¬)"""
        tokens = self.analyze(text)
        return [token.form for token in tokens if token.tag in ['NNG', 'NNP']]
    
    def extract_verbs(self, text: str) -> List[str]:
        """ë™ì‚¬ë§Œ ì¶”ì¶œ"""
        return self.extract_by_pos(text, 'VV')
    
    def extract_adjectives(self, text: str) -> List[str]:
        """í˜•ìš©ì‚¬ë§Œ ì¶”ì¶œ"""
        return self.extract_by_pos(text, 'VA')
    
    def get_detailed_info(self, text: str) -> List[Dict]:
        """
        ìƒì„¸ ì •ë³´ í¬í•¨ ë¶„ì„
        
        Args:
            text: ë¶„ì„í•  í…ìŠ¤íŠ¸
            
        Returns:
            ìƒì„¸ ì •ë³´ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸
        """
        tokens = self.analyze(text)
        
        result = []
        for token in tokens:
            result.append({
                'í˜•íƒœì†Œ': token.form,
                'í’ˆì‚¬': token.tag,
                'í’ˆì‚¬ì„¤ëª…': self.POS_TAGS.get(token.tag, 'ê¸°íƒ€'),
                'ì ìˆ˜': round(token.score, 4),
                'í™•ë¥ ': round(token.probability, 6),
                'ì‹œì‘ìœ„ì¹˜': token.start,
                'ê¸¸ì´': token.length,
                'íƒœê·¸í˜•íƒœ': token.tagged_form
            })
        
        return result
    
    def analyze_with_confidence(
        self, 
        text: str, 
        confidence_threshold: float = -10.0,
        show_details: bool = True
    ) -> Dict:
        """
        ì‹ ë¢°ë„ ê¸°ë°˜ ë¶„ì„ (í†µí•© ë²„ì „)
        
        Args:
            text: ë¶„ì„í•  í…ìŠ¤íŠ¸
            confidence_threshold: ì‹ ë¢°ë„ ì„ê³„ê°’
            show_details: ìƒì„¸ ì •ë³´ í‘œì‹œ ì—¬ë¶€
            
        Returns:
            ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        tokens = self.analyze(text)
        
        # ì‹ ë¢°ë„ ë†’ì€ í† í° í•„í„°ë§
        confident_tokens = [
            token for token in tokens
            if token.score > confidence_threshold 
            and token.tag in self.MEANINGFUL_TAGS
        ]
        
        result = {
            'ì›ë³¸': text,
            'ì „ì²´í† í°ìˆ˜': len(tokens),
            'í‚¤ì›Œë“œìˆ˜': len(confident_tokens),
            'í‚¤ì›Œë“œ': [t.form for t in confident_tokens],
            'ëª…ì‚¬': [t.form for t in confident_tokens if t.tag in ['NNG', 'NNP']],
            'ë™ì‚¬': [t.form for t in confident_tokens if t.tag == 'VV'],
            'í˜•ìš©ì‚¬': [t.form for t in confident_tokens if t.tag == 'VA'],
        }
        
        if show_details:
            result['ìƒì„¸ì •ë³´'] = [
                {
                    'í˜•íƒœì†Œ': t.form,
                    'í’ˆì‚¬': t.tag,
                    'ì ìˆ˜': round(t.score, 4),
                    'í™•ë¥ ': round(t.probability, 6)
                }
                for t in confident_tokens
            ]
        
        return result
    
    def print_analysis(self, text: str, detailed: bool = False):
        """
        ë¶„ì„ ê²°ê³¼ ì˜ˆì˜ê²Œ ì¶œë ¥
        
        Args:
            text: ë¶„ì„í•  í…ìŠ¤íŠ¸
            detailed: ìƒì„¸ ì •ë³´ ì¶œë ¥ ì—¬ë¶€
        """
        print("=" * 60)
        print(f"ğŸ“ ì›ë³¸: {text}")
        print("=" * 60)
        
        tokens = self.analyze(text)
        
        if detailed:
            print(f"\n{'ë²ˆí˜¸':<4} {'í˜•íƒœì†Œ':<15} {'í’ˆì‚¬':<6} {'í’ˆì‚¬ì„¤ëª…':<12} {'ì ìˆ˜':<10} {'í™•ë¥ ':<10}")
            print("-" * 70)
            for i, token in enumerate(tokens):
                pos_desc = self.POS_TAGS.get(token.tag, 'ê¸°íƒ€')
                print(f"[{i+1:2d}] {token.form:<15s} {token.tag:<6s} {pos_desc:<12s} "
                      f"{token.score:>8.4f} {token.probability:>8.6f}")
        else:
            print(f"\n{'ë²ˆí˜¸':<4} {'í˜•íƒœì†Œ':<15} {'í’ˆì‚¬':<8} {'ì ìˆ˜':<12}")
            print("-" * 50)
            for i, token in enumerate(tokens):
                print(f"[{i+1:2d}] {token.form:<15s} {token.tag:<8s} {token.score:>10.4f}")
        
        # ìš”ì•½ ì •ë³´
        nouns = [t.form for t in tokens if t.tag in ['NNG', 'NNP']]
        verbs = [t.form for t in tokens if t.tag == 'VV']
        
        print(f"\nğŸ’¡ ìš”ì•½:")
        print(f"  - ì´ í† í°: {len(tokens)}ê°œ")
        print(f"  - ëª…ì‚¬: {nouns}")
        print(f"  - ë™ì‚¬: {verbs}")


# ========================================
# ğŸ¯ ì‚¬ìš© ì˜ˆì œ ëª¨ìŒ
# ========================================

def example_basic():
    """ê¸°ë³¸ ì‚¬ìš© ì˜ˆì œ"""
    print("=" * 60)
    print("ğŸš€ ì˜ˆì œ 1: ê¸°ë³¸ í˜•íƒœì†Œ ë¶„ì„")
    print("=" * 60)
    
    analyzer = KoreanMorphologicalAnalyzer()
    
    text = "ì•ˆë…•í•˜ì„¸ìš”, Kiwi í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•´ë³´ê² ìŠµë‹ˆë‹¤."
    analyzer.print_analysis(text, detailed=True)


def example_keyword_extraction():
    """í‚¤ì›Œë“œ ì¶”ì¶œ ì˜ˆì œ"""
    print("\n" + "=" * 60)
    print("ğŸš€ ì˜ˆì œ 2: ì‹ ë¢°ë„ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ")
    print("=" * 60)
    
    analyzer = KoreanMorphologicalAnalyzer()
    
    texts = [
        "Pythonì€ AI ê°œë°œì— ì í•©í•œ ì–¸ì–´ì…ë‹ˆë‹¤.",
        "LangChainê³¼ Kiwië¥¼ í™œìš©í•œ ê²€ìƒ‰ ì‹œìŠ¤í…œ",
        "í˜•íƒœì†Œ ë¶„ì„ì€ ìì—°ì–´ ì²˜ë¦¬ì˜ ê¸°ì´ˆì…ë‹ˆë‹¤."
    ]
    
    for text in texts:
        keywords = analyzer.extract_keywords(text, confidence_threshold=-10.0)
        print(f"\nğŸ“ ì›ë³¸: {text}")
        print(f"âœ… í‚¤ì›Œë“œ: {keywords}")


def example_pos_extraction():
    """í’ˆì‚¬ë³„ ì¶”ì¶œ ì˜ˆì œ"""
    print("\n" + "=" * 60)
    print("ğŸš€ ì˜ˆì œ 3: í’ˆì‚¬ë³„ í† í° ì¶”ì¶œ")
    print("=" * 60)
    
    analyzer = KoreanMorphologicalAnalyzer()
    
    text = "LangChainì€ ëŒ€í™”í˜• AI ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬ì¶•í•˜ê¸° ìœ„í•œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤."
    
    print(f"ğŸ“ ì›ë³¸: {text}\n")
    print(f"ğŸ“Œ ëª…ì‚¬: {analyzer.extract_nouns(text)}")
    print(f"ğŸ“Œ ë™ì‚¬: {analyzer.extract_verbs(text)}")
    print(f"ğŸ“Œ í˜•ìš©ì‚¬: {analyzer.extract_adjectives(text)}")


def example_detailed_analysis():
    """ìƒì„¸ ë¶„ì„ ì˜ˆì œ"""
    print("\n" + "=" * 60)
    print("ğŸš€ ì˜ˆì œ 4: ìƒì„¸ ë¶„ì„")
    print("=" * 60)
    
    analyzer = KoreanMorphologicalAnalyzer()
    
    text = "RAG ì‹œìŠ¤í…œì„ ì‰½ê²Œ ë§Œë“¤ ìˆ˜ ìˆì–´ìš”."
    
    result = analyzer.analyze_with_confidence(text, confidence_threshold=-10.0)
    
    print(f"ğŸ“ ì›ë³¸: {result['ì›ë³¸']}")
    print(f"ğŸ“Š ì „ì²´ í† í°: {result['ì „ì²´í† í°ìˆ˜']}ê°œ")
    print(f"ğŸ“Š í‚¤ì›Œë“œ: {result['í‚¤ì›Œë“œìˆ˜']}ê°œ")
    print(f"\nğŸ’¡ ì¶”ì¶œëœ ì •ë³´:")
    print(f"  - í‚¤ì›Œë“œ: {result['í‚¤ì›Œë“œ']}")
    print(f"  - ëª…ì‚¬: {result['ëª…ì‚¬']}")
    print(f"  - ë™ì‚¬: {result['ë™ì‚¬']}")
    
    if 'ìƒì„¸ì •ë³´' in result:
        print(f"\nğŸ” ìƒì„¸ ì •ë³´:")
        for info in result['ìƒì„¸ì •ë³´']:
            print(f"  - {info['í˜•íƒœì†Œ']:<10s} | {info['í’ˆì‚¬']:<6s} | "
                f"ì ìˆ˜: {info['ì ìˆ˜']:>8.4f} | í™•ë¥ : {info['í™•ë¥ ']:.6f}")


def example_batch_processing():
    """ë°°ì¹˜ ì²˜ë¦¬ ì˜ˆì œ"""
    print("\n" + "=" * 60)
    print("ğŸš€ ì˜ˆì œ 5: ë°°ì¹˜ í…ìŠ¤íŠ¸ ì²˜ë¦¬")
    print("=" * 60)
    
    analyzer = KoreanMorphologicalAnalyzer()
    
    documents = [
        "Pythonì€ ë°ì´í„° ê³¼í•™ì— ë„ë¦¬ ì‚¬ìš©ë©ë‹ˆë‹¤.",
        "AI ê¸°ìˆ ì´ ë¹ ë¥´ê²Œ ë°œì „í•˜ê³  ìˆìŠµë‹ˆë‹¤.",
        "ìì—°ì–´ ì²˜ë¦¬ëŠ” ë§¤ìš° ì¤‘ìš”í•œ ë¶„ì•¼ì…ë‹ˆë‹¤."
    ]
    
    print("ğŸ“š ë¬¸ì„œë³„ í‚¤ì›Œë“œ ì¶”ì¶œ:\n")
    for i, doc in enumerate(documents):
        keywords = analyzer.extract_keywords(doc, confidence_threshold=-8.0)
        print(f"[{i+1}] {doc}")
        print(f"    â†’ í‚¤ì›Œë“œ: {keywords}\n")


# ========================================
# ğŸ¬ ë©”ì¸ ì‹¤í–‰
# ========================================

if __name__ == "__main__":
    # ëª¨ë“  ì˜ˆì œ ì‹¤í–‰
    example_basic()
    example_keyword_extraction()
    example_pos_extraction()
    example_detailed_analysis()
    example_batch_processing()
    
    print("\n" + "=" * 60)
    print("âœ… ëª¨ë“  ì˜ˆì œ ì™„ë£Œ!")
    print("=" * 60)