# í™˜ê²½ë³€ìˆ˜ ì²˜ë¦¬ ë° í´ë¼ì´ì–¸íŠ¸ ìƒì„±
from langsmith import Client
from dotenv import load_dotenv

import os
import json

#################################        
# í•„ìš”í•œ ëª¨ë“ˆ ì„í¬íŠ¸
#################################     
from langchain.chat_models import init_chat_model
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

import streamlit as st                          # ìŠ¤íŠ¸ë¦¼ë¦¿ ì‚¬ìš©ì— í•„ìš”í•œ ëª¨ë“ˆ ì„í¬íŠ¸

# gpt, gemini ìŠ¤ìœ„ì¹˜ë˜ë„ë¡ í…ŒìŠ¤íŠ¸
import os
from dotenv import load_dotenv

load_dotenv()

# í™˜ê²½ë³€ìˆ˜ì—ì„œ API í‚¤ ê°€ì ¸ì˜¤ê¸°
openai_key = os.getenv("OPENAI_API_KEY")
google_key = os.getenv("GOOGLE_API_KEY")


#################################   
# LLM ì„¤ì •
#################################   
def get_llm(provider: str):
    """providerì— ë”°ë¼ LLM ê°ì²´ ë°˜í™˜"""
    if provider == "openai":
        return init_chat_model(
            model="gpt-4o-mini",                    # ì‚¬ìš©í•˜ëŠ” gpt ëª¨ë¸
            model_provider="openai",
            temperature=0.7,
            api_key=openai_key,
            max_output_tokens=1024
        )
    elif provider == "google":
        return init_chat_model(
            model="gemini-2.5-flash-lite",          # ì‚¬ìš©í•˜ëŠ” gemini ëª¨ë¸
            model_provider="google_genai",
            temperature=0.7,
            api_key=google_key,
            max_output_tokens=4096
        )
    else:
        raise ValueError("ì§€ì›í•˜ì§€ ì•ŠëŠ” providerì…ë‹ˆë‹¤.")

# ğŸ”„ ì›í•˜ëŠ” ëª¨ë¸ ì„ íƒ
llm = get_llm("google")                             # "openai"ë¡œ ë°”ê¾¸ë©´ gpt ì‚¬ìš© ê°€ëŠ¥ 

#################################   
# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±
#################################   
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant."),
    ("user", "{input}")
])

#################################   
# ë¬¸ìì—´ ì¶œë ¥ íŒŒì„œ
#################################   
output_parser = StrOutputParser()

#################################   
#LLM ì²´ì¸ êµ¬ì„±
#################################   
chain = prompt | llm | output_parser


#################################   
# Streamlit 
#################################   

#ì œëª©
st.title("ì¸ê³µì§€ëŠ¥ ì‹œì¸")

# content ë¯¸ë¦¬ ì„¤ì •í•´ë³´ê¸° = "ì½”ë”©"
content = "ì½”ë”©"
result = chain.invoke({"input": content + "ì— ëŒ€í•œ ì‹œë¥¼ ì¨ì¤˜"})
print(result)

# ì¶œë ¥í•´ë³´ê¸°
st.title("This is a title")
st.title("_Streamlit_is :blue[cool] :sunglasses:")
