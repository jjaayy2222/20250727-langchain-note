#################################        
# í•„ìš”í•œ ëª¨ë“ˆ ì„í¬íŠ¸
#################################     
from langchain.chat_models import init_chat_model
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

import streamlit as st                          # ìŠ¤íŠ¸ë¦¼ë¦¿ ì‚¬ìš©ì— í•„ìš”í•œ ëª¨ë“ˆ ì„í¬íŠ¸

# gpt, gemini ìŠ¤ìœ„ì¹˜ë˜ë„ë¡ í…ŒìŠ¤íŠ¸
import os
import json
from dotenv import load_dotenv

load_dotenv()

# í™˜ê²½ë³€ìˆ˜ì—ì„œ API í‚¤ ê°€ì ¸ì˜¤ê¸°
openai_key = os.getenv("OPENAI_API_KEY")
google_key = os.getenv("GOOGLE_API_KEY")


#################################   
# LLM ì„¤ì •
#################################   
def get_llm(provider: str):
    """providerì— ë”°ë¼ LLM ê°ì²´ ë°˜í™˜"""
    if provider == "openai":
        return init_chat_model(
            model="gpt-4o-mini",                    # ì‚¬ìš©í•˜ëŠ” gpt ëª¨ë¸
            model_provider="openai",
            temperature=0.7,
            api_key=openai_key,
            max_output_tokens=1024
        )
    elif provider == "google":
        return init_chat_model(
            model="gemini-2.5-flash-lite",          # ì‚¬ìš©í•˜ëŠ” gemini ëª¨ë¸
            model_provider="google_genai",
            temperature=0.7,
            api_key=google_key,
            max_output_tokens=4096
        )
    else:
        raise ValueError("ì§€ì›í•˜ì§€ ì•ŠëŠ” providerì…ë‹ˆë‹¤.")

# ğŸ”„ ì›í•˜ëŠ” ëª¨ë¸ ì„ íƒ
llm = get_llm("google")                             # "openai"ë¡œ ë°”ê¾¸ë©´ gpt ì‚¬ìš© ê°€ëŠ¥ 

#################################   
# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±
#################################   
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant."),
    ("user", "{input}")
])

#################################   
# ë¬¸ìì—´ ì¶œë ¥ íŒŒì„œ
#################################   
output_parser = StrOutputParser()

#################################   
#LLM ì²´ì¸ êµ¬ì„±
#################################   
chain = prompt | llm | output_parser


#################################   
# Streamlit 
#################################   

# 1. ì œëª©
st.title("ì¸ê³µì§€ëŠ¥ ì‹œì¸")

# 2. ì‹œ ì£¼ì œ ì…ë ¥ í•„ë“œ
# st.text_input(ì…ë ¥ í•„ë“œì˜ ëª©ì ì„ ì„¤ëª…í•˜ëŠ” label) = ì…ë ¥ê°’ â†’ contentì— ì €ì¥
content = st.text_input("ì‹œì˜ ì£¼ì œë¥¼ ì œì‹œí•´ì£¼ì„¸ìš”")

# st.write() = í™”ë©´ì— í…ìŠ¤íŠ¸ë¥¼ í‘œì‹œí•˜ëŠ” í•¨ìˆ˜
# "ì‹œì˜ ì£¼ì œëŠ”" +(ë”í•˜ê¸° ì—°ì‚°ì) content ë³€ìˆ˜ê°’
st.write("ì‹œì˜ ì£¼ì œëŠ”", content)


# 3. ì‹¤í–‰ ë²„íŠ¼ ì„¤ì •í•˜ê¸° 
# ì‹œ ì‘ì„± ìš”ì²­í•˜ê¸° = st.button() = ifë¬¸ì„ í†µí•´ ì‚¬ìš©ìê°€ ë²„íŠ¼ì„ ëˆŒë €ì„ ë•Œ ì‹¤í–‰í•  ë™ì‘ ëª…ì‹œ
# st.button() â†’ â†’ chain.invoke() = ({"input": content + "ì— ëŒ€í•œ ì‹œë¥¼ ì¨ì¤˜"})

if st.button("ì‹œ ì‘ì„± ìš”ì²­í•˜ê¸°"):
    with st.spinner('Wait for it...'):                  # ë¡œë”© ì• ë‹ˆë©”ì´ì…˜ ì„¤ì • (with í‚¤ì›Œë“œ ì•„ë˜ì— ìˆëŠ” ì½”ë“œ ë¸”ëŸ­ì—ë§Œ ì ìš©)
        result = chain.invoke({"input": content + "ì— ëŒ€í•œ ì‹œë¥¼ ì¨ì¤˜"})
        st.write(result)