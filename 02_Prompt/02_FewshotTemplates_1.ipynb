{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6f24ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 모듈 임포트\n",
    "import os\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API KEY를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API KEY 정보 로드\n",
    "load_dotenv()                   # true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714d7919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 환경 변수 확인하기\n",
    "\n",
    "# 마스킹 처리 함수 정의\n",
    "def mask_key(key: str, visible_count: int = 2) -> str:\n",
    "    if not key or len(key) <= visible_count:\n",
    "        return '*' * len(key)\n",
    "    return key[:visible_count] + '*' * (len(key) - visible_count)\n",
    "\n",
    "# 환경변수 불러오기\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"GOOGLE_API_KEY 환경 변수가 설정되지 않았습니다.\")\n",
    "\n",
    "# 마스킹된 형태로 출력\n",
    "print(f\"GOOGLE_API_KEY: {mask_key(api_key)}\")           # GOOGLE_API_KEY: AI*************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e46030a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangSmith 추적 설정 (https://smith.langchain.com)\n",
    "\n",
    "\"\"\"\n",
    "- !pip install -qU langsmith\n",
    "- !pip install -qU langchain-teddynote\n",
    "    -> 제미나이와 poetry와의 의존성 충돌로 langchain_teddy 설치 X \n",
    "    -> langsmith로 진행\n",
    "\"\"\"\n",
    "# LangSmith 추적을 위한 라이브러리 임포트\n",
    "from langsmith import traceable                                                             # @traceable 데코레이터 사용 시\n",
    "\n",
    "# LangSmith 환경 변수 확인\n",
    "\n",
    "print(\"\\n--- LangSmith 환경 변수 확인 ---\")\n",
    "langchain_tracing_v2 = os.getenv('LANGCHAIN_TRACING_V2')\n",
    "langchain_project = os.getenv('LANGCHAIN_PROJECT')\n",
    "langchain_api_key_status = \"설정됨\" if os.getenv('LANGCHAIN_API_KEY') else \"설정되지 않음\"      # API 키 값은 직접 출력하지 않음\n",
    "\n",
    "if langchain_tracing_v2 == \"true\" and os.getenv('LANGCHAIN_API_KEY') and langchain_project:\n",
    "    print(f\"✅ LangSmith 추적 활성화됨 (LANGCHAIN_TRACING_V2='{langchain_tracing_v2}')\")\n",
    "    print(f\"✅ LangSmith 프로젝트: '{langchain_project}'\")\n",
    "    print(f\"✅ LangSmith API Key: {langchain_api_key_status}\")\n",
    "    print(\"  -> 이제 LangSmith 대시보드에서 이 프로젝트를 확인해 보세요.\")\n",
    "else:\n",
    "    print(\"❌ LangSmith 추적이 완전히 활성화되지 않았습니다. 다음을 확인하세요:\")\n",
    "    if langchain_tracing_v2 != \"true\":\n",
    "        print(f\"  - LANGCHAIN_TRACING_V2가 'true'로 설정되어 있지 않습니다 (현재: '{langchain_tracing_v2}').\")\n",
    "    if not os.getenv('LANGCHAIN_API_KEY'):\n",
    "        print(\"  - LANGCHAIN_API_KEY가 설정되어 있지 않습니다.\")\n",
    "    if not langchain_project:\n",
    "        print(\"  - LANGCHAIN_PROJECT가 설정되어 있지 않습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b345ed6",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* --- LangSmith 환경 변수 확인 ---\n",
    "* ✅ LangSmith 추적 활성화됨 (LANGCHAIN_TRACING_V2='true')\n",
    "* ✅ LangSmith 프로젝트: 'LangChain-prantice'\n",
    "* ✅ LangSmith API Key: 설정됨\n",
    "*   -> 이제 LangSmith 대시보드에서 이 프로젝트를 확인해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e56563a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain 및 Google GenAI 모델 관련 모듈 임포트\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI           # Google GenAI 임포트\n",
    "\n",
    "\n",
    "print(\"\\n--- LangChain 체인 설정 ---\")                                # --- LangChain 체인 설정 ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf48fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM 객체 정의하기\n",
    "\n",
    "# model1 = gemini-1.5-flash\n",
    "# model2 = gemini-2.5-falsh-lite\n",
    "\n",
    "try:\n",
    "    model1 = ChatGoogleGenerativeAI(                                      # 모델 호출\n",
    "        model=\"gemini-1.5-flash\",\n",
    "        #temperature=0.1,\n",
    "    )\n",
    "    print(\"✅ gemini-1.5-flash 호출 성공.\")\n",
    "except Exception as e:                                                   # 디버깅 메시지\n",
    "    print(f\"❌ Google GenAI 모델 초기화 실패: {e}\")\n",
    "    print(\"  -> GEMINI_API_KEY 환경 변수가 올바르게 설정되었는지 확인하세요.\")      # ✅ gemini-2.5-flash 호출 성공.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bff0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model2 = gemini-2.5-falsh-lite\n",
    "\n",
    "try:\n",
    "    model2 = ChatGoogleGenerativeAI(                                      # 모델 호출\n",
    "        model=\"gemini-2.5-flash-lite\",\n",
    "        #temperature=0.1,\n",
    "    )\n",
    "    print(\"✅ gemini-2.5-flash-lite 호출 성공.\")\n",
    "except Exception as e:                                                   # 디버깅 메시지\n",
    "    print(f\"❌ Google GenAI 모델 초기화 실패: {e}\")\n",
    "    print(\"  -> GEMINI_API_KEY 환경 변수가 올바르게 설정되었는지 확인하세요.\")      # ✅ gemini-2.5-flash-lite 호출 성공."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d49712",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dc21ce",
   "metadata": {},
   "source": [
    "\n",
    "### (1) **`FewShotPromptTemplate`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7e984f",
   "metadata": {},
   "source": [
    "#### ➀ `gemini` 모델별 `max-token`\n",
    "\n",
    "<br>\n",
    "\n",
    "| 모델                    | 최대 출력 토큰 | 일반적인 용도  | 긴 글/복잡한 답변 | 매우 긴 문서/코드  |\n",
    "|-----------------------|----------|----------|------------|-------------|\n",
    "| Gemini 1.5 Flash      | 8,192    | 512~1024 | 4096~8192  | -           |\n",
    "| Gemini 2.5 Flash      | 65,536   | 512~1024 | 4096~16384 | 32768~65536 |\n",
    "| Gemini 2.5 Flash-Lite | 65,536   | 512~1024 | 4096~16384 | 32768~65536 |\n",
    "\n",
    "<br>\n",
    "\n",
    "***\n",
    "\n",
    "<br>\n",
    "\n",
    "  * `gemini-1.5-flash`\n",
    "    * **최대 출력 토큰**: **8,192 토큰**\n",
    "      * 권장 설정\n",
    "        * 일반적인 대화, 요약, 짧은 코드: 512-1024 토큰\n",
    "        * 장문의 글, 긴 코드, 복잡한 답변: 4096-8192 토큰\n",
    "      * `tip!` - 상대적으로 출력 토큰이 적으므로, 응답이 중간에 잘리지 않도록 응답의 길이를 예측해서 값을 설정하는 것이 중요!\n",
    "\n",
    "<br>\n",
    "\n",
    "  * `gemini-2.5-flash-lite`\n",
    "    * 최대 출력 토큰**: **65,536 토큰**\n",
    "    * 권장 설정\n",
    "        * 일반적인 대화, 요약, 짧은 코드: 512-1024 토큰\n",
    "        * 장문의 글, 긴 코드, 복잡한 답변: 4096-16384 토큰\n",
    "        * 매우 긴 문서 작성, 전체 책 요약, 방대한 코드 생성: 32768~65536 토큰\n",
    "    * `tip!`\n",
    "        * 출력 토큰 한도가 매우 높아서 대부분의 경우 넉넉하게 값을 설정할 수 있음\n",
    "        * 굳이 최대치로 설정하기보다는, 필요한 만큼만 설정하여 비용을 관리하는 것을 권장\n",
    "\n",
    "<br>\n",
    "\n",
    " * `결론`\n",
    "    * **`일반적인 용도`** (대부분의 질문): `1024` or `2048` 토큰 정도 충분\n",
    "    * **`긴 글`**, **`코드`**: **`4096 토큰 이상`으로 설정**\n",
    "    * **`매우 긴 문서 요약 등 방대한 응답이 필요한 경우`**: 모델의 `최대 출력 토큰에 가깝게` 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "048cf5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# 환경변수 사전 로드 필요\n",
    "\n",
    "# --- 1. Few-shot 예시 데이터 정의 ---\n",
    "# 모델 = 학습할 예시들을 파이썬 리스트 형태로 정의\n",
    "# 각 예시 = 'question'과 'answer' 키를 가진 딕셔너리\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"스티브 잡스와 아인슈타인 중 누가 더 오래 살았나요?\",\n",
    "        \"answer\": \"\"\"이 질문에 추가 질문이 필요한가요: 예.\n",
    "추가 질문: 스티브 잡스는 몇 살에 사망했나요?\n",
    "중간 답변: 스티브 잡스는 56세에 사망했습니다.\n",
    "추가 질문: 아인슈타인은 몇 살에 사망했나요?\n",
    "중간 답변: 아인슈타인은 76세에 사망했습니다.\n",
    "최종 답변은: 아인슈타인\n",
    "\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"네이버의 창립자는 언제 태어났나요?\",\n",
    "        \"answer\": \"\"\"이 질문에 추가 질문이 필요한가요: 예.\n",
    "추가 질문: 네이버의 창립자는 누구인가요?\n",
    "중간 답변: 네이버는 이해진에 의해 창립되었습니다.\n",
    "추가 질문: 이해진은 언제 태어났나요?\n",
    "중간 답변: 이해진은 1967년 6월 22일에 태어났습니다.\n",
    "최종 답변은: 1967년 6월 22일\n",
    "\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"율곡 이이의 어머니가 태어난 해의 통치하던 왕은 누구인가요?\",\n",
    "        \"answer\": \"\"\"이 질문에 추가 질문이 필요한가요: 예.\n",
    "추가 질문: 율곡 이이의 어머니는 누구인가요?\n",
    "중간 답변: 율곡 이이의 어머니는 신사임당입니다.\n",
    "추가 질문: 신사임당은 언제 태어났나요?\n",
    "중간 답변: 신사임당은 1504년에 태어났습니다.\n",
    "추가 질문: 1504년에 조선을 통치한 왕은 누구인가요?\n",
    "중간 답변: 1504년에 조선을 통치한 왕은 연산군입니다.\n",
    "최종 답변은: 연산군\n",
    "\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"올드보이와 기생충의 감독이 같은 나라 출신인가요?\",\n",
    "        \"answer\": \"\"\"이 질문에 추가 질문이 필요한가요: 예.\n",
    "추가 질문: 올드보이의 감독은 누구인가요?\n",
    "중간 답변: 올드보이의 감독은 박찬욱입니다.\n",
    "추가 질문: 박찬욱은 어느 나라 출신인가요?\n",
    "중간 답변: 박찬욱은 대한민국 출신입니다.\n",
    "추가 질문: 기생충의 감독은 누구인가요?\n",
    "중간 답변: 기생충의 감독은 봉준호입니다.\n",
    "추가 질문: 봉준호는 어느 나라 출신인가요?\n",
    "중간 답변: 봉준호는 대한민국 출신입니다.\n",
    "최종 답변은: 예\n",
    "\"\"\",\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b23b952d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: 스티브 잡스와 아인슈타인 중 누가 더 오래 살았나요?\n",
      "이 질문에 추가 질문이 필요한가요: 예.\n",
      "추가 질문: 스티브 잡스는 몇 살에 사망했나요?\n",
      "중간 답변: 스티브 잡스는 56세에 사망했습니다.\n",
      "추가 질문: 아인슈타인은 몇 살에 사망했나요?\n",
      "중간 답변: 아인슈타인은 76세에 사망했습니다.\n",
      "최종 답변은: 아인슈타인\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 2. 예시 포맷 및 전체 프롬프트 템플릿 정의 ---\n",
    "# 모델이 각 예시를 어떤 형식으로 인식할지 정의하기\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"answer\"],\n",
    "    template=\"질문: {question}\\n{answer}\"\n",
    ")\n",
    "\n",
    "print(example_prompt.format(**examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7230b391",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "원래 코드\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Question:\\n{question}\\nAnswer:\",\n",
    "    input_variables=[\"question\"],\n",
    ")\n",
    "\n",
    "question = \"Google이 창립된 연도에 Bill Gates의 나이는 몇 살인가요?\"\n",
    "final_prompt = prompt.format(question=question)\n",
    "print(final_prompt)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Few-shot 예시들을 포함하는 최종 프롬프트 템플릿 생성\n",
    "# prefix: 예시들 앞에 붙을 설명\n",
    "# suffix: 예시들 뒤에 붙을 새로운 질문\n",
    "# input_variables: suffix에 들어갈 변수\n",
    "\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"다음은 몇 가지 질문과 답변 예시입니다. 이 형식을 따라 새로운 질문에 답해주세요.\\n\\n\",\n",
    "    suffix=\"질문: {input}\\n\",\n",
    "    input_variables=[\"input\"],\n",
    ")\n",
    "\n",
    "# print(few_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ae593f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. LLM (대규모 언어 모델) 객체 정의 ---\n",
    "# LangChain에서 Gemini 모델을 호출하는 객체 만들기\n",
    "# model = 사용할 Gemini 모델명 (예: \"gemini-2.5-flash-lite\")\n",
    "# temperature  = 0으로 설정하여 창의성을 낮추고, 예시 패턴을 엄격하게 따르도록 유도\n",
    "\n",
    "gemini_lc = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486b8e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. LangChain Expression Language (LCEL)을 사용한 체인(Chain) 구성 ---\n",
    "# 프롬프트 → LLM → 결과 파싱(parsing) 순서로 작업 연결하기\n",
    "# StrOutputParser는 모델의 응답을 단순한 문자열로 변환하기\n",
    "\n",
    "chain_fsp = few_shot_prompt | gemini_lc | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8751e258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. 새로운 질문으로 체인 실행 ---\n",
    "# 'chain.invoke' 호출 → Few-shot 프롬프트를 모델에 전달하고 응답을 받음\n",
    "\n",
    "new_question = \"제임스 고슬링과 귀도 반 로섬 중 누가 파이썬을 만들었나요?\"\n",
    "\n",
    "response = chain_fsp.invoke({\"input\": new_question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac79915c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. 결과 출력 ---\n",
    "print(\"--- Few-shot 프롬프트를 적용한 모델의 응답 ---\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f55082f",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 셀 출력 \n",
    "  \n",
    "    ```<markdown>\n",
    "    --- Few-shot 프롬프트를 적용한 모델의 응답 ---\n",
    "    이 질문에 추가 질문이 필요한가요: 예.\n",
    "    추가 질문: 제임스 고슬링은 무엇을 만들었나요?\n",
    "    중간 답변: 제임스 고슬링은 자바를 만들었습니다.\n",
    "    추가 질문: 귀도 반 로섬은 무엇을 만들었나요?\n",
    "    중간 답변: 귀도 반 로섬은 파이썬을 만들었습니다.\n",
    "    최종 답변은: 귀도 반 로섬\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1eadb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 교안처럼 출력해보기\n",
    "# FewShotPromptTemplate 객체를 그대로 사용\n",
    "# 이 객체 = examples와 suffix 등의 정보를 이미 가지고 있음\n",
    "\n",
    "# 테스트할 새로운 질문을 변수에 담기\n",
    "new_question = \"Google이 창립된 연도에 Bill Gates의 나이는 몇 살인가요?\"                        # 교안 속 질문\n",
    "\n",
    "# `.format()` 메서드를 사용하여 최종 프롬프트 문자열을 만들기\n",
    "# 'input' 변수에 new_question 값을 전달하기\n",
    "final_prompt = few_shot_prompt.format(input=new_question)\n",
    "\n",
    "# 완성된 프롬프트 문자열을 출력하기\n",
    "print(final_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6ccddc",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 셀 출력\n",
    "\n",
    "    ```<mardown>\n",
    "    다음은 몇 가지 질문과 답변 예시입니다. 이 형식을 따라 새로운 질문에 답해주세요.\n",
    "\n",
    "\n",
    "\n",
    "    질문: 스티브 잡스와 아인슈타인 중 누가 더 오래 살았나요?\n",
    "    이 질문에 추가 질문이 필요한가요: 예.\n",
    "    추가 질문: 스티브 잡스는 몇 살에 사망했나요?\n",
    "    중간 답변: 스티브 잡스는 56세에 사망했습니다.\n",
    "    추가 질문: 아인슈타인은 몇 살에 사망했나요?\n",
    "    중간 답변: 아인슈타인은 76세에 사망했습니다.\n",
    "    최종 답변은: 아인슈타인\n",
    "\n",
    "\n",
    "    질문: 네이버의 창립자는 언제 태어났나요?\n",
    "    이 질문에 추가 질문이 필요한가요: 예.\n",
    "    추가 질문: 네이버의 창립자는 누구인가요?\n",
    "    중간 답변: 네이버는 이해진에 의해 창립되었습니다.\n",
    "    추가 질문: 이해진은 언제 태어났나요?\n",
    "    중간 답변: 이해진은 1967년 6월 22일에 태어났습니다.\n",
    "    최종 답변은: 1967년 6월 22일\n",
    "\n",
    "\n",
    "    질문: 율곡 이이의 어머니가 태어난 해의 통치하던 왕은 누구인가요?\n",
    "    이 질문에 추가 질문이 필요한가요: 예.\n",
    "    추가 질문: 율곡 이이의 어머니는 누구인가요?\n",
    "    중간 답변: 율곡 이이의 어머니는 신사임당입니다.\n",
    "    추가 질문: 신사임당은 언제 태어났나요?\n",
    "    중간 답변: 신사임당은 1504년에 태어났습니다.\n",
    "    추가 질문: 1504년에 조선을 통치한 왕은 누구인가요?\n",
    "    중간 답변: 1504년에 조선을 통치한 왕은 연산군입니다.\n",
    "    최종 답변은: 연산군\n",
    "\n",
    "\n",
    "    질문: 올드보이와 기생충의 감독이 같은 나라 출신인가요?\n",
    "    이 질문에 추가 질문이 필요한가요: 예.\n",
    "    추가 질문: 올드보이의 감독은 누구인가요?\n",
    "    중간 답변: 올드보이의 감독은 박찬욱입니다.\n",
    "    추가 질문: 박찬욱은 어느 나라 출신인가요?\n",
    "    중간 답변: 박찬욱은 대한민국 출신입니다.\n",
    "    추가 질문: 기생충의 감독은 누구인가요?\n",
    "    중간 답변: 기생충의 감독은 봉준호입니다.\n",
    "    추가 질문: 봉준호는 어느 나라 출신인가요?\n",
    "    중간 답변: 봉준호는 대한민국 출신입니다.\n",
    "    최종 답변은: 예\n",
    "\n",
    "\n",
    "    질문: Google이 창립된 연도에 Bill Gates의 나이는 몇 살인가요?\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabc9171",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ecd485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리들 불러오기\n",
    "from langchain_core.prompts.few_shot import FewShotChatMessagePromptTemplate\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# --- 1. Few-shot 예시 데이터 정의 ---\n",
    "examples = [\n",
    "    {\n",
    "        \"text\": \"새로 나온 노트북 '슈퍼북 프로'를 150만원에 구매했습니다. 성능이 좋아서 별 5개 드립니다.\",\n",
    "        \"output\": '{\"상품명\": \"슈퍼북 프로\", \"가격\": 1500000, \"평점\": 5}'\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"이 커피 머신 '아로마 드림'은 20만원에 샀는데, 맛이 별로네요. 평점은 2점입니다.\",\n",
    "        \"output\": '{\"상품명\": \"아로마 드림\", \"가격\": 200000, \"평점\": 2}'\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"'블루투스 이어폰 X10'을 7만원에 구매했습니다. 음질이 훌륭하여 별 4개를 주고 싶습니다.\",\n",
    "        \"output\": '{\"상품명\": \"블루투스 이어폰 X10\", \"가격\": 70000, \"평점\": 4}'\n",
    "    },\n",
    "]\n",
    "\n",
    "# --- 2. 예시 포맷 및 전체 프롬프트 템플릿 정의 ---\n",
    "# ChatPromptTemplate 사용\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"리뷰: {text}\"),\n",
    "        (\"ai\", \"출력: {output}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# FewShotPromptTemplate 대신 FewShotChatMessagePromptTemplate 사용\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples\n",
    ")\n",
    "\n",
    "# --- 3. LLM 객체 정의 ---\n",
    "gemini_lc = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash-lite\",\n",
    "    temperature=0.8,\n",
    "    response_mime_type=\"application/json\"\n",
    ")\n",
    "\n",
    "# --- 4. 최종 프롬프트 구성 ---\n",
    "# 최종 프롬프트에 system 메시지와 Few-shot 프롬프트, 그리고 새로운 질문을 포함시킵니다.\n",
    "final_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"주어진 리뷰 문장에서 상품명, 가격, 평점을 추출하여 JSON 형식으로 출력하세요.\"),\n",
    "    few_shot_prompt,\n",
    "    (\"human\", \"리뷰: {input}\"),\n",
    "])\n",
    "\n",
    "\n",
    "# --- 5. 체인 구성 ---\n",
    "parser = JsonOutputParser()\n",
    "chain = final_prompt | gemini_lc | parser\n",
    "\n",
    "# --- 6. 새로운 리뷰 문장으로 체인 실행 ---\n",
    "new_review = \"'스마트워치 A30'을 30만원에 샀는데, 디자인이 정말 마음에 듭니다. 평점은 5점입니다.\"\n",
    "response = chain.invoke({\"input\": new_review})\n",
    "\n",
    "# --- 7. 결과 출력 ---\n",
    "print(\"--- Few-shot 프롬프트를 적용한 모델의 응답 ---\")\n",
    "print(response)\n",
    "\n",
    "try:\n",
    "    print(\"\\n--- JSON 파싱 결과 ---\")\n",
    "    print(f\"상품명: {response['상품명']}\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ 오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd0ff8c",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 셀 출력 \n",
    "  \n",
    "  * temperature = 0 (1.3s)\n",
    "\n",
    "    ```<markdown>\n",
    "        --- Few-shot 프롬프트를 적용한 모델의 응답 ---\n",
    "        {'상품명': '스마트워치 A30', '가격': 300000, '평점': 5}\n",
    "\n",
    "        --- JSON 파싱 결과 ---\n",
    "        상품명: 스마트워치 A30\n",
    "\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5069ddb",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "***\n",
    "\n",
    "* 올바른 조합\n",
    "\n",
    "    ```<python>\n",
    "        few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "            example_prompt=example_prompt,  # <-- ChatPromptTemplate 객체를 올바르게 처리함\n",
    "            examples=examples\n",
    "        )\n",
    "    ```\n",
    "\n",
    "***\n",
    "\n",
    "* **온도(Temperature)가 출력에 영향을 주지 않는 이유**\n",
    "  * `response_mime_type=\"application/json\"`\n",
    "    * 모델에게 반드시 JSON 형식으로만 응답하라는 강력한 제약 조건 = `temperature`가 높든 낮든 모델이 정해진 **문법** 을 지키도록 **강제**\n",
    "\n",
    "  * `Few-shot 예시`\n",
    "    * `FewShotChatMessagePromptTemplate`에 포함된 명확한 예시들은 모델이 특정 패턴을 따르도록 강력하게 유도\n",
    "    * 이 예시들이 모델의 응답에 대한 가이드 역할 → `temperature의 영향 감소` = **특정 형식이나 패턴을 강제하는 제약 조건이 우선시**\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d414b625",
   "metadata": {},
   "source": [
    "***💡 tip 💡***\n",
    "\n",
    "| 영향력 순위 | 영향 요소                            | 설명 및 이유                                                                                                                            |\n",
    "|--------|----------------------------------|------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| 1순위    | 프롬프트 내용                          | 모델의 행동을 결정하는 가장 근본적인 요소. 명확한 지시, 작업 목표, 그리고 Few-shot 예시 등 프롬프트에 포함된 내용이 모델의 응답 방향을 90% 이상 좌우                                   |\n",
    "| 2순위    | 형식 제약<br>(response_mime_type)    | 모델의 응답 형식을 강제하는 강력한 규칙. application/json과 같이 특정 형식을 지정하면, 모델은 temperature 값과 관계없이 해당 형식에 맞춰 내용을 생성해야 함 → 이 제약은 모델의 자유도를 크게 제한 |\n",
    "| 3순위    | 무작위성/다양성<br>(temperature, top_p) | 모델의 창의성과 다양성을 미세하게 조절하는 요소. 프롬프트의 지시와 형식 제약 안에서 답변의 예측 가능성을 조절. 정답이 없는 창작 작업에서 중요한 역할을 하지만, 정해진 형식이 있는 작업에서는 영향력이 상대적으로 낮음   |\n",
    "\n",
    "\n",
    "\n",
    "  * **프롬프트** = 모델이 **무엇을** 할지 알려주는 **목표**\n",
    "      * **`가장 근본적인 영향`** = **모델의 출력에 가장 큰 영향을 주는 것** = **프롬프트 내용 자체입니다.**\n",
    "      * **언제 중요??**\n",
    "        * 모든 작업에서 중요\n",
    "        * 특히 `Few-shot` 예시처럼 `명확한 지시`나 `패턴`을 줄 때 그 영향력이 극대화\n",
    "        * 모델은 프롬프트에 담긴 예시를 따르도록 학습하기 때문에, 아무리 temperature를 높여도 프롬프트의 패턴을 벗어나기 어려움\n",
    "\n",
    "  * **`response_mime_type`** = **어떻게** 해야 할지 알려주는 **가이드라인**\n",
    "    * 가장 강력한 제약 = `temperature`의 영향을 **무시할 만큼 강력한 형식 제약 부여**\n",
    "    * **언제 중요??**\n",
    "      * `JSON`, `YAML` 등 **특정 구조**의 응답이 필요한 **엔티티 추출** → 데이터 정리 같은 작업에서 가장 중요\n",
    "      * **`정해진 형식`** 에 맞춰 정보를 채워 넣는 데 집중 **>** 모델 창의성 발휘\n",
    "\n",
    "  * **`temperature`및 `top_p`** = **그 가이드라인 안에서** 얼마나 자유롭게 할지 알려주는 **미세 조정 역할**\n",
    "      * 모델의 **출력 다양성을 조절**\n",
    "        * `temperature` = 무작위성, `top_p` = `확률적 선택의 범위` 제어\n",
    "      * **언제 중요??**\n",
    "        * 창의적인 글쓰기, 시, 스토리텔링 등 정답이 없는 `창작 작업`에서 가장 중요 → 이 값을 `높이면` 더 다양하고 예측 불가능한 답변을 얻을 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d835e55",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fa9895",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* next : `Example Selector`~"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
