{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c976a97f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8ab679",
   "metadata": {},
   "source": [
    "* 출처: LangChain 공식 문서, 조코딩의 랭체인으로 AI 에이전트 서비스 만들기\n",
    "\n",
    "* [깃허브 저장소 출처](https://github.com/sw-woo/hanbit-langchain): https://github.com/sw-woo/hanbit-langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6190853",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec160ae",
   "metadata": {},
   "source": [
    "### **01. `다국어 이메일 생성기 만들기`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9666f311",
   "metadata": {},
   "source": [
    "* *출처: 위에 표기*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed30210e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5e08d5",
   "metadata": {},
   "source": [
    "#### **1) `교재 설명`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de097257",
   "metadata": {},
   "source": [
    "* **목표**\n",
    "\n",
    "  * `Ollama`기반의 `Llama 3.1 모델` + `로컬 환경`\n",
    "\n",
    "  * `CTransformers` 이용한 `Local Llama 2` 모델 활용\n",
    "\n",
    "  * **사용자가 선택한 언어에 맞춰 `이메일 주제`, `발신자`, `수신자 정보` 입력 → `자동으로 이메일 내용 생성` → 다국어 자동 이메일 생성기 구축하기** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa914619",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41eda4c9",
   "metadata": {},
   "source": [
    "* **핵심 키워드**\n",
    "\n",
    "  * **`Ollama`**\n",
    "\n",
    "  * **`Llama 3.1`**: *`Ollama` 통해 연결됨*\n",
    "\n",
    "  * **`Local Llama 2`**: *`CTransformers` 라이브러리 통해 사용됨*\n",
    "\n",
    "  * **`CTransformers`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5583f054",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c92307d",
   "metadata": {},
   "source": [
    "* **`LLaMa` 사용하는 이유**\n",
    "\n",
    "  * **`Open source model`**: 오픈 가중치 모델 제공 → 대규모 호출 → 비용 부담 ↓ \n",
    "\n",
    "  * **`Local Llama 2`** → 데이터 외부 유출 ❌ → 프라이버시, 준법 규정 리스크 ↓\n",
    "\n",
    "  * 인터넷 불안정한 환경에서도 네트워크 지연 없이 오프라인에서도 동작 → 일관된 응답 속도 제공 \n",
    "\n",
    "  * **`하이브리드` 전략**: **`요청 길이`, `언어`, `하드웨어 여유`에 따라 두 모델을 동적으로 바꿈 → 비용, 품질 동시에 최적화**\n",
    "\n",
    "    * **a. `Ollama` + `Llama 3.1`**: 고품질이 필요한 한국어 or 장문 이메일 → **`GPU`** 서버에서 실행 → 처리\n",
    "\n",
    "    * **b. `CTransformers` → `Local Llama 2`**: 짧은 영문 알림 메일 or 오프라인 환경에서 사용\n",
    "\n",
    "  * **`Ollama`** = `LLM`을 로컬 환경에서 실행할 수 있도록 도와주는 오픈 소스 도구\n",
    "\n",
    "    * `모델 레지스트리`, `실행 런타임`, `langchain용 어댑터` 통합\n",
    "\n",
    "      * `ollama pull llama3.1` → 모델 다운로드 → 바로 사용 가능\n",
    "\n",
    "      * `REST`, `gRPC`, `CLI` 등 원하는 방식으로 호출 가능\n",
    "\n",
    "      * **`LangChain-Ollama` 패키지 사용 시 → `ChatOpenAI`와 거의 동일한 인터페이스로 코드에 연결 가능**\n",
    "\n",
    "    * `Linux`, `macOS`, `Windows` 등 지원 → 노트북 or 클라우드 GPU 서버 모두 `배포 절차 동일` → `Llama 3.1` 손쉽게 활용할 수 있는 모델 허브 역할\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d8c3a4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c6faef",
   "metadata": {},
   "source": [
    "* **`Ollama` 설치 및 환경 설정**\n",
    "\n",
    "  * **`Ollama` 설치**: [Ollama 설치 다운로드 페이지](https://ollama.com/download)\n",
    "\n",
    "    * 지원 플랫폼 (`Linux`, `macOS`, `Windows` 등)에 맞는 **`Ollama` 설치하기**\n",
    "\n",
    "```bash\n",
    "\n",
    "            pip install langchain-ollama\n",
    "\n",
    "```\n",
    "\n",
    "* \n",
    "  * **`원하는 모델 설치하기`**\n",
    "\n",
    "    * **`터미널`** 로 이동 → `Ollama pull <모델 이름>` 입력\n",
    "\n",
    "```bash\n",
    "\n",
    "            ollama pull llama3.1:8b                 # 교재 예시\n",
    "\n",
    "            # 70b 이상부터는 고성능 GPU 컴퓨팅 파워 필요함\n",
    "\n",
    "\n",
    "```\n",
    "* \n",
    "  * [8b 모델 다운로드](https://ollama.com/library/llama3.1:8b)\n",
    "\n",
    "<br>\n",
    "\n",
    "* \n",
    "  * *추가 명령어*\n",
    "\n",
    "    * *`모델 목록 확인`*: ***`ollama list`*** → 다운로드 된 모든 모델 확인하고 싶을 때\n",
    "\n",
    "    * *`모델과 직접 채팅`*: ***`ollama run <모델 이름>`*** → 명령 줄에서 직접 모델과 대화하고 싶을 때\n",
    "\n",
    "    * *`추가 명령어 확인`*: ***`ollma help`*** → 더 많은 명령어 확인하고 싶을 때\n",
    "      * 혹은 [`Ollama 공식 문서 참조`](https://docs.ollama.com/)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078d1034",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16497774",
   "metadata": {},
   "source": [
    "* **`CTransformers` 환경 설정**\n",
    "\n",
    "  * **`CTransformers`, `langchin-community` 라이브러리 설치 필요 !**\n",
    "\n",
    "    * `CPU`에서 `Transformer` 모델 실행위해 사용 \n",
    "\n",
    "    * `LLaMa`, `GPT4ALL`, `MPT` 등 다양한 모델 지원\n",
    "\n",
    "```bash\n",
    "\n",
    "            pip install ctransformsers langchain-community\n",
    "\n",
    "```\n",
    "\n",
    "* \n",
    "  * **`Llama-2-7B-Chat`** 모델의 경량화 버전 다운로드 \n",
    "\n",
    "    * `CTransformers` = `GGML` 포맷으로 저장된 모델 사용\n",
    "\n",
    "    * [Llama-2-7B-Chat-GGML](https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/tree/main) → `llama-2-7b-chat.ggmlv3.q8_0.bin` 다운로드 및 사용\n",
    "\n",
    "    * 다운로드 → **`로컬 디렉터리`에 `저장`** → `CTransformers` 라이브러리와 함께 사용할 준비 마치기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e367cfc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6ebe12",
   "metadata": {},
   "source": [
    "* **`Streamlit` 및 기타 패키지 설치**\n",
    "\n",
    "  * **a. `Streamlit`**\n",
    "\n",
    "    * 터미널 (or 명령 프름프트)에서 다음 명령어 입력하기\n",
    "\n",
    "```bash\n",
    "\n",
    "            pip install streamlit\n",
    "\n",
    "```\n",
    "* \n",
    "  * **b. `그 외 필요한 패키지` 설치**\n",
    "\n",
    "    * ***`langchain.prompts`***: `LLM`에 전달한 프롬프트 → 템플릿화 → 관리하는 데 사용\n",
    "\n",
    "    * ***`CTransformers`***: 다양한 오픈 소스 `LLM` → `CPU` 환경에서 실행할 수 있는 라이브러리\n",
    "\n",
    "    * ***`OllamaLLM`***: `Ollama` 통해 제공되는 `Llama 3.1` 모델 호출 → 사용 가능\n",
    "\n",
    "```python\n",
    "\n",
    "            import streamlit as st\n",
    "            from langchain.prompts import PromptTemplate\n",
    "\n",
    "            # CTransformers → Llama, GPT4ALL-J, MPT, Falcon 같은 다양한 오픈 소스 모델 지원\n",
    "            from langchain_community.llms.ctransformers import CTransformers\n",
    "\n",
    "            # Ollama llama 3.1 model 연결하기\n",
    "            from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db672072",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866a9070",
   "metadata": {},
   "source": [
    "* **`이메일 응답 생성하기`**\n",
    "\n",
    "  * **a. `LLM 응답 생성 함수`**\n",
    "\n",
    "    * `getLLMResponse()` = 입력 → `LLM` → 이메일 응답 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cec4b8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb2b827",
   "metadata": {},
   "source": [
    "#### **2) `환경 설정`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8b6b37",
   "metadata": {},
   "source": [
    "* **a. `Python 3.12 버전 설치 확인 및 설치`**\n",
    "\n",
    "```bash\n",
    "\n",
    "        # 설치 가능한 Python 3.12 버전 목록 확인하기\n",
    "        pip intsll --list | grep \"3.12\"\n",
    "\n",
    "        # 최신 안정 버전인 Python 3.12.x 설치 (예시: 3.12.4)\n",
    "        pyenv install 3.12.4\n",
    "\n",
    "        # 이미 설치되어 있다면 \"already installed\" 메시지가 출력됨\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1632e92b",
   "metadata": {},
   "source": [
    "* **b. `새 가상환경 생성하기`**\n",
    "\n",
    "```bash\n",
    "\n",
    "        # 원하는 가상환경 이름으로 생성하기\n",
    "\n",
    "        # pyenv virtuenv <설치한 Python 버전> <원하는 가상환경 이름>\n",
    "\n",
    "        pyenv virtuenv 3.12.4. lc_multi_email_env\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59063ce",
   "metadata": {},
   "source": [
    "* **c. `가상 환경 활성화`**\n",
    "\n",
    "```bash\n",
    "\n",
    "        pyenv activate lc_multi_email_env\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1d38e1",
   "metadata": {},
   "source": [
    "* **d. `패키지 설치 및 안정화`**\n",
    "\n",
    "  * 가상환경 활성화된 상태에서 [`requirements_multi_email.txt`](/requirements_multi_email.txt)의 모든 패키지 설치하기\n",
    "\n",
    "  * [원본](https://github.com/sw-woo/hanbit-langchain/blob/main/requirements.txt)\n",
    "\n",
    "```bash\n",
    "\n",
    "        # requirements_multi_email.txt의 모든 패키지 설치\n",
    "        pip install -r requirements_multi_email.txt\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8bf31b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b39098c",
   "metadata": {},
   "source": [
    "* **`이메일 응답 생성하기`**\n",
    "\n",
    "  * `LLM` 응답을 생성하는 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fe279b",
   "metadata": {},
   "source": [
    "<small> <small>\n",
    "\n",
    "* **`Llama-2-7B-Chat용 래퍼: CPU에서 Llama 2 실행`** 보충 설명\n",
    "\n",
    "  * `C Transformers is the Python library that provides bindings for transformer models implemented in C/C++ using the GGML library`\n",
    "\n",
    "  * 모델 `local` 컴퓨터에 `다운` → `model=\"다운받은 모델명\"`을 `작성` → 진행\n",
    "\n",
    "  * 용량이 작을수록 `경량화` 된 버전 → *`※주의: 성능이 조금 떨어질 수 있음`*\n",
    "\n",
    "  * `GPT`와 같은 언어 모델\n",
    "    * 이러한 `파일 형식` 사용 → `모델`의 `학습된 가중치`와 `구조`를 `저장` → 이를 `추론`(`inference`) 시 `불러와서` `사용`\n",
    "    * `모델`을 `효율적`으로 `저장`하고 `불러올` 수 있게 함 → **`다양한 플랫폼`과 `환경`에서 `모델 추론`을 `원활`하게 할 수 있게 함**\n",
    "\n",
    "  * 따라서, **`GGUF` 및 `GGML` 파일 형식 = `GPT`와 같은 `언어 모델`의 `맥락`에서 `모델 추론`을 위해 `사용`되는 `중요한 파일 형식`**\n",
    "\n",
    "* [Llama-2-7B-Chat.ggmlv3.q8_0.bin](https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/tree/main)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb9b300",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd206334",
   "metadata": {},
   "source": [
    "#### **3) `새로운 버전으로 업그레이드` → 코드 재생성**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e0b6cc",
   "metadata": {},
   "source": [
    "* ***기존에 설치해뒀던 `llama3.2:3b`가 있어 `llama3.1:8b` 대신 사용 예정***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4498bb2c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a2d5f3",
   "metadata": {},
   "source": [
    "##### **`➀ 📝 다국어 이메일 생성기 코드 변경 사항`**\n",
    "\n",
    "* 최신 `Ollama` 버전은 파일 메타데이터 관리가 더 안정적인 **`GGUF` 포맷** 공식적으로 권장\n",
    "\n",
    "  * **a. `GGUF 포맷` 파일로 교체하기** \n",
    "    * *반면, 교재 속 버전 및 코드: `GGML` 파일 → 종종 `unknown type` 오류 반환함*\n",
    "    * 따라서 최신 모델 파일로 교체하는 것을 권장: **`llama2-7b.gguf`** \n",
    "      * 예시: `04_K_M.gguf`, `05_K_M.gguf` 등\n",
    "\n",
    "  * **b. `Modelfile` 수정 및 저장**\n",
    "    * `llama2-7b.ggub` 파일과 같은 폴더에 있어야 함\n",
    "\n",
    "  * **c. `Ollama` 모델 재등록**\n",
    "    * **`Ollama 서버 실행 중인 상태`** → 터미널 열기 → 파일들이 있는 폴더로 이동 → 다음 명령어 실행\n",
    "\n",
    "```bash\n",
    "\n",
    "                ollama create llama2-7b-imported -f Modelfile\n",
    "\n",
    "```\n",
    "\n",
    "<small> <small>\n",
    "\n",
    "* ⚠️ *`Modelfile` 저장 및 실행 과정에서 오류 발생으로 해당 모델 임시 보류*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6025d5df",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223bc7bd",
   "metadata": {},
   "source": [
    "* [**`app.py`**](../01_Multilingual-Email-Generator/app.py)\n",
    "\n",
    "<small> <small>\n",
    "\n",
    "```python\n",
    "\n",
    "                # app.py (새 코드)\n",
    "\n",
    "                import streamlit as st\n",
    "                from langchain.prompts import PromptTemplate\n",
    "                from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "                def getLLMResponse(form_input, email_sender, email_recipient, language):\n",
    "                    \"\"\"\n",
    "                    getLLMResponse 함수는 주어진 입력을 사용하여 LLM(대형 언어 모델)으로부터 이메일 응답을 생성합니다.\n",
    "\n",
    "                    매개변수:\n",
    "                    - form_input: 사용자가 입력한 이메일 주제.\n",
    "                    - email_sender: 이메일을 보낸 사람의 이름.\n",
    "                    - email_recipient: 이메일을 받는 사람의 이름.\n",
    "                    - language: 이메일이 생성될 언어 (한국어 또는 영어).\n",
    "\n",
    "                    반환값:\n",
    "                    - LLM이 생성한 이메일 응답 텍스트.\n",
    "                    \"\"\"\n",
    "\n",
    "                    # 💡 설치된 'llama3.2:3b' 모델을 사용하도록 지정\n",
    "                    llm = OllamaLLM(model=\"llama3.2:3b\", temperature=0.7)\n",
    "\n",
    "                    # 이메일 생성 프롬프트\n",
    "                    if language == \"한국어\":\n",
    "                        template = \"\"\" \n",
    "                        다음 정보를 사용하여 전문적인 이메일을 한국어로 작성해 주세요. \n",
    "                        \\n\\n이메일 주제: {email_topic}\n",
    "                        \\n보낸 사람: {sender}\\n받는 사람: {recipient}\n",
    "                        \\n\\n이메일 내용:\n",
    "                        \"\"\"\n",
    "                    else: \n",
    "                        template = \"\"\" \n",
    "                        Write a professional email in English using the following information.\n",
    "                        \\n\\nEmail Topic: {email_topic}\n",
    "                        \\nSender: {sender}\\nRecipient: {recipient}\n",
    "                        \\n\\nEmail content:\n",
    "                        \"\"\"\n",
    "\n",
    "                    # 최종 PROMPT 생성\n",
    "                    prompt = PromptTemplate(\n",
    "                        input_variables=[\"email_topic\", \"sender\", \"recipient\", \"language\"],\n",
    "                        template=template,\n",
    "                    )\n",
    "\n",
    "                    # LLM을 사용하여 응답 생성\n",
    "                    response = llm.invoke(prompt.format(email_topic=form_input, sender=email_sender, recipient=email_recipient, language=language))\n",
    "                    print(response)\n",
    "\n",
    "                    return response\n",
    "\n",
    "\n",
    "                st.set_page_config(\n",
    "                    page_title=\"이메일 생성기 📮\",\n",
    "                    page_icon='📮',\n",
    "                    layout='centered',\n",
    "                    initial_sidebar_state='collapsed'\n",
    "                )\n",
    "                st.header(\"이메일 생성기 📮 \")\n",
    "\n",
    "                # 이메일 작성 언어 선택 \n",
    "                language_choice = st.selectbox('이메일을 작성할 언어를 선택하세요:', ['한국어', 'English'])\n",
    "\n",
    "                form_input = st.text_area('이메일 주제를 입력하세요', height=100)\n",
    "\n",
    "                # 사용자 입력을 받기 위한 UI 열 생성\n",
    "                col1, col2 = st.columns([10, 10])\n",
    "                with col1:\n",
    "                    email_sender = st.text_input('보낸 사람 이름')\n",
    "                with col2:\n",
    "                    email_recipient = st.text_input('받는 사람 이름')\n",
    "\n",
    "                submit = st.button(\"생성하기\")\n",
    "\n",
    "                # '생성하기' 버튼이 클릭되면, 아래 코드를 실행하기\n",
    "                if submit:\n",
    "                    if not form_input or not email_sender or not email_recipient:\n",
    "                        st.error(\"이메일 주제, 보낸 사람, 받는 사람 이름을 모두 입력해야 합니다.\")\n",
    "                    else:\n",
    "                        with st.spinner('생성 중입니다...'):\n",
    "                            response = getLLMResponse(form_input, email_sender, email_recipient, language_choice)\n",
    "                            st.write(response)\n",
    "\n",
    "\n",
    "                # 실행 방법: 해당 폴더로 이동\n",
    "                # 터미널: streamlit run app.py\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc9b04b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90484f6",
   "metadata": {},
   "source": [
    "* [**`app.py`**](../01_Multilingual-Email-Generator/app.py) - **`local 환경에 맞춰 실행`**\n",
    "\n",
    "  * **a. `Ollama 모델 이름 일치`시키기**\n",
    "\n",
    "```python\n",
    "\n",
    "            model=\"llama2-7b-imported\"          # 사용할 모델 이름\n",
    "\n",
    "```\n",
    "* \n",
    "  * **b. `Ollama 서버 실행`**\n",
    "\n",
    "```bash\n",
    "\n",
    "            # 코드 실행 전\n",
    "            ollama serve                        # 백그라운드에서 서버가 실행 중이어야 함\n",
    "\n",
    "```\n",
    "* \n",
    "  * **c. `모델 파일 형식`**\n",
    "\n",
    "    * `GGML` 파일 ❌ → **`GGUF` 파일 ⭕️**\n",
    "\n",
    "<br>\n",
    "\n",
    "* **`변경 사항 요약`** - *`교재 코드` vs `app.py`*\n",
    "  \n",
    "  | 구분      | 교재 초기 코드                                   | 수정된 app.py 코드                                         | 변경 이유 및 목적                                                                             |\n",
    "  |---------|--------------------------------------------|-------------------------------------------------------|----------------------------------------------------------------------------------------|\n",
    "  | `LLM` 모듈  | `from langchain.llms import CTransformers`   | `from langchain_ollama.llms import OllamaLLM`           | * **`Ollama` 환경 변경**: `CTransformers` 대신 **`로컬 Ollama 서버`에 `직접 연결`하여 `llama3.2:3b` 모델을 `사용`하기 위함**          |\n",
    "  | `LLM` 객체  | `llm = CTransformers(...)`                   | `llm = OllamaLLM(model=\"llama3.2:3b\", temperature=0.7)` | * **`사용자 환경` 맞춤**: `CTransformers` 코드 `제거`/`주석 처리` → **`Ollama`에 설치된 `llama3.2:3b` 모델을 `지정`하여 `실제 작동`하도록 설정** |\n",
    "  | 프롬프트    | `LLM`에 더 명확한 지침을 제공하도록 템플릿 미세 변경             | LLM에 더 명확한 지침을 제공하도록 템플릿 미세 변경                        | * **`지시 강화`**: 모델에게 전문적인 이메일을 **`특정 언어`** *(한국어/영어)* 로 **`작성`하라고 `명확히` 지시 → 원하는 `결과물 품질`과 `일관성`을 `높임`**                |\n",
    "  | 불필요한 코드 | `CTransformers` 및 `Llama-2-7b-chat` 관련 주석 및 코드 | *모두 제거* 또는 *주석 처리*                                        | * **`클린업` 및 `효율화`**: 사용하지 않는 라이브러리 및 모델 관련 코드 정리 → **`가독성` 높임**                                       |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d781819e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d668a4b3",
   "metadata": {},
   "source": [
    "##### **`➁ 📝 LLM 효율성 분석`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81bf1cc",
   "metadata": {},
   "source": [
    "* **`Llama-2-7b-chat`** (*CTransformers*) **모델을 제거하는 것이 효율적인 이유**\n",
    "\n",
    "  * **a. `통합 환경 구성` 및 `관리의 단순화`** *(Single Source of Truth)*\n",
    "\n",
    "    * **`Ollama`의 역할**: `Ollama` = 다양한 `LLM`을 **`다운로드`, `관리` 및 `실행`하는 `표준화`된 `서버` 역할**\n",
    "      * 예시: `llama3.2:3b`, `exaone3.5:7.8b` 등\n",
    "\n",
    "    * **`비효율성`**: 만약 OllamaLLM (Ollama를 통해 모델 실행)과 CTransformers (별도의 라이브러리를 통해 모델 파일 직접 로드 및 실행)를 동시에 사용하려고 하면, `다음과 같은 문제 발생`\n",
    "\n",
    "      * *`자원 중복`*: 두 라이브러리가 *`메모리` / `CPU` / `GPU` 자원을 놓고 `경쟁`하거나 `충돌`할 가능성*\n",
    "\n",
    "      * *`설정 복잡성`*: *`모델 파일 경로`, `로드 설정`* *(quantization, CPU/GPU 할당 등)* 을 `Ollama`와 `CTransformers` 두 곳 `모두`에서 `관리` → *`설정`이 `복잡`해지고 `오류 가능성`이 `높아짐`*\n",
    "\n",
    "    * **`효율성`**: `OllamaLLM` 하나만 사용 = `모든 모델 실행`을 `Ollama 서버`에 맡김 → **`관리 포인트`가 `하나로 통일`되어 `매우` `깔끔`하고 `효율적`**\n",
    "\n",
    "  * **b. `성능` 및 `기능 측면`의 `이점`**\n",
    "\n",
    "    * **`최신 기술 활용`**: **`Ollama`는 `지속적`으로 `업데이트`** → **`최신 모델`과 `최적화`된 실행 `환경`을 `지원`함**\n",
    "      * *최신 모델 예시: `Llama 3.2`, `Exaone 3.5` 등*\n",
    "      * *최적화된 실행 환경 예시: `CUDA`, `Metal` 등*\n",
    "\n",
    "    * **`모델 교체의 용이성`**: \n",
    "\n",
    "      * **`Ollama 환경`**: **`코드 크게 변경 ❌` → `model=\"모델명\"`만 바꿈 → `즉시 다른 모델`** *(예: llama3.2:3b → exaone3.5:7.8b)* 로 **`실험 가능`**\n",
    "\n",
    "      * *`CTransformers`*: *`모델을 변경할 때마다` `새로운 .gguf 파일` (혹은 `.ggml 파일`)을 `다운로드` → `로드 설정`을 `다시 구성`해야 하는 번거로움 발생*\n",
    "\n",
    "\n",
    "* 결론적으로, **현재 `Ollama`를 `사용`하고 있는 경우**, **`OllamaLLM`만을 사용 → `통합된 환경`에서 `최신 모델`을 `활용`하는 것 = `가장 효율적`이고 `안정적인 로컬 LLM 개발 방식`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6ee481",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbd75a0",
   "metadata": {},
   "source": [
    "##### **`➂ 환경 설정`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515e5f63",
   "metadata": {},
   "source": [
    "* ⚠️ 실행 전 필수 준비 사항\n",
    "\n",
    "  * `mac 터미널`\n",
    "\n",
    "```bash\n",
    "\n",
    "            # Ollama 서버가 실행 중이어야 함\n",
    "            ollama serve\n",
    "\n",
    "            # `llama3.2:3b` 모델이 설치되어 있어야 함\n",
    "            ps aux | grep ollama            # ollama 설치된 모델 확인하기\n",
    "            \n",
    "            # 설치된 모델이 없다면 설치하기\n",
    "            ollama run llama3.2:3b\n",
    "\n",
    "            # 설치된 목록 확인하기 \n",
    "            ollama list                     # 테스트로 정상 동작하는지 확인하기 \n",
    "\n",
    "            # `langchain`, `langchain-ollama` 라이브러리가 설치되어 있어야 함\n",
    "\n",
    "            pip install langchain langchain-ollama\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8181e0e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e48ad7f",
   "metadata": {},
   "source": [
    "##### **`➃ Python Script로 실행하기`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b27ba6",
   "metadata": {},
   "source": [
    "* [**`app.py`**](../01_Multilingual-Email-Generator/app.py)\n",
    "\n",
    "  * ![Streamlit_1](../01_Multilingual-Email-Generator/img/app_py_1.png)\n",
    "\n",
    "  * ![Streamlit_2](../01_Multilingual-Email-Generator/img/app_py_2.png)\n",
    "\n",
    "  * ![Streamlit_3](../01_Multilingual-Email-Generator/img/app_py_3.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20352047",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5e3d0b",
   "metadata": {},
   "source": [
    "##### **`➄ 주피터 노트북으로 실행해보기`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62a9547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 패키지 임포트 해보기\n",
    "import sys\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "print(\"✅ 필요한 라이브러리 임포트 완료.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e19ce52",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* **`✅ 필요한 라이브러리 임포트 완료.`** - (`0.9s`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c49058",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLLMResponse(form_input, email_sender, email_recipient, language):\n",
    "    \"\"\"\n",
    "    getLLMResponse 함수는 주어진 입력을 사용하여 LLM(대형 언어 모델)으로부터 이메일 응답을 생성함\n",
    "\n",
    "    매개변수:\n",
    "    - form_input: 사용자가 입력한 이메일 주제.\n",
    "    - email_sender: 이메일을 보낸 사람의 이름.\n",
    "    - email_recipient: 이메일을 받는 사람의 이름.\n",
    "    - language: 이메일이 생성될 언어 (한국어 또는 영어).\n",
    "\n",
    "    반환값:\n",
    "    - LLM이 생성한 이메일 응답 텍스트.\n",
    "    \"\"\"\n",
    "\n",
    "    # OllamaLLM 객체 생성 = 'llama3.2:3b' 모델 사용\n",
    "    # 참고: Ollama가 다른 포트에서 실행 중이라면 base_url을 지정해야 함\n",
    "    try:\n",
    "        llm = OllamaLLM(model=\"llama3.2:3b\", temperature=0.7)\n",
    "    except Exception as e:\n",
    "        print(f\"Ollama 연결 오류: {e}\", file=sys.stderr)\n",
    "        print(\"Ollama 서버가 실행 중인지, 모델이 설치되어 있는지 확인해 주세요.\", file=sys.stderr)\n",
    "        return \"ERROR: Ollama LLM connection failed.\"\n",
    "\n",
    "\n",
    "    # 이메일 생성 프롬프트\n",
    "    if language == \"한국어\":\n",
    "        template = \"\"\" \n",
    "        다음 정보를 사용하여 전문적인 이메일을 한국어로 작성해 주세요. \n",
    "        \n",
    "        이메일 주제: {email_topic}\n",
    "        보낸 사람: {sender}\n",
    "        받는 사람: {recipient}\n",
    "        \n",
    "        이메일 내용:\n",
    "        \"\"\"\n",
    "    else: \n",
    "        template = \"\"\" \n",
    "        Write a professional email in English using the following information.\n",
    "        \n",
    "        Email Topic: {email_topic}\n",
    "        Sender: {sender}\n",
    "        Recipient: {recipient}\n",
    "        \n",
    "        Email content:\n",
    "        \"\"\"\n",
    "\n",
    "    # 최종 PROMPT 생성\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"email_topic\", \"sender\", \"recipient\", \"language\"],\n",
    "        template=template,\n",
    "    )\n",
    "\n",
    "    # LLM → 응답 생성\n",
    "    response = llm.invoke(prompt.format(email_topic=form_input, sender=email_sender, recipient=email_recipient, language=language))\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e39e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 테스트 실행\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "# === 1차 테스트: 한국어 이메일 ===\n",
    "\n",
    "email_topic_kr = \"다음 분기 마케팅 전략 변경 사항 보고\"\n",
    "email_sender_kr = \"김영희 부장\"\n",
    "email_recipient_kr = \"이철수 팀장\"\n",
    "language_choice_kr = \"한국어\"\n",
    "\n",
    "print(f\"\\n--- [1차 테스트: {language_choice_kr}] ---\")\n",
    "print(\"생성 중... (Ollama 응답 대기)\")\n",
    "response_kr = getLLMResponse(email_topic_kr, email_sender_kr, email_recipient_kr, language_choice_kr)\n",
    "\n",
    "print(\"\\n[생성된 한국어 이메일 내용]\\n\")\n",
    "print(response_kr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8004c59",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* **`test_1`(*`ver_ko`*)** - (`31.8s`)\n",
    "\n",
    "<br>\n",
    "\n",
    "* \n",
    "  * *`Llama 모델이 한국어에 가진 한계점이 보임`*\n",
    "\n",
    "    ```markdown\n",
    "\n",
    "    --- [1차 테스트: 한국어] ---\n",
    "    생성 중... (Ollama 응답 대기)\n",
    "\n",
    "    [생성된 한국어 이메일 내용]\n",
    "\n",
    "    subject: 다음 분기 마케팅 전략 변경 사항 보고\n",
    "\n",
    "    dear 이철수 팀장,\n",
    "\n",
    "    안녕하세요. kimyounghee입니다.\n",
    "\n",
    "    이번 달은 새로운 분기로进入합니다. 이에 따라 마케팅 전략도 변경되어야 합니다.\n",
    "\n",
    "    제가 제안한 변경 사항은 다음과 같습니다.\n",
    "\n",
    "    1.  **마케팅 objetivo의 변경** : 현재의 objectives를 review하고, 새로 정의하는 objective가 필요합니다.\n",
    "    2.  **Target audience의 확장** : 새로운 target audience를 추가하여 마케팅 전략을 변경할 수 있습니다.\n",
    "    3.  **마케팅 채널의 diversification** : 기존의 channel에 새로운 channel을 추가하여 마케팅 전략을 보다 đa dạng화할 수 있습니다.\n",
    "    4.  **마케팅 budget의 increase** : 현재의 budget를 review하고, 새로 증가하는 budget가 필요합니다.\n",
    "\n",
    "    이 변경 사항은 팀과 동등한 의견을 benöt습니다.因此, I propose a meeting to discuss these changes in more detail. The proposed meeting date and time is [insert date and time].\n",
    "\n",
    "    If you have any questions or concerns, please don't hesitate to reach out to me.\n",
    "\n",
    "    Thank you for your time and consideration.\n",
    "\n",
    "    Best regards,\n",
    "\n",
    "    kimyounghee 부장\n",
    "\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a975a368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 2차 테스트: 영어 이메일 ===\n",
    "email_topic_en = \"Follow-up on the Q3 Financial Review meeting action items\"\n",
    "email_sender_en = \"Alice Johnson\"\n",
    "email_recipient_en = \"Bob Williams\"\n",
    "language_choice_en = \"English\"\n",
    "\n",
    "print(f\"\\n--- [2차 테스트: {language_choice_en}] ---\")\n",
    "print(\"생성 중... (Ollama 응답 대기)\")\n",
    "response_en = getLLMResponse(email_topic_en, email_sender_en, email_recipient_en, language_choice_en)\n",
    "\n",
    "print(\"\\n[생성된 영어 이메일 내용]\\n\")\n",
    "print(response_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c124937",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* **`test_2`** (*`ver_eng`*) - (`21.6s`)\n",
    "\n",
    "    ```markdown\n",
    "\n",
    "    --- [2차 테스트: English] ---\n",
    "    생성 중... (Ollama 응답 대기)\n",
    "\n",
    "    [생성된 영어 이메일 내용]\n",
    "\n",
    "    Here is a professional email:\n",
    "\n",
    "    Subject: Follow-up on the Q3 Financial Review Meeting Action Items\n",
    "\n",
    "    Dear Bob,\n",
    "\n",
    "    I hope this email finds you well. I am writing to follow up on the action items discussed during our Q3 Financial Review meeting, which took place on [Date of Meeting]. As per our agreement, I would like to confirm the status of each item and request any updates or progress reports.\n",
    "\n",
    "    Could you please provide me with a brief update on the following action items:\n",
    "\n",
    "    * [Item 1: Insert specific action item 1]\n",
    "    * [Item 2: Insert specific action item 2]\n",
    "    * [Item 3: Insert specific action item 3]\n",
    "\n",
    "    Additionally, I would appreciate any information regarding the current status of our Q4 budget projections and any potential adjustments that may be required.\n",
    "\n",
    "    If you have any questions or concerns, please do not hesitate to reach out. I look forward to hearing back from you soon.\n",
    "\n",
    "    Best regards,\n",
    "\n",
    "    Alice Johnson\n",
    "\n",
    "    ```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc_multi_email_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
