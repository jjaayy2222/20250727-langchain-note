{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4553e3c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8616d8",
   "metadata": {},
   "source": [
    "* 출처: LangChain 공식 문서 또는 해당 교재명\n",
    "* 원본 URL: https://smith.langchain.com/hub/teddynote/summary-stuff-documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d5f375",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121ca113",
   "metadata": {},
   "source": [
    "### **`토큰 텍스트 분할 (TokenTextSplitter)`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cd7463",
   "metadata": {},
   "source": [
    "* **`언어 모델 = 토큰 제한` → 토큰 제한을 초과하지 않아야 함**\n",
    "\n",
    "* **`TokenTextSplitter` = 텍스트를 `토큰 수`를 `기반`으로 `청크`를 `생성`할 때 `유용`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aab8aa5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e64076f",
   "metadata": {},
   "source": [
    "### **`tiktoken`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fd3c6c",
   "metadata": {},
   "source": [
    "* **`tiktoken` = `OpenAI`에서 만든 빠른 `BPE Tokenizer`**\n",
    "\n",
    "* 사전 `VS Code` 터미널에 설치할 것\n",
    "\n",
    "```bash\n",
    "        pip install --upgrade --quiet langchain-text-splitters tiktoken\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbf2b0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data/appendix-keywords.txt 파일을 열어서 f라는 파일 객체 생성하기\n",
    "with open(\"../07_Text_Splitter/data/appendix-keywords.txt\") as f:\n",
    "    file = f.read()                                         # 파일의 내용을 읽어서 file 변수에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5fe26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(file))                   # <class 'str'>\n",
    "print(len(file))                    # 5733"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5505dcdf",
   "metadata": {},
   "source": [
    "* 파일로부터 읽은 파일의 일부 내용을 출력해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3c41c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일으로부터 읽은 내용 일부 출력하기\n",
    "\n",
    "print(file[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67b6cc2",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 셀 출력\n",
    "\n",
    "    ```markdown\n",
    "    Semantic Search\n",
    "\n",
    "    정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
    "    예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
    "    연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
    "\n",
    "    Embedding\n",
    "\n",
    "    정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
    "    예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n",
    "    연관키워드: 자연어 처리, 벡터화, 딥러닝\n",
    "\n",
    "    Token\n",
    "\n",
    "    정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다.\n",
    "    예시: 문장 \"나는 학교에 간다\"를 \"나는\", \"학교에\", \"간다\"로 분할합니다.\n",
    "    연관키워드: 토큰화, 자연어\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e853f14d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bcea0c",
   "metadata": {},
   "source": [
    "### **`RecursiveCharacterTextSplitter` 사용**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c504a8b",
   "metadata": {},
   "source": [
    "* **`CharacterTextSplitter`** 사용 → 텍스트 분할\n",
    "\n",
    "* **`from_tiktoken_encoder` 메서드 사용** → `Tiktoken` 인코더 기반의 텍스트 분할기를 `초기화`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb695f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300,                                 # 청크 크기 300으로 설정\n",
    "    chunk_overlap=0,                                # 청크 간 중복되는 부분이 없도록 설정하기\n",
    ")\n",
    "\n",
    "\n",
    "# file 텍스트를 청크 단위로 분할하기\n",
    "texts = text_splitter.split_text(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f48ca3",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 셀 출력 (4.0s)\n",
    "\n",
    "    ```markdown\n",
    "    Created a chunk of size 358, which is longer than the specified 300\n",
    "    Created a chunk of size 315, which is longer than the specified 300\n",
    "    Created a chunk of size 305, which is longer than the specified 300\n",
    "    Created a chunk of size 366, which is longer than the specified 300\n",
    "    Created a chunk of size 330, which is longer than the specified 300\n",
    "    Created a chunk of size 351, which is longer than the specified 300\n",
    "    Created a chunk of size 378, which is longer than the specified 300\n",
    "    Created a chunk of size 361, which is longer than the specified 300\n",
    "    Created a chunk of size 350, which is longer than the specified 300\n",
    "    Created a chunk of size 362, which is longer than the specified 300\n",
    "    Created a chunk of size 335, which is longer than the specified 300\n",
    "    Created a chunk of size 353, which is longer than the specified 300\n",
    "    Created a chunk of size 358, which is longer than the specified 300\n",
    "    Created a chunk of size 336, which is longer than the specified 300\n",
    "    Created a chunk of size 324, which is longer than the specified 300\n",
    "    Created a chunk of size 337, which is longer than the specified 300\n",
    "    Created a chunk of size 307, which is longer than the specified 300\n",
    "    Created a chunk of size 361, which is longer than the specified 300\n",
    "    Created a chunk of size 354, which is longer than the specified 300\n",
    "    Created a chunk of size 378, which is longer than the specified 300\n",
    "    Created a chunk of size 381, which is longer than the specified 300\n",
    "    Created a chunk of size 365, which is longer than the specified 300\n",
    "    Created a chunk of size 377, which is longer than the specified 300\n",
    "    Created a chunk of size 329, which is longer than the specified 300\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bc012d",
   "metadata": {},
   "source": [
    "* 분할된 청크의 개수 출력해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c13fc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(texts))                       # 51"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f476473",
   "metadata": {},
   "source": [
    "* `texts` 리스트의 첫 번쨰 요소 출력하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11e8973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# texts 리스트의 첫 번째 요소 출력하기\n",
    "\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae2a5e1",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 셀 출력\n",
    "\n",
    "    ```markdown\n",
    "    Semantic Search\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b33739",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468c5dc3",
   "metadata": {},
   "source": [
    "#### **`참고`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44fc1fd",
   "metadata": {},
   "source": [
    "* **`CharacterTextSplitter.from_tiktoken_encoder`를 사용하는 경우**:\n",
    "\n",
    "  * 텍스트: **`CharacterTextSplitter`에 의해서만 분할** \n",
    "\n",
    "  * **`tiktoken 토크나이저`** = `분할된 텍스트`를 `병합`하는 데 사용\n",
    "\n",
    "  * **즉, `분할된 텍스트`가 `tiktoken 토크나이저로 측정한 청크 크기보다 클 수 있음`을 의미**\n",
    "\n",
    "<br>\n",
    "\n",
    "* **`RecursiveCharacterTextSplitter.from_tiktoken_encoder`를 사용하는 경우**:\n",
    "\n",
    "  * **`분할된 텍스트`가 언어 모델에서 허용하는 `토큰의 청크 크기보다 크지 않도록 할 수 있음`**\n",
    "\n",
    "    * **`각 분할은 크기가 더 큰 경우 재귀적으로 분할함`** \n",
    "\n",
    "  * **`tiktoken 분할기` 직접 로드 → `각 분할`이 `청크 크기보다 작음`을 보장**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c0f837",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f754c3",
   "metadata": {},
   "source": [
    "### **`TokenTextSplitter`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54fd993",
   "metadata": {},
   "source": [
    "* **`TokenTextSplitter` 클래스 사용 → 텍스트를 `토큰 단위`로 분할**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5585195c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import TokenTextSplitter\n",
    "\n",
    "text_splitter = TokenTextSplitter(\n",
    "    chunk_size=200,                             # 청크 크기를 10으로 설정\n",
    "    chunk_overlap=0,                            # 청크 간 중복을 0으로 설정\n",
    ")\n",
    "\n",
    "# state_of_the_union 텍스트를 청크로 분할하기\n",
    "texts = text_splitter.split_text(file)\n",
    "\n",
    "# 분할된 텍스트의 첫 번째 청크를 출력하기\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9398a37b",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 셀 출력\n",
    "\n",
    "    ```markdown\n",
    "    Semantic Search\n",
    "\n",
    "    정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
    "    예시: 사용자가 \"태양계 행성\"�\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbd0fb2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a628fe",
   "metadata": {},
   "source": [
    "### **`spaCy`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9ceeda",
   "metadata": {},
   "source": [
    "* **`spaCy`** = `Python`과 `Cython` 프로그래밍 언어로 작성된 **`고급 자연어 처리`를 위한 `오픈 소스 소프트웨어 라이브러리`**\n",
    "\n",
    "* `NLTK`의 또 다른 대안 = `spaCy tokenizer`를 사용하는 것\n",
    "\n",
    "  * `텍스트`가 `분할`되는 방식: **`spaCy tokenizer`에 의해 분할**\n",
    "\n",
    "  * **`chunk size`가 측정되는 방법**: `문자 수`로 측정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11b253d",
   "metadata": {},
   "source": [
    "* 사전 `VS Code` 터미널에 설치할 것\n",
    "\n",
    "```bash\n",
    "        pip install --upgrade --quiet spacy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff366476",
   "metadata": {},
   "source": [
    "* **`en_core_web_sm` 모델 다운로드**\n",
    "\n",
    "```bash\n",
    "        python -m spacy download en_core_web_sm --quiet\n",
    "```\n",
    "\n",
    "↓\n",
    "\n",
    "```bash\n",
    "        ✔ Download and installation successful\n",
    "        You can now load the package via spacy.load('en_core_web_sm')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e973d10b",
   "metadata": {},
   "source": [
    "* **`appendix-keywords.txt` 파일 읽기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9179a499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data/appendix-keywords.txt 파일을 열어서 f라는 파일 객체 생성하기\n",
    "with open(\"../07_Text_Splitter/data/appendix-keywords.txt\") as f:\n",
    "    file = f.read()                                         # 파일의 내용을 읽어서 file 변수에 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba74d3f",
   "metadata": {},
   "source": [
    "* 일부 내용 출력해서 확인해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb4066e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일으로부터 읽은 내용 일부 출력해보기\n",
    "\n",
    "print(file[:350])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561e732b",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 셀 출력\n",
    "\n",
    "    ```markdown\n",
    "    Semantic Search\n",
    "\n",
    "    정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
    "    예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
    "    연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
    "\n",
    "    Embedding\n",
    "\n",
    "    정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
    "    예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n",
    "    연관키워드: 자연어 처\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d375682",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc8ff01",
   "metadata": {},
   "source": [
    "* **`SpacyTextSplitter` 클래스 사용 → 텍스트 분할기 생성**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c35a628",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from langchain_text_splitters import SpacyTextSplitter\n",
    "\n",
    "\n",
    "# 경고 메시지를 무시하도록 설정하기\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# SpacyTextSplitter 생성\n",
    "text_splitter = SpacyTextSplitter(\n",
    "    chunk_size=200,                         # 청크 크기를 200으로 설정\n",
    "    chunk_overlap=50,                       # 청크 간 중복을 50으로 설정\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7140da",
   "metadata": {},
   "source": [
    "* **`text_splitter` 객체의 `split_text` 메서드 사용 → `file` 텍스트 분할**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889fb3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_splitter를 사용하여 file 텍스트를 분할하기\n",
    "texts = text_splitter.split_text(file)\n",
    "\n",
    "# 분할된 텍스트의 첫 번째 요소를 출력하기\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f1d2f3",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 셀 출력\n",
    "\n",
    "    ```markdown\n",
    "    Semantic Search\n",
    "\n",
    "    정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된\n",
    "\n",
    "    결과를 반환하는 검색 방식입니다.\n",
    "\n",
    "\n",
    "    예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28328198",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e2c627",
   "metadata": {},
   "source": [
    "### **❌ `SentenceTransformers`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e328d768",
   "metadata": {},
   "source": [
    "* **`SentenceTransformersTokenTextSplitter` = `sentence-transformer 모델`에 `특화`된 텍스트 분할기**\n",
    "\n",
    "* 기본 동작: 사용하고자 하는 `sentence transformer 모델`의 `토큰 윈도우`에 맞게 `텍스트`를 `청크`로 `분할`하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8cbb59",
   "metadata": {},
   "source": [
    "* 사전에 `VS Code` 터미널에 설치할 것\n",
    "\n",
    "```bash\n",
    "        pip install sentence-transformers\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e90fe2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cdbf15",
   "metadata": {},
   "source": [
    "* **❌ `파이썬 버전 충돌로 해당 패키지 실제 실행 불가`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda4ce39",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ceaa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import SentenceTransformersTokenTextSplitter\n",
    "\n",
    "# 문장 분할기 생성\n",
    "splitter = SentenceTransformersTokenTextSplitter(\n",
    "    chunk_size=200, \n",
    "    chunk_overlap=0,\n",
    ")\n",
    "\n",
    "# 분할할 텍스트\n",
    "text = \"The quick brown fox jumps over the lazy dog. This is another sentence. And a third one.\"\n",
    "\n",
    "# 텍스트 분할\n",
    "chunks = splitter.split_text(text)\n",
    "\n",
    "# 결과 출력\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"청크 {i+1}: {chunk}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d9550c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import SentenceTransformersTokenTextSplitter\n",
    "\n",
    "# 문장 분할기 생성\n",
    "splitter = SentenceTransformersTokenTextSplitter(\n",
    "    chunk_size=200, \n",
    "    chunk_overlap=0,                        # 청크 간 중복 = 0 으로 설정하기\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a752d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data/appendix-keywords.txt 파일을 열어서 f라는 파일 객체 생성하기\n",
    "with open(\"../07_Text_Splitter/data/appendix-keywords.txt\") as f:\n",
    "    file = f.read()                                         # 파일의 내용을 읽어서 file 변수에 저장\n",
    "\n",
    "# 파일로부터 읽은 내용을 일부 출력하기\n",
    "print(file[:350])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146b412e",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 셀 출력\n",
    "\n",
    "    ```markdown\n",
    "    Semantic Search\n",
    "\n",
    "    정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
    "    예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
    "    연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
    "\n",
    "    Embedding\n",
    "\n",
    "    정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
    "    예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n",
    "    연관키워드: 자연어 처\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd5007e",
   "metadata": {},
   "source": [
    "* `file` 변수에 담긴 텍스트의 토큰 개수 세는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db91e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰 개수 세기\n",
    "\n",
    "count_start_and_stop_tokens = 2                     # 시작과 종료 토큰의 개수= 2로 설정\n",
    "\n",
    "# 텍스트의 토큰 개수에서 시작과 종료 토큰의 개수 제외한 후 출력하기\n",
    "text_token_count = splitter.count_tokens(\n",
    "    text=file) - count_start_and_stop_tokens\n",
    "print(text_token_count)                             # 계산된 텍스트 토큰 개수 출력 = 7686"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb9faf0",
   "metadata": {},
   "source": [
    "* `splitter.split_text()` 함수 사용 → `text_to_split` 변수에 저장된 텍스트를 `chunk` 단위로 분할하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b8b3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트를 청크로 분할하기\n",
    "\n",
    "text_chunks = splitter.split_text(text=file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76eed21",
   "metadata": {},
   "source": [
    "* 첫 번째 청크를 출력해 내용 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210e5196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0번째 청크 출력하기\n",
    "\n",
    "print(text_chunks[1])                               # 분할된 텍스트 청크 중 두 번째 청크 출력함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf7c44e",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 셀 출력\n",
    "\n",
    "    ```markdown\n",
    "    . 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 [UNK] 합니다. [UNK] : \" 사과 \" 라는 단어를 [ 0. 65, - 0. 23, 0. 17 ] 과 [UNK] 벡터로 표현합니다. 연관키워드 : 자연어 처리, 벡터화, 딥러닝 token 정의 : 토큰은 텍스트를 더 작은 [UNK] 분할하는 [UNK] 의미합니다. 이는 일반적으로 단어, 문장, [UNK] 구절일 수 [UNK]. [UNK] : 문장 \" 나는 학교에 간다 \" 를 \" 나는 \", \" 학교에 \", \" 간다 \" 로 분할합니다. 연관키워드 : 토큰화, 자연어 처리, 구문 분석 tokenizer 정의 : 토크\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c52c9f7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cdb3f6",
   "metadata": {},
   "source": [
    "### **`NLTK`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266c085d",
   "metadata": {},
   "source": [
    "* **`Natural Language Toolkit (NLTK)`** \n",
    "\n",
    "  * `Python` 프로그래밍 언어로 작성된 `영어 자연어 처리(NLP)`를 위한 라이브러리와 프로그램 모음\n",
    "\n",
    "  * **`텍스트 데이터의 전처리, 토큰화, 형태소 분석, 품사 태깅 등 다양한 NLP 작업 수행 가능`**\n",
    "\n",
    "<br>\n",
    "\n",
    "* 단순히 `\"\\n\\n\"`으로 분할하는 대신, **`NLTK tokenizers`** 를 기반으로 텍스트를 분할하는 데 사용\n",
    "\n",
    "  * **`텍스트 분할` 방법: `NLTK tokenizer`에 의해 분할**\n",
    "  * **`chunk 크기` 측정 방법: `문자 수`에 의해 측정**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83241a2b",
   "metadata": {},
   "source": [
    "* 사전에 `VS Code` 터미널에 설치할 것\n",
    "\n",
    "```bash\n",
    "        pip install -qU nltk\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f414108d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data/appendix-keywords.txt 파일을 열어서 f라는 파일 객체 생성하기\n",
    "with open(\"../07_Text_Splitter/data/appendix-keywords.txt\") as f:\n",
    "    file = f.read()                                         # 파일의 내용을 읽어서 file 변수에 저장\n",
    "\n",
    "# 파일로부터 읽은 내용을 일부 출력하기\n",
    "print(file[:350])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d6cde3",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 셀 출력\n",
    "\n",
    "    ```markdown\n",
    "    Semantic Search\n",
    "\n",
    "    정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
    "    예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
    "    연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
    "\n",
    "    Embedding\n",
    "\n",
    "    정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
    "    예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n",
    "    연관키워드: 자연어 처\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7219a0be",
   "metadata": {},
   "source": [
    "* **`NLTKTextSplitter` 클래스 사용 → 텍스트 분할기 생성**\n",
    "\n",
    "* **`chunk_size` 매개변수를 1000으로 설정 → `텍스트`를 `최대 1000자 단위`로 `분할`하도록 지정**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47f062b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import NLTKTextSplitter\n",
    "\n",
    "text_splitter = NLTKTextSplitter(\n",
    "    chunk_size=200,                                         # 청크 크기 = 200\n",
    "    chunk_overlap=0,                                        # 청크 간 중복 = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61adfba5",
   "metadata": {},
   "source": [
    "* **`text_splitter` 객체의 `split_text 메서드`를 사용하여 `file 텍스트`를 `분할`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a269f23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_splitter 사용 → file 텍스트 분할\n",
    "texts = text_splitter.split_text(file)\n",
    "\n",
    "# 분할된 텍스트의 첫 번째 요소를 출력\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e3176e",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 셀 출력\n",
    "\n",
    "    ```markdown\n",
    "    Semantic Search\n",
    "\n",
    "    정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
    "\n",
    "    예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e62d163",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6479bddf",
   "metadata": {},
   "source": [
    "### **`KoNLPy의 Kkma 분석기를 사용한 한국어 토큰 분할`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438d83b7",
   "metadata": {},
   "source": [
    "* **`한국어 텍스트`**: **`KoNLPY`에 `Kkma`(Korean Knowledge Morpheme Analyzer)라는 형태소 분석기가 포함되어 있음**\n",
    "\n",
    "* **`Kkma`**\n",
    "  * `한국어 텍스트`에 대한 상세한 **`형태소 분석`을 제공**\n",
    "\n",
    "  * `문장`을 `단어`로, `단어`를 **`각각의 형태소`로 분해 → 각 토큰에 대한 품사를 식별**\n",
    "\n",
    "  * 텍스트 블록을 `개별 문장`으로 `분할` 가능 **→ `긴 텍스트 처리에 특히 유용`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1c9dfa",
   "metadata": {},
   "source": [
    "* **`Kkma 사용시 주의사항`**\n",
    "\n",
    "  * 장점: **`상세한 분석`** → *정밀성이 `처리 속도`에 영향을 미칠 수 있음*\n",
    "\n",
    "  * **`Kkma`는 신속한 텍스트 처리 < `분석적 깊이 우선시` 되는 애플리케이션에 가장 적합**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2eb7e3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e23c00",
   "metadata": {},
   "source": [
    "* 사전에 `VS Code`에 설치할 것\n",
    "```bash\n",
    "        pip install -qU konlpy\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "* \n",
    "  * **`KoNLPy`**\n",
    "    * **`한국어 자연어 처리를 위한 파이썬 패키지`**\n",
    "    * **`형태소 분석`, `품사 태깅`, `구문 분석` 등의 기능 제공**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1043e62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data/appendix-keywords.txt 파일을 열어서 f라는 파일 객체 생성하기\n",
    "with open(\"../07_Text_Splitter/data/appendix-keywords.txt\") as f:\n",
    "    file = f.read()                                         # 파일의 내용을 읽어서 file 변수에 저장\n",
    "\n",
    "# 파일로부터 읽은 내용을 일부 출력하기\n",
    "print(file[:350])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a79adcf",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 셀 출력\n",
    "\n",
    "    ```markdown\n",
    "    Semantic Search\n",
    "\n",
    "    정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
    "    예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
    "    연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
    "\n",
    "    Embedding\n",
    "\n",
    "    정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
    "    예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n",
    "    연관키워드: 자연어 처\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee298642",
   "metadata": {},
   "source": [
    "* **`KonlpyTextSplitter` 사용 → 한국어 텍스트 분할하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b02d11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import KonlpyTextSplitter\n",
    "\n",
    "# KonlpyTextSplitter를 사용하여 텍스트 분할기 객체 생성하기\n",
    "text_splitter = KonlpyTextSplitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3053b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['이 문장을 한국어로 분할해 보세요.\\n\\nKonlpy가 제대로 동작하나요?']\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import KonlpyTextSplitter\n",
    "\n",
    "text_splitter = KonlpyTextSplitter()  # 기본 초기화 (Kkma 호출 테스트)\n",
    "\n",
    "test_text = \"이 문장을 한국어로 분할해 보세요. Konlpy가 제대로 동작하나요?\"\n",
    "chunks = text_splitter.split_text(test_text)\n",
    "print(chunks)  # 형태소/문장 단위 리스트 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d0c25f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Java Runtime 설치\n",
    "# brew install openjdk\n",
    "\n",
    "# KonlpyTextSplitter 임포트\n",
    "from langchain_text_splitters import KonlpyTextSplitter\n",
    "\n",
    "# KonlpyTextSplitter를 사용하여 텍스트 분할기 객체 생성하기\n",
    "text_splitter = KonlpyTextSplitter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f325b203",
   "metadata": {},
   "source": [
    "* **`text_splitter` 사용 → `file`를 문장 단위로 분할**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f47e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한국어 문서를 문장 단위로 분할하기\n",
    "texts = text_splitter.split_text(file)  \n",
    "\n",
    "# 분할된 문장 중 첫 번째 문장을 출력해보기\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3018f5",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 셀 출력\n",
    "\n",
    "    ```markdown\n",
    "    Semantic Search 정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
    "\n",
    "    예시: 사용자가 \" 태양계 행성\" 이라고 검색하면, \" 목성\", \" 화 성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
    "\n",
    "    연관 키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝 Embedding 정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다.\n",
    "\n",
    "    이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
    "\n",
    "    예시: \" 사과\" 라는 단어를 [0.65, -0.23, 0.17] 과 같은 벡터로 표현합니다.\n",
    "\n",
    "    연관 키워드: 자연어 처리, 벡터화, 딥 러닝 Token 정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다.\n",
    "\n",
    "    이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다.\n",
    "\n",
    "    예시: 문장 \" 나는 학교에 간다 \"를 \" 나는\", \" 학교에\", \" 간다\" 로 분할합니다.\n",
    "\n",
    "    연관 키워드: 토큰 화, 자연어 처리, 구 문 분석 Tokenizer 정의: 토크 나이저는 텍스트 데이터를 토큰으로 분할하는 도구입니다.\n",
    "\n",
    "    이는 자연어 처리에서 데이터를 전처리하는 데 사용됩니다.\n",
    "\n",
    "    예시: \"I love programming.\" 이라는 문장을 [ \"I\", \"love\", \"programming\", \".\" ]으로 분할합니다.\n",
    "\n",
    "    연관 키워드: 토큰 화, 자연어 처리, 구 문 분석 VectorStore 정의: 벡터 스토어는 벡터 형식으로 변환된 데이터를 저장하는 시스템 입니\n",
    "\n",
    "    다. 이는 검색, 분류 및 기타 데이터 분석 작업에 사용됩니다.\n",
    "\n",
    "    예시: 단어 임 베 딩 벡터들을 데이터베이스에 저장하여 빠르게 접근할 수 있습니다.\n",
    "\n",
    "    연관 키워드: 임 베 딩, 데이터베이스, 벡터화 SQL 정의: SQL(Structured Query Language) 은 데이터베이스에서 데이터를 관리하기 위한 프로그래밍 언어입니다.\n",
    "\n",
    "    데이터 조회, 수정, 삽입, 삭제 등 다양한 작업을 수행할 수 있습니다.\n",
    "\n",
    "    예시: SELECT * FROM users WHERE age > 18; 은 18세 이상의 사용자 정보를 조회합니다.\n",
    "\n",
    "    연관 키워드: 데이터베이스, 쿼 리, 데이터 관리 CSV 정의: CSV(Comma-Separated Values) 는 데이터를 저장하는 파일 형식으로, 각 데이터 값은 쉼표로 구분됩니다.\n",
    "\n",
    "    표 형태의 데이터를 간단하게 저장하고 교환할 때 사용됩니다.\n",
    "\n",
    "    예시: 이름, 나이, 직업이라는 헤더를 가진 CSV 파일에는 홍길동, 30, 개발자와 같은 데이터가 포함될 수 있습니다.\n",
    "\n",
    "    연관 키워드: 데이터 형식, 파일 처리, 데이터 교환 JSON 정의: JSON(JavaScript Object Notation) 은 경량의 데이터 교환 형식으로, 사람과 기계 모두에게 읽기 쉬운 텍스트를 사용하여 데이터 객체를 표현합니다.\n",
    "\n",
    "    예시: {\" 이름\": \" 홍길동\", \" 나이\": 30, \" 직업\": \" 개발자\"} 는 JSON 형식의 데이터 입니\n",
    "\n",
    "    다. 연관 키워드: 데이터 교환, 웹 개발, API Transformer 정의: 트랜스포머는 자연어 처리에서 사용되는 딥 러닝 모델의 한 유형으로, 주로 번역, 요약, 텍스트 생성 등에 사용됩니다.\n",
    "\n",
    "    이는 Attention 메커니즘을 기반으로 합니다.\n",
    "\n",
    "    예시: 구 글 번역기는 트랜스포머 모델을 사용하여 다양한 언어 간의 번역을 수행합니다.\n",
    "\n",
    "    연관 키워드: 딥 러닝, 자연어 처리, Attention HuggingFace 정의: HuggingFace는 자연어 처리를 위한 다양한 사전 훈련된 모델과 도구를 제공하는 라이브러리입니다.\n",
    "\n",
    "    이는 연구자와 개발자들이 쉽게 NLP 작업을 수행할 수 있도록 돕습니다.\n",
    "\n",
    "    예시: HuggingFace의 Transformers 라이브러리를 사용하여 감정 분석, 텍스트 생성 등의 작업을 수행할 수 있습니다.\n",
    "\n",
    "    연관 키워드: 자연어 처리, 딥 러닝, 라이브러리 Digital Transformation 정의: 디지털 변환은 기술을 활용하여 기업의 서비스, 문화, 운영을 혁신하는 과정입니다.\n",
    "\n",
    "    이는 비즈니스 모델을 개선하고 디지털 기술을 통해 경쟁력을 높이는 데 중점을 둡니다.\n",
    "\n",
    "    예시: 기업이 클라우드 컴퓨팅을 도입하여 데이터 저장과 처리를 혁신하는 것은 디지털 변환의 예입니다.\n",
    "\n",
    "    연관 키워드: 혁신, 기술, 비즈니스 모델 Crawling 정의: 크롤링은 자동화된 방식으로 웹 페이지를 방문하여 데이터를 수집하는 과정입니다.\n",
    "\n",
    "    이는 검색 엔진 최적화나 데이터 분석에 자주 사용됩니다.\n",
    "\n",
    "    예시: 구 글 검색 엔진이 인터넷 상의 웹사이트를 방문 하여 콘텐츠를 수집하고 인 덱싱하는 것이 크롤링입니다.\n",
    "\n",
    "    연관 키워드: 데이터 수집, 웹 스크 래핑, 검색 엔진 Word2Vec 정의: Word2Vec 은 단어를 벡터 공간에 매 핑 하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다.\n",
    "\n",
    "    이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.\n",
    "\n",
    "    예시: Word2Vec 모델에서 \" 왕\" 과 \" 여 왕\" 은 서로 가까운 위치에 벡터로 표현됩니다.\n",
    "\n",
    "    연관 키워드: 자연어 처리, 임 베 딩, 의미론적 유사성 LLM (Large Language Model) 정의: LLM은 대규모의 텍스트 데이터로 훈련된 큰 규모의 언어 모델을 의미합니다.\n",
    "\n",
    "    이러한 모델은 다양한 자연어 이해 및 생성 작업에 사용됩니다.\n",
    "\n",
    "    예시: OpenAI의 GPT 시리즈는 대표적인 대규모 언어 모델입니다.\n",
    "\n",
    "    연관 키워드: 자연어 처리, 딥 러닝, 텍스트 생성 FAISS (Facebook AI Similarity Search) 정의: FAISS는 페이스 북에서 개발한 고속 유사성 검색 라이브러리로, 특히 대규모 벡터 집합에서 유사 벡터를 효과적으로 검색할 수 있도록 설계되었습니다.\n",
    "\n",
    "    예시: 수백만 개의 이미지 벡터 중에서 비슷한 이미지를 빠르게 찾는 데 FAISS가 사용될 수 있습니다.\n",
    "\n",
    "    연관 키워드: 벡터 검색, 머신 러닝, 데이터베이스 최적화 Open Source 정의: 오픈 소스는 소스 코드가 공개되어 누구나 자유롭게 사용, 수정, 배포할 수 있는 소프트웨어를 의미합니다.\n",
    "\n",
    "    이는 협업과 혁신을 촉진하는 데 중요한 역할을 합니다.\n",
    "\n",
    "    예시: 리눅스 운영 체제는 대표적인 오픈 소스 프로젝트입니다.\n",
    "\n",
    "    연관 키워드: 소프트웨어 개발, 커뮤니티, 기술 협업 Structured Data 정의: 구조화된 데이터는 정해진 형식이나 스키마에 따라 조직된 데이터입니다.\n",
    "\n",
    "    이는 데이터베이스, 스프레드 시트 등에서 쉽게 검색하고 분석할 수 있습니다.\n",
    "\n",
    "    예시: 관계 형 데이터베이스에 저장된 고객 정보 테이블은 구조화된 데이터의 예입니다.\n",
    "\n",
    "    연관 키워드: 데이터베이스, 데이터 분석, 데이터 모델링 Parser 정의: 파서는 주어진 데이터( 문자열, 파일 등 )를 분석하여 구조화된 형태로 변환하는 도구입니다.\n",
    "\n",
    "    이는 프로그래밍 언어의 구문 분석이나 파일 데이터 처리에 사용됩니다.\n",
    "\n",
    "    예시: HTML 문서를 구 문 분석하여 웹 페이지의 DOM 구조를 생성하는 것은 파싱의 한 예입니다.\n",
    "\n",
    "    연관 키워드: 구 문 분석, 컴파일러, 데이터 처리 TF-IDF (Term Frequency-Inverse Document Frequency) 정의: TF-IDF는 문서 내에서 단어의 중요도를 평가하는 데 사용되는 통계적 척도입니다.\n",
    "\n",
    "    이는 문서 내 단어의 빈도와 전체 문서 집합에서 그 단어의 희소성을 고려합니다.\n",
    "\n",
    "    예시: 많은 문서에서 자주 등장하지 않는 단어는 높은 TF-IDF 값을 가집니다.\n",
    "\n",
    "    연관 키워드: 자연어 처리, 정보 검색, 데이터 마이닝 Deep Learning 정의: 딥 러닝은 인공 신경망을 이용하여 복잡한 문제를 해결하는 머신 러닝의 한 분야입니다.\n",
    "\n",
    "    이는 데이터에서 고수준의 표현을 학습하는 데 중점을 둡니다.\n",
    "\n",
    "    예시: 이미지 인식, 음성 인식, 자연어 처리 등에서 딥 러닝 모델이 활용됩니다.\n",
    "\n",
    "    연관 키워드: 인공 신경망, 머신 러닝, 데이터 분석 Schema 정의: 스키마는 데이터베이스나 파일의 구조를 정의하는 것으로, 데이터가 어떻게 저장되고 조직되는 지에 대한 청사진을 제공합니다.\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5593afbf",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dcf7b5",
   "metadata": {},
   "source": [
    "### **`Hugging Face tokenizer`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7228fe",
   "metadata": {},
   "source": [
    "* **`Hugging Face` = 다양한 토크나이저 제공**\n",
    "\n",
    "* 다양한 토크나이저 中 하나인 **`GPT2TokenizerFast`** 사용해 텍스트의 토큰 길이 계산하기\n",
    "  * **`텍스트 분할 방식`: 전달된 문자 단위로 분할**\n",
    "  * **`청크 크기 측정 방식`**: **`Hugging Face 토크나이저`에 의해 게산된 토큰 수를 기준으로 함**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b124eecb",
   "metadata": {},
   "source": [
    "* 사전에 `VS Code` 터미널에 설치할 것\n",
    "\n",
    "```bash\n",
    "        pip install transformers\n",
    "\n",
    "        pip install --upgrade jupyter ipywidgets    # 오류 발생 시\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df6587d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa91f2fe",
   "metadata": {},
   "source": [
    "* **`GPT2TokenizerFast` 클래스 사용 → `tokenizer` 객체 생성하기**\n",
    "\n",
    "* **`from_pretrained` 메서드 호출 → 사전 학습된 `gpt2` `토크나이저 모델` 로드**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3905a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformers 라이브러리 임포트\n",
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "# GPT-2 모델의 토크나이저 불러오기\n",
    "hf_tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a5198a",
   "metadata": {},
   "source": [
    "* 샘플 텍스트 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c739b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data/appendix-keywords.txt 파일을 열어서 f라는 파일 객체 생성하기\n",
    "with open(\"../07_Text_Splitter/data/appendix-keywords.txt\") as f:\n",
    "    file = f.read()                                         # 파일의 내용을 읽어서 file 변수에 저장\n",
    "\n",
    "# 파일로부터 읽은 내용을 일부 출력하기\n",
    "print(file[:350])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529f85f6",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 셀 출력\n",
    "\n",
    "    ```markdown\n",
    "    Semantic Search\n",
    "\n",
    "    정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
    "    예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
    "    연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
    "\n",
    "    Embedding\n",
    "\n",
    "    정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
    "    예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n",
    "    연관키워드: 자연어 처\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ad7e42",
   "metadata": {},
   "source": [
    "* `from_huggingface_tokenizer` 메서드 → **`허깅페이스 토크나이저`(`tokenizer`) 사용** → **텍스트 분할기 초기화**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1b002c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 허깅페이스 토크나이저 사용 → CharacterTextSplitter 객체 생성하기\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    hf_tokenizer,\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50,\n",
    ")\n",
    "\n",
    "\n",
    "# state_of_the_union 텍스트를 분할하여 texts 변수에 저장하기\n",
    "texts = text_splitter.split_text(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b7799a",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 셀 출력\n",
    "\n",
    "    ```markdown\n",
    "    Created a chunk of size 358, which is longer than the specified 300\n",
    "    Created a chunk of size 315, which is longer than the specified 300\n",
    "    Created a chunk of size 305, which is longer than the specified 300\n",
    "    Created a chunk of size 366, which is longer than the specified 300\n",
    "    Created a chunk of size 330, which is longer than the specified 300\n",
    "    Created a chunk of size 351, which is longer than the specified 300\n",
    "    Created a chunk of size 378, which is longer than the specified 300\n",
    "    Created a chunk of size 361, which is longer than the specified 300\n",
    "    Created a chunk of size 350, which is longer than the specified 300\n",
    "    Created a chunk of size 362, which is longer than the specified 300\n",
    "    Created a chunk of size 335, which is longer than the specified 300\n",
    "    Created a chunk of size 353, which is longer than the specified 300\n",
    "    Created a chunk of size 358, which is longer than the specified 300\n",
    "    Created a chunk of size 336, which is longer than the specified 300\n",
    "    Created a chunk of size 324, which is longer than the specified 300\n",
    "    Created a chunk of size 337, which is longer than the specified 300\n",
    "    Created a chunk of size 307, which is longer than the specified 300\n",
    "    Created a chunk of size 361, which is longer than the specified 300\n",
    "    Created a chunk of size 354, which is longer than the specified 300\n",
    "    Created a chunk of size 378, which is longer than the specified 300\n",
    "    Created a chunk of size 381, which is longer than the specified 300\n",
    "    Created a chunk of size 365, which is longer than the specified 300\n",
    "    Created a chunk of size 377, which is longer than the specified 300\n",
    "    Created a chunk of size 329, which is longer than the specified 300\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe445a8",
   "metadata": {},
   "source": [
    "* 첫 번째 요소의 분할 결과 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcc0bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# texts 리스트의 1 번째 요소 출력해보기\n",
    "\n",
    "print(texts[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b7198f",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 셀 출력\n",
    "\n",
    "    ```markdwon\n",
    "    정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
    "    예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
    "    연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f54adbf",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bfe079",
   "metadata": {},
   "source": [
    "* *next: **`시멘틱 청커 (SemanticChunker)`***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353a5347",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
