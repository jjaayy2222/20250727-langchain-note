{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19356e6f",
   "metadata": {},
   "source": [
    "#### (1) Runnable 이란?\n",
    "\n",
    "* LangChain에서 **다양한 컴포넌트(예: LLM, 프롬프트, 파서 등)를 연결하고 실행하기 위한 기본 인터페이스**\n",
    "\n",
    "* 여러 컴포넌트를 연결 -> 복잡한 데이터 처리 **파이프라인 구축** 가능\n",
    "    * 예시: prompt -> LLM에 전달 -> LLM의 출력을 파서로 전달\n",
    "    * `invoke`, `batch`, `stream` 등의 메서드를 통해 실행\n",
    "    * 각 Runnabel은 입력 데이터를 받아 처리하고, 그 결과를 다음 Runnable에 전달하는 역할\n",
    "\n",
    "* **LCEL과의 관계** : LCEL은 Runnable을 기반으로 Chain을 보다 쉽게 구성할 수 있도록 하는 선언적인 언어\n",
    "    * \n",
    "    * `RunnablePassthrough`: 입력을 변경하지 않거나 추가 키를 더하여 전달\n",
    "        * `RunnablePassthrough()`: 단독으로 호출 -> 단순히 입력을 받아 그대로 전달\n",
    "        * `RunnablePassthrough()`: 입력을 받아 assign 함수에 전달된 추가 인수를 추가\n",
    "    * `RunnableParallel`: 여러 Runnable을 병렬로 실행\n",
    "    * `RunnableSequence`: `Runnable`의 시퀀스를 정의함\n",
    "    * `RunnableLambda`: 사용자 정의 함수를 `Runnable`로 래핑함\n",
    "    * \n",
    "    * 위와 같이 다양한 Runnable 클래스 조합으로 체인 구축 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb82a87",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2411517",
   "metadata": {},
   "source": [
    "* 기본 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c7a22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 모듈 임포트\n",
    "import os\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API KEY를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API KEY 정보 로드\n",
    "load_dotenv()                   # true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc449415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 환경 변수 확인하기\n",
    "\n",
    "# 마스킹 처리 함수 정의\n",
    "def mask_key(key: str, visible_count: int = 2) -> str:\n",
    "    if not key or len(key) <= visible_count:\n",
    "        return '*' * len(key)\n",
    "    return key[:visible_count] + '*' * (len(key) - visible_count)\n",
    "\n",
    "# 환경변수 불러오기\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"GOOGLE_API_KEY 환경 변수가 설정되지 않았습니다.\")\n",
    "\n",
    "# 마스킹된 형태로 출력\n",
    "print(f\"GOOGLE_API_KEY: {mask_key(api_key)}\")           # GOOGLE_API_KEY: AI*************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8679f034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangSmith 추적 설정 (https://smith.langchain.com)\n",
    "\n",
    "\"\"\"\n",
    "- !pip install -qU langsmith\n",
    "- !pip install -qU langchain-teddynote\n",
    "    -> 제미나이와 poetry와의 의존성 충돌로 langchain_teddy 설치 X \n",
    "    -> langsmith로 진행\n",
    "\"\"\"\n",
    "# LangSmith 추적을 위한 라이브러리 임포트\n",
    "from langsmith import traceable         # @traceable 데코레이터 사용 시\n",
    "\n",
    "# LangSmith 환경 변수 확인\n",
    "\n",
    "print(\"\\n--- LangSmith 환경 변수 확인 ---\")\n",
    "langchain_tracing_v2 = os.getenv('LANGCHAIN_TRACING_V2')\n",
    "langchain_project = os.getenv('LANGCHAIN_PROJECT')\n",
    "langchain_api_key_status = \"설정됨\" if os.getenv('LANGCHAIN_API_KEY') else \"설정되지 않음\" # API 키 값은 직접 출력하지 않음\n",
    "\n",
    "if langchain_tracing_v2 == \"true\" and os.getenv('LANGCHAIN_API_KEY') and langchain_project:\n",
    "    print(f\"✅ LangSmith 추적 활성화됨 (LANGCHAIN_TRACING_V2='{langchain_tracing_v2}')\")\n",
    "    print(f\"✅ LangSmith 프로젝트: '{langchain_project}'\")\n",
    "    print(f\"✅ LangSmith API Key: {langchain_api_key_status}\")\n",
    "    print(\"  -> 이제 LangSmith 대시보드에서 이 프로젝트를 확인해 보세요.\")\n",
    "else:\n",
    "    print(\"❌ LangSmith 추적이 완전히 활성화되지 않았습니다. 다음을 확인하세요:\")\n",
    "    if langchain_tracing_v2 != \"true\":\n",
    "        print(f\"  - LANGCHAIN_TRACING_V2가 'true'로 설정되어 있지 않습니다 (현재: '{langchain_tracing_v2}').\")\n",
    "    if not os.getenv('LANGCHAIN_API_KEY'):\n",
    "        print(\"  - LANGCHAIN_API_KEY가 설정되어 있지 않습니다.\")\n",
    "    if not langchain_project:\n",
    "        print(\"  - LANGCHAIN_PROJECT가 설정되어 있지 않습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98c6653",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 셀 출력\n",
    "    * --- LangSmith 환경 변수 확인 ---\n",
    "    * ✅ LangSmith 추적 활성화됨 (LANGCHAIN_TRACING_V2='true')\n",
    "    * ✅ LangSmith 프로젝트: 'L***************'\n",
    "    * ✅ LangSmith API Key: 설정됨\n",
    "    *   -> 이제 LangSmith 대시보드에서 이 프로젝트를 확인해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d6d944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain 및 Google GenAI 모델 관련 모듈 임포트\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI           # Google GenAI 임포트\n",
    "\n",
    "\n",
    "print(\"\\n--- LangChain 체인 설정 ---\")\n",
    "\n",
    "template = \"{topic}에 대해 3문장으로 설명해줘.\"                           # 템플릿 정의\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)                     # 프롬프트를 프롬프트템플릿 객체로 생성\n",
    "\n",
    "try:\n",
    "    model = ChatGoogleGenerativeAI(                                 # 모델 호출\n",
    "        model=\"gemini-1.5-flash\",\n",
    "        temperature=0.1,\n",
    "    )\n",
    "    print(\"✅ Google GenAI 모델 초기화 성공.\")\n",
    "except Exception as e:                                              # 디버깅 메시지\n",
    "    print(f\"❌ Google GenAI 모델 초기화 실패: {e}\")\n",
    "    print(\"  -> GOOGLE_API_KEY 환경 변수가 올바르게 설정되었는지 확인하세요.\")\n",
    "    \n",
    "# 출력 파서\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# 체인 구성\n",
    "chain = prompt | model | output_parser                              # 프롬프트, 모델, 출력 파서 연결 -> 체인 구성\n",
    "print(\"✅ LangChain LCEL 체인 구성 완료.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4771870e",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 셀 출력\n",
    "    * --- LangChain 체인 설정 ---\n",
    "    * ✅ Google GenAI 모델 초기화 성공.\n",
    "    * ✅ LangChain LCEL 체인 구성 완료."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2b60a2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fce3b2d",
   "metadata": {},
   "source": [
    "#### (2) **`RunnablePassthrough`**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4d48ac",
   "metadata": {},
   "source": [
    "* **`RunnablePassthrough`**\n",
    "  \n",
    "  * **입력 데이터를 변형하지 않고 그대로 통과**시키거나, **필요에 따라 일부 키/값을 추가**해 입력에 함께 전달할 수 있는 Runnable 객체\n",
    "\n",
    "  * Q. 언제 쓸까?\n",
    "    * A1. 여러 체인에서, **입력 데이터의 특정 부분을 그대로 넘기고 싶을 때**\n",
    "    * A2. **기존 입력데이터 + 추가 데이터 조합이 필요**할 때\n",
    "      * 예시: `assign` 활용\n",
    "    \n",
    "  * 특징\n",
    "      * `.invoke()` -> **입력을 그대로 반환**\n",
    "      * `.assign()` -> 입력 dict에 **원하는 계산 결과를 병합**해줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9227ff00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 교재와 같이 시도\n",
    "\n",
    "# prompt, model 생성\n",
    "prompt = PromptTemplate.from_template(\"{num}의 10는?\")\n",
    "\n",
    "try:\n",
    "    model = ChatGoogleGenerativeAI(                                 # 기본 모델 호출\n",
    "        model=\"gemini-1.5-flash\",\n",
    "        temperature=0.1,\n",
    "    )\n",
    "    print(\"✅ Google GenAI 모델 초기화 성공.\")\n",
    "    print(\"---\")\n",
    "    \n",
    "except Exception as e:                                              # 디버깅 메시지\n",
    "    print(f\"❌ Google GenAI 모델 초기화 실패: {e}\")\n",
    "    print(\"  -> GOOGLE_API_KEY 환경 변수가 올바르게 설정되었는지 확인하세요.\")\n",
    "    print(\"---\")\n",
    "\n",
    "# Chain 생성\n",
    "chain = prompt | model\n",
    "\n",
    "# Chain 실행\n",
    "chain.invoke({\"num\":5})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de87312e",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* \n",
    "    * 셀 출력\n",
    "        * ✅ Google GenAI 모델 초기화 성공.\n",
    "        * ---\n",
    "        * `AIMessage(content='5의 10승은 5를 10번 곱한 값입니다.  즉, 5 x 5 x 5 x 5 x 5 x 5 x 5 x 5 x 5 x 5 = **9,765,625** 입니다.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, id='run--a7f7c69f-7477-42fd-a200-93d5f949b716-0', usage_metadata={'input_tokens': 7, 'output_tokens': 67, 'total_tokens': 74, 'input_token_details': {'cache_read': 0}})`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af5979c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1개의 변수만 템플릿에 포함하고 있다면 값만 전달하는 것도 가능\n",
    "chain.invoke(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d7b034",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* \n",
    "    * 셀 출력\n",
    "        * `AIMessage(content='5의 10승은 5 x 5 x 5 x 5 x 5 x 5 x 5 x 5 x 5 x 5 = **9,765,625** 입니다.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, id='run--2b1a9028-cdd9-4748-be61-0f20d3dab11f-0', usage_metadata={'input_tokens': 7, 'output_tokens': 51, 'total_tokens': 58, 'input_token_details': {'cache_read': 0}})`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb99dbde",
   "metadata": {},
   "source": [
    "* **`RunnablePassthrough()`**: 입력 값을 **그대로 받아 전달**\n",
    "    * `invoke()` 메소드와 사용\n",
    "    * chain 구성 연습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6596414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RunnablePassthrough 관련 모듈 임포트\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# RunnablePassthrough()_1\n",
    "RunnablePassthrough().invoke({\"num: 10\"})                           # {'num: 10'}\n",
    "\n",
    "# RunnablePassthrough()_2\n",
    "result = RunnablePassthrough().invoke({\"num\": 10})\n",
    "print(result)                                                       # {'num': 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7f5fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RunnablePassthrough -> Chain 구성하기\n",
    "\n",
    "runnablepassthrough_chain = {\"num\" : RunnablePassthrough()} | prompt | model\n",
    "\n",
    "# dict 값이 RunnablePassthrough()로 변경됨\n",
    "runnablepassthrough_chain.invoke(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61451e3f",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* \n",
    "    * 셀 출력\n",
    "        * `AIMessage(content='10의 10승은 10,000,000,000 (100억) 입니다.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, id='run--2c9d7372-8b1c-4da2-928d-da5bfe77de31-0', usage_metadata={'input_tokens': 8, 'output_tokens': 32, 'total_tokens': 40, 'input_token_details': {'cache_read': 0}})`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f73062a",
   "metadata": {},
   "source": [
    "* **`RunnablePassthrough()`**: 입력 값에 **추가 키 더하여 전달 가능**\n",
    "    * `assign()` 함수에 전달된 추가 인수 전달\n",
    "    * chain 구성 연습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3a51e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RunnablePassthrough.assign()_1\n",
    "RunnablePassthrough.assign().invoke({\"num\": 1})         # {'num': 1}\n",
    "\n",
    "# RunnablePassthrough.assign()_2\n",
    "result_assign = RunnablePassthrough.assign().invoke({\"num\": 1})\n",
    "print(result_assign)                                      # {`num``: 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c58314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RunnablePassthrough.assign()_3 - 새롭게 할당된 key/value 쌍 병합\n",
    "# 입력 키: num, 할당(assign) 키: new_num\n",
    "\n",
    "(RunnablePassthrough.assign(new_num=lambda x: x[\"num\"] * 3)).invoke({\"num\": 1})             # {'num': 1, 'new_num': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095b5da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RunnablePassthrough.assign()_4 - assign에 함수 추가해보기\n",
    "\n",
    "result_assign2 = RunnablePassthrough().assign(new_num=lambda x: x[\"num\"] * 3)\n",
    "print(result_assign2.invoke({\"num\": 5}))                                                    # {'num': 5, 'new_num': 15}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac1e186",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144bd4b8",
   "metadata": {},
   "source": [
    "#### (3) **`RunnableParallel`**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c7a3a4",
   "metadata": {},
   "source": [
    "* **`RunnableParallel`**\n",
    "  \n",
    "  * 입력 데이터를 **여러 개의 Runnable 체인에 병렬로 전달**하고, 각 실행 **결과를 dict형태로 반환**\n",
    "\n",
    "  * Q. 언제 쓸까?\n",
    "    * A. 여러 모델 or 다른 연산을 동시에 돌려 **다중 결과가 필요**한 경우\n",
    "      * 예시_1: 한 입력으로 여러 LLM 질의\n",
    "      * 예시_2: 전처리 + 후처리 결과 동시 반환 등\n",
    "    \n",
    "  * 특징\n",
    "      * **dict형태** -> **Key별로 runnable 매핑 가능**\n",
    "      * **결과 또한 Key별로 추출**\n",
    "      * 병렬 실행은 내부적으로 `await`/`gather`로 처리, **I/O 차원에선 동시에 실행**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5036e963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RunnableParallel 모듈 임포트\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "# RunnableParallel 인스턴스 생성 -> 여러 인스턴스 병렬 실행 가능\n",
    "runnable_parallel = RunnableParallel(\n",
    "    # RunnablePassthrough() = 'passed' 키워드 인자로 전달 -> 입력된 데이터를 그대로 통과\n",
    "    passed=RunnablePassthrough(),\n",
    "    \n",
    "    # 'extra' 키워드 인자로 RunnablePassthrough.assign('mult' 람다 함수) -> 딕셔너리의 'num' 키에 해당하는 값을 3배로 증가\n",
    "    extra=RunnablePassthrough.assign(mult=lambda x: x[\"num\"] * 3),\n",
    "    \n",
    "    # 람다 함수 = 'modified' 키워드 인자 -> 입력된 딕셔너리의 'num' 키에 해당하는 값에 1 더하기\n",
    "    modified=lambda x: x[\"num\"] + 1,\n",
    ")\n",
    "\n",
    "# {'num': 1} 딕셔너리 입력 -> invoke 메소드로 호출해보기\n",
    "runnable_parallel.invoke({\"num\": 1})                                \n",
    "\n",
    "# {'passed': {'num': 1}, 'extra': {'num': 1, 'mult': 3}, 'modified': 2} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5b5fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time                                                                                 #시간 측정을 위해 임포트\n",
    "\n",
    "# 모델 초기화 (gemini-2.5-flash-lite 사용)\n",
    "try:\n",
    "    model2 = ChatGoogleGenerativeAI(                                 \n",
    "        model=\"gemini-2.5-flash-lite\",\n",
    "        temperature=0.1,\n",
    "    )\n",
    "    print(\"✅ Google GenAI 모델 초기화 성공.\")\n",
    "    print(\"---\")\n",
    "except Exception as e:                                              \n",
    "    print(f\"❌ Google GenAI 모델 초기화 실패: {e}\")\n",
    "    print(\"  -> GOOGLE_API_KEY 환경 변수가 올바르게 설정되었는지 확인하세요.\")\n",
    "    print(\"---\")\n",
    "    exit() # 모델 초기화 실패 시 프로그램 종료\n",
    "\n",
    "# 프롬프트 구성\n",
    "prompt_capital = PromptTemplate.from_template(\"{country}의 수도는?\")\n",
    "prompt_area = PromptTemplate.from_template(\"{country}의 면적은?\")\n",
    "\n",
    "###################################################\n",
    "# 동기적 실행과 RunnableParaller 실행을 모두 진행하여 비교 #\n",
    "###################################################\n",
    "\n",
    "# 동기적 실행을 위한 개별 체인 (RunnableParallel 없이 순차적으로 호출)\n",
    "chain_capital_sync = {\"country\": RunnablePassthrough()} | prompt_capital | model2\n",
    "chain_area_sync = {\"country\": RunnablePassthrough()} | prompt_area | model2\n",
    "\n",
    "# RunnableParallel을 사용한 병렬 체인\n",
    "# model2를 사용하도록 변경\n",
    "combined_chain_parallel = RunnableParallel(capital=chain_capital_sync, area=chain_area_sync)\n",
    "\n",
    "# --- 동기적(순차적) 실행 시간 측정 ---\n",
    "print(\"--- 동기적(순차적) 실행 시작 ---\")\n",
    "start_time_sync = time.time()\n",
    "\n",
    "# 두 체인을 순차적으로 호출\n",
    "capital_result_sync = chain_capital_sync.invoke(\"대한민국\")\n",
    "area_result_sync = chain_area_sync.invoke(\"대한민국\")\n",
    "\n",
    "end_time_sync = time.time()\n",
    "\n",
    "print(f\"수도 결과: {capital_result_sync}\")                                     \n",
    "print(f\"면적 결과: {area_result_sync}\")                                         \n",
    "print(f\"동기적 실행 시간: {end_time_sync - start_time_sync:.4f} 초\")\n",
    "print(\"---\")\n",
    "\n",
    "# --- RunnableParallel (병렬) 실행 시간 측정 ---\n",
    "print(\"\\n--- RunnableParallel (병렬) 실행 시작 ---\")\n",
    "start_time_parallel = time.time()\n",
    "\n",
    "# RunnableParallel 체인 호출\n",
    "result_parallel = combined_chain_parallel.invoke(\"대한민국\")\n",
    "\n",
    "end_time_parallel = time.time()\n",
    "\n",
    "# 결과 출력 (딕셔너리 형태이므로 직접 접근)\n",
    "print(f\"병렬 실행 결과 - 수도: {result_parallel['capital']}\")                    \n",
    "print(f\"병렬 실행 결과 - 면적: {result_parallel['area']}\")                      \n",
    "print(f\"병렬 실행 시간: {end_time_parallel - start_time_parallel:.4f} 초\")\n",
    "print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e8771b",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* \n",
    "    * 셀 출력\n",
    "        * ✅ Google GenAI 모델 초기화 성공.\n",
    "        * ---\n",
    "        * --- 동기적(순차적) 실행 시작 ---\n",
    "        * 수도 결과: content='대한민국의 수도는 **서울**입니다.' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': []} id='run--aee20300-a869-415d-a425-69e21ccd30ed-0' usage_metadata={'input_tokens': 7, 'output_tokens': 10, 'total_tokens': 17, 'input_token_details': {'cache_read': 0}}\n",
    "        * 면적 결과: content='대한민국의 면적은 약 **100,410 제곱킬로미터**입니다.\\n\\n이는 세계에서 109번째로 넓은 면적이며, 한반도 전체 면적의 약 45%에 해당합니다.' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': []} id='run--e9164940-80c1-4e10-bd51-cd6203de12b1-0' usage_metadata={'input_tokens': 8, 'output_tokens': 55, 'total_tokens': 63, 'input_token_details': {'cache_read': 0}}\n",
    "        * 동기적 실행 시간: 1.9750 초\n",
    "        * ---\n",
    "\n",
    "        * --- RunnableParallel (병렬) 실행 시작 ---\n",
    "        * 병렬 실행 결과 - 수도: content='대한민국의 수도는 **서울**입니다.' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': []} id='run--485dc267-1502-4323-a326-501811f157b5-0' usage_metadata={'input_tokens': 7, 'output_tokens': 10, 'total_tokens': 17, 'input_token_details': {'cache_read': 0}}\n",
    "        * 병렬 실행 결과 - 면적: content='대한민국의 면적은 약 **100,410 제곱킬로미터**입니다.\\n\\n이는 세계적으로 볼 때 중간 정도의 크기이며, 한반도 전체 면적의 약 45%에 해당합니다.' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': []} id='run--ba33162d-bd75-4ca4-9c9c-e38534d4afc5-0' usage_metadata={'input_tokens': 8, 'output_tokens': 52, 'total_tokens': 60, 'input_token_details': {'cache_read': 0}}\n",
    "        * 병렬 실행 시간: 1.2327 초\n",
    "        * ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59702bf7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0d4d31",
   "metadata": {},
   "source": [
    "#### (4) **`RunnableLambda`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7364dd",
   "metadata": {},
   "source": [
    "* **`RunnableLambda`**\n",
    "  \n",
    "  * 파이썬의 **any 함수(람다 등)를 체인 내에서 실행**해 **결과를 반환**하는 Runnable 객체\n",
    "\n",
    "  * Q. 언제 쓸까?\n",
    "    * A1. 체인 중간에 직접적 데이터 가공/전처리가 필요할 때\n",
    "    * A2. 입력값 동적 계산/포맷팅/파싱 로직이 필요할 때\n",
    "    \n",
    "  * 특징\n",
    "      * 나만의 커스텀 연산을 체인에 쉽게 삽입 가능\n",
    "      * `invoke()` 시 입력값 받아 함수 적용, 반환값 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e94b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RunnalbeLambda_1\n",
    "\n",
    "# 필요한 임포트\n",
    "from datetime import datetime\n",
    "\n",
    "def get_today(a):\n",
    "    # 오늘 날짜 가져오기\n",
    "    return datetime.today().strftime(\"%b-%d\")\n",
    "\n",
    "# 출력하기\n",
    "get_today(None)                                      # 'Jul-30'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00756346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RunnalbeLambda_2\n",
    "\n",
    "# 필요한 모듈 임포트\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# 입력값의 제곱을 반환하는 lambda 적용\n",
    "runnable_lambda = RunnableLambda(lambda x: x[\"num\"] ** 2)\n",
    "print(runnable_lambda.invoke({\"num\": 7}))                  # 49"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3097ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RunnableLambda_3 - Chain 생성\n",
    "\n",
    "def get_today(a):\n",
    "    return datetime.today().strftime(\"%b-%d\")\n",
    "\n",
    "\n",
    "# 필요한 모듈 임포트\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "# 프롬프트 생성\n",
    "prompt_lambda = PromptTemplate.from_template(\n",
    "    \"{today}가 생일인 유명인 (n) 명을 나열하세요. 생년월일을 표기하세요.\"\n",
    ")\n",
    "\n",
    "# model = gemini-2.5-flash-lite 사용\n",
    "try:\n",
    "    model2 = ChatGoogleGenerativeAI(                                 # 모델 호출\n",
    "        model=\"gemini-2.5-flash-lite\",\n",
    "        temperature=0.1,\n",
    "    )\n",
    "    print(\"✅ Google GenAI 모델 초기화 성공.\")\n",
    "    print(\"---\")\n",
    "except Exception as e:                                              # 디버깅 메시지\n",
    "    print(f\"❌ Google GenAI 모델 초기화 실패: {e}\")\n",
    "    print(\"  -> GOOGLE_API_KEY 환경 변수가 올바르게 설정되었는지 확인하세요.\")\n",
    "    print(\"---\")\n",
    "\n",
    "\n",
    "# chain 생성\n",
    "chain_lambda =(\n",
    "    {\"today\":RunnableLambda(get_today), \"n\":RunnablePassthrough()}\n",
    "    | prompt_lambda\n",
    "    | model2\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 체인 객체의 구성 출력 (현재 교재에서 보여주는 단계)\n",
    "print(\"--- Chain 구성 정보 ---\")\n",
    "print(chain_lambda)\n",
    "print(\"---\")\n",
    "\n",
    "# 체인 실행 및 실제 결과 출력\n",
    "print(\"\\n--- 체인 실행 결과 ---\")\n",
    "try:\n",
    "    # \"n\"에 해당하는 값을 invoke() 메서드에 딕셔너리 형태로 전달합니다.\n",
    "    # 예: 3명의 유명인\n",
    "    result = chain_lambda.invoke({\"n\": 3})\n",
    "    print(result)\n",
    "except Exception as e:\n",
    "    print(f\"❌ 체인 실행 중 오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67023db8",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* \n",
    "    * 셀 출력 \n",
    "        * ✅ Google GenAI 모델 초기화 성공.\n",
    "\n",
    "        ---\n",
    "\n",
    "    * --- Chain 구성 정보 ---\n",
    "        * chain_lambda 객체 자체의 **표현** = 어떻게 구성되었는지를 표혀줌\n",
    "        * `RunnabelLambda`, `PromptTemplate`, `ChatGoogleGenerativeAI`, `StrOutParser` 같은 컴포넌트들을 성공적으로 연결해서 하나의 체인을 만들었음을 보여줌 \n",
    "          * `first={today: RunnableLambda(get_today), n: RunnablePassthrough()}` = 체인의 첫 번째 단계에서 입력(n)과 get_today 함수(today)를 어떻게 처리하는지 보여줌\n",
    "          * `middle=[PromptTemplate(input_variables=['today'], input_types={}, partial_variables={}, template='{today}가 생일인 유명인 (n) 명을 나열하세요. 생년월일을 표기하세요.'), ChatGoogleGenerativeAI(model='models/gemini-2.5-flash-lite', google_api_key=SecretStr('**********'), temperature=0.1, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x11f2079b0>, default_metadata=(), model_kwargs={})]` = 프롬프트 템플릿과 실제 언어 모델(ChatGoogleGenerativeAI)이 어떻게 연결되어 있는지 나타냄\n",
    "          * `last=StrOutputParser()` = 마지막으로 StrOutputParser()를 사용하여 모델의 응답을 문자열로 파싱하는 단계를 보여줌\n",
    "        \n",
    "        ---\n",
    "\n",
    "    * --- 체인 실행 결과 ---\n",
    "        * 7월 30일에 생일을 맞은 유명인 5명을 생년월일과 함께 알려드리겠습니다.\n",
    "\n",
    "        * 1.  **헨리 포드 (Henry Ford)**\n",
    "        *     *   생년월일: 1863년 7월 30일\n",
    "\n",
    "        * 2.  **에밀리 브론테 (Emily Brontë)**\n",
    "        *     *   생년월일: 1818년 7월 30일\n",
    "\n",
    "        * 3.  **폴 앤더슨 (Paul Anderson)**\n",
    "        *     *   생년월일: 1932년 7월 30일\n",
    "\n",
    "        * 4.  **리사 쿤트로 (Lisa Kudrow)**\n",
    "        *     *   생년월일: 1963년 7월 30일\n",
    "\n",
    "        * 5.  **크리스토퍼 멜로니 (Christopher Meloni)**\n",
    "        *     *   생년월일: 1961년 7월 30일"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47b0ecf",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01e5ff5",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* \n",
    "    * **문제점** 발견 : 3명 출력 요구 -> 5명이 출력\n",
    "      * 주요 원인1. model의 **자유성**\n",
    "          * 현재 설정: temperature = 0.1 -> randomness or creativity 낮음 \n",
    "      * 주요 원인2. **프롬프트의 모호성**\n",
    "          * **정확히**라는 강조가 부족했을 수 있음\n",
    "          * `유명인 (n) 명을 나열하세요` -> `(n)` = 모델에게 `n`이 **단순한 플레이스홀더**인지, 아니면 **정확히 지켜야 할 숫자 제한**인지 **혼동**을 줄 수 있음\n",
    "            * 즉, 모델이 **(n)의 의미를 숫자 제한으로 정확히 이해하지 못했을 가능성**\n",
    "          * 프롬프트가 길어지거나 복잡해질수록, 모델은 특정 숫자 제한보다는 **전체적인 내용과 의미를 파악**하는 데 집중할 필요가 있음\n",
    "      * 주요 원인3. 데이터의 다양성\n",
    "          * 모델이 학습한 데이터에는 **몇 명**이라는 숫자가 명확히 지켜지지 않은 목록이나 설명이 많을 수 있음\n",
    "          * 이러한 학습 데이터의 편향이 모델의 응답에 영향을 미칠 수도 있음\n",
    "      * 주요 원인4. model의 최적화 목표\n",
    "          * temperature가 낮을 때 모델은 자신이 학습한 데이터에서 \"가장 자연스럽고 그럴듯한\" 답변을 내놓으려고 함 \n",
    "              * 만약 학습 데이터에 **유명인 나열**이라는 프롬프트에 대해 3명보다 **5명을 나열하는 경우가 더 흔하거나** 모델이 **5명을 나열하는 패턴을 더 강력하게 학습**했다면, temperature가 낮을수록 오히려 그 강력한 패턴을 따르려 할 가능성이 있음\n",
    "              * 3명으로 제한하는 것보다 5명으로 제한하지 않고 나열하는 것이 모델 입장에서는 더 \"자연스러운\" 답변이라고 판단했을 것\n",
    "          * 내부적 선호도\n",
    "              * 모델 내부적으로 특정 유형의 정보(**이름 목록**)를 생성할 때 **일정 개수 이상을 선호하는 경향**이 있을 수 있음\n",
    "              * 낮은 온도는 이런 내부적인 선호도를 더 강하게 표출하게 만들 수도 있음\n",
    "\n",
    "    * **모델의 temperature가 낮아도 프롬프트가 모호하면 다른 내부적 우선순위를 따르게 됨**\n",
    "    ---\n",
    "\n",
    "    * 해결 방안\n",
    "        * temperature 더 낮추기? -> **X**\n",
    "            * temperature가 낮다고 해서 지시를 무조건 따르는 것이 아님\n",
    "            * model의 내부적인 학습 패턴과 프롬프트의 명확성과 함께 고려해야 함\n",
    "        * **프롬프트 개선 필요**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c2c37e",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a8603f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # RunnableLambda_4 - Chain 생성 + 프롬프트 개선\n",
    "\n",
    "from datetime import datetime\n",
    "import re\n",
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough, RunnableParallel\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "\n",
    "# 날짜 함수\n",
    "def get_today(_): return datetime.today().strftime(\"%b-%d\")\n",
    "\n",
    "# 프롬프트 템플릿\n",
    "prompt_lambda2 = PromptTemplate.from_template(\"\"\"\n",
    "당신은 데이터를 엄격히 다루는 어시스턴트입니다.\n",
    "\n",
    "규칙:\n",
    "- '{today}가 생일인 유명인 {n_val}명:'으로 시작\n",
    "- 한 문장에 한 명: 1. 이름 (생년월일)\n",
    "- 예시만 따르기, 설명・공백・인삿말 없이\n",
    "\n",
    "예시:\n",
    "{today}가 생일인 유명인 3명:\n",
    "1. 알베르트 아인슈타인 (1879년 3월 14일)\n",
    "2. 김연아 (1990년 9월 5일)\n",
    "3. BTS의 정국 (1997년 9월 1일)\n",
    "\n",
    "---\n",
    "\n",
    "{today}가 생일인 유명인 {n_val}명을 위와 같은 형식으로 출력하세요.\n",
    "\"\"\".strip())\n",
    "\n",
    "# 모델 초기화\n",
    "model2 = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash-lite\",\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "# 후처리 함수\n",
    "def post_process(data):\n",
    "    text = data[\"text\"]\n",
    "    n = data[\"n_val\"]\n",
    "    if isinstance(n, dict):\n",
    "        n = n.get(\"n_val\", 1)\n",
    "    try:\n",
    "        limit = int(n)\n",
    "    except:\n",
    "        print(f\"[ERROR] 잘못된 n_val 값: {n} ({type(n)})\")\n",
    "        return \"\"\n",
    "\n",
    "    lines = [\n",
    "        l.strip()\n",
    "        for l in text.strip().split(\"\\n\")\n",
    "        if re.match(r\"^\\d+\\.\\s.+\\(\\d{4}\", l.strip())\n",
    "    ]\n",
    "    return \"\\n\".join(lines[:limit])\n",
    "\n",
    "# 체인 구성\n",
    "chain_lambda2 = (\n",
    "    {\n",
    "        \"today\": RunnableLambda(get_today),\n",
    "        \"n_val\": RunnablePassthrough()\n",
    "    }\n",
    "    | RunnableParallel(\n",
    "        text=(prompt_lambda2 | model2 | StrOutputParser()),\n",
    "        n_val=itemgetter(\"n_val\")  # 🔥 핵심 수정: 중첩 dict 방지\n",
    "    )\n",
    "    | RunnableLambda(post_process)\n",
    ")\n",
    "\n",
    "# 실행\n",
    "num = 3\n",
    "print(\"✅ 실행 중...\")\n",
    "result = chain_lambda2.invoke({\"n_val\": num})\n",
    "print(\"\\n🎯 최종 결과:\\n\" + result)\n",
    "\n",
    "\n",
    "# 체인 객체의 구성 출력 (현재 교재에서 보여주는 단계)\n",
    "print(\"\\n--- Chain 구성 정보 ---\")\n",
    "print(chain_lambda2)\n",
    "print(\"---\")\n",
    "\n",
    "# 체인 실행 및 실제 결과 출력\n",
    "print(\"\\n--- 체인 실행 결과 ---\")\n",
    "try:\n",
    "    # \"n\"에 해당하는 값을 invoke() 메서드에 딕셔너리 형태로 전달합니다.\n",
    "    # 예: 3명의 유명인\n",
    "    result = chain_lambda2.invoke({\"n_val\": 3})\n",
    "    print(result)\n",
    "except Exception as e:\n",
    "    print(f\"❌ 체인 실행 중 오류 발생: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54707ae",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* \n",
    "    * 셀 출력\n",
    "        * ✅ 실행 중...\n",
    "\n",
    "        * 🎯 최종 결과:\n",
    "        * 1. 헨리 포드 (1863년 7월 30일)\n",
    "        * 2. 닐 암스트롱 (1930년 8월 5일)\n",
    "        * 3. 톰 행크스 (1956년 7월 9일)\n",
    "\n",
    "        ---\n",
    "\n",
    "        * --- Chain 구성 정보 ---\n",
    "        * first=`{today: RunnableLambda(get_today), n_val: RunnablePassthrough()}`\n",
    "            * **chin_lambda2의 구성 자체**를 보여줌\n",
    "        * middle=`[{text: PromptTemplate(input_variables=['n_val', 'today'], input_types={}, partial_variables={}, template=\"당신은 데이터를 엄격히 다루는 어시스턴트입니다.\\n\\n규칙:\\n- '{today}가 생일인 유명인 {n_val}명:'으로 시작\\n- 한 문장에 한 명: 1. 이름 (생년월일)\\n- 예시만 따르기, 설명・공백・인삿말 없이\\n\\n예시:\\n{today}가 생일인 유명인 3명:\\n1. 알베르트 아인슈타인 (1879년 3월 14일)\\n2. 김연아 (1990년 9월 5일)\\n3. BTS의 정국 (1997년 9월 1일)\\n\\n---\\n\\n{today}가 생일인 유명인 {n_val}명을 위와 같은 형식으로 출력하세요.\") | ChatGoogleGenerativeAI(model='models/gemini-2.5-flash-lite', google_api_key=SecretStr('**********'), temperature=0.1, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x1081a8eb0>, default_metadata=(), model_kwargs={})| StrOutputParser(),n_val: RunnableLambda(itemgetter('n_val'))}]`\n",
    "            * 프롬프트 템플릿과 실제 언어 모델(ChatGoogleGenerativeAI)이 어떻게 연결되어 있는지 나타냄\n",
    "        * last=`RunnableLambda(post_process)`\n",
    "            * 마지막으로 StrOutputParser()를 사용하여 모델의 응답을 문자열로 파싱하는 단계를 보여줌\n",
    "\n",
    "        ---\n",
    "\n",
    "        * --- 체인 실행 결과 ---\n",
    "        * 1. 헨리 포드 (1863년 7월 30일)\n",
    "        * 2. 닐 암스트롱 (1930년 8월 5일)\n",
    "        * 3. 톰 행크스 (1956년 7월 9일)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47e0030",
   "metadata": {},
   "source": [
    "#### (5) **`itemgetter`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a07e563",
   "metadata": {},
   "source": [
    "* **`operator.itemgetter`** \n",
    "  * 개념\n",
    "    * 파이썬의 표준 라이브러리인 operator 모듈에 있는 함수\n",
    "    * **딕셔너리**나 **리스트** 같은 객체에서 **특정 키(key) 또는 인덱스(index)에 해당하는 값**을 효율적으로 추출하기 위한 **호출 가능한(callable)** 객체를 반환\n",
    "\n",
    "  * Q. 언제 쓸까?\n",
    "    * A1. **딕셔너리**나 **리스트**처럼 여러 값이 모여 있는 데이터에서 **특정 키나 인덱스에 해당하는 값** 하나 또는 여러 개를 **효율적으로 꺼내고 싶을 때 사용**\n",
    "        * 딕셔너리 예시: {'name': '앨리스', 'age': 30}\n",
    "        * 리스트 예시: ['사과', '바나나', '오렌지']\n",
    "    * A2. 특히 **데이터를 가공**하거나, **특정 필드만** 다음 단계로 넘겨야 할 때\n",
    "\n",
    "  * 특징\n",
    "    * **간결하고 명확**\n",
    "        * `lambda x: x['key']` 같은 복잡하게 람다 함수 사용할 필요 X\n",
    "        * `itemgetter('key')` -> **직관적으로 특정 값 호출 가능**\n",
    "    * **재사용성 및 효율성**\n",
    "        *  **한 번 만들어진 itemgetter 객체는 여러 데이터에 반복적으로 적용 가능**\n",
    "        *  **C 언어로 구현** -> 람다 함수보다 약간 더 빠를 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a95ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# itemgetter_1\n",
    "\n",
    "####################################\n",
    "# 1. 필요한 라이브러리 및 클래스, 모듈 임포트\n",
    "####################################\n",
    "# 필요한 라이브러리 임포트 (기타 반복되는 임포트 생략)\n",
    "from operator import itemgetter\n",
    "from langchain_core.prompts import ChatPromptTemplate               # 채팅 프롬프트 템플릿을 위한 클래스 임포트\n",
    "\n",
    "####################################\n",
    "# 2. 입력 가공 및 준비\n",
    "####################################\n",
    "\n",
    "# 문장의 길이를 반환하는 함수\n",
    "def length_function(text):\n",
    "    return len(text)\n",
    "\n",
    "\n",
    "# 두 문장의 길이를 곱한 값을 반환하는 내부(helper)함수\n",
    "def _multiple_length_function(text1, text2):\n",
    "    return len(text1) * len(text2)\n",
    "\n",
    "\n",
    "# _multiple_length_function 함수를 사용하여 두 문장의 길이를 곱한 값을 반환하는 함수\n",
    "# 딕셔너리 형태의 입력 값을 처리하도록 설계됨\n",
    "def multiple_length_function(_dict):\n",
    "    return _multiple_length_function(_dict[\"text1\"], _dict[\"text2\"])\n",
    "\n",
    "####################################\n",
    "# 3. LLM 호출\n",
    "####################################\n",
    "\n",
    "# 프롬프트 템플릿 정의: {a}와 {b} 값을 받아 질문 생성\n",
    "prompt_itemgetter = ChatPromptTemplate.from_template(\"{a} + {b} 는 무엇인가요?\")\n",
    "\n",
    "# 모델 초기화 (gemini-1.5-flash)\n",
    "try:\n",
    "    model = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-1.5-flash\", # 빠른 응답을 위한 Gemini 1.5 Flash 모델 사용\n",
    "        temperature=0.1, # 일관된 답변을 위해 낮은 온도 설정\n",
    "    )\n",
    "    print(\"✅ Google GenAI 모델 초기화 성공.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Google GenAI 모델 초기화 실패: {e}\")\n",
    "    print(\"  -> GOOGLE_API_KEY 환경 변수가 올바르게 설정되었는지 확인하세요.\")\n",
    "    print(\"---\")\n",
    "    exit() # 모델 초기화 실패 시 프로그램 종료\n",
    "\n",
    "# 기본적인 chain 구성 예시\n",
    "chain1 = prompt_itemgetter | model          # 이 예시에서 사용되지는 않지만 체인 구성의 기본적인 틀을 보여줌\n",
    "\n",
    "# main chain 생성 : 입력 가공 -> 프롬프트 채우기 -> 모델 호출\n",
    "\"\"\"\n",
    "이 체인은 입력 값을 가공해 프롬프트 템플릿에 전달하여 완성된 질문을 언어 모델로 호출하여 다시 전달한 후 답변을 얻어내는 과정을 보여줍니다.\n",
    "    1. 입력 가공 : 'word1'과 'word2' 두 개의 문자열 입력을 받아 처리합니다.\n",
    "        - 'a' 값\n",
    "            - 'word1'의 길이\n",
    "            - 입력 딕셔너리 -> 'word1' 추출 -> 앞서 정의한 def length_function으로 길이 계산\n",
    "        -'b' 값\n",
    "            - 'word1' 길이와 'word2' 길이를 곱한 값\n",
    "            - 파이썬 내장 라이브러리 함수인 itemgetter 사용\n",
    "            - 입력 딕셔너리에서 'word1'과 'word2'를 추출하여 {'text1': word1, 'text2': word2} 형태의 딕셔너리 생성\n",
    "            - 앞서 정의한 def multiple_length_function으로 두 길의 곱 계산\n",
    "\n",
    "    2. 가공된 {\"a\": 값, \"b\": 값} 딕셔너리를 prompt 템플릿에 전달\n",
    "        - {\"a\" : word1의 길이, \"b\": word1 길이 * word2 길이}\n",
    "        - dict 형태 -> 질문이 완성되면 model에게 전달\n",
    "        \n",
    "    3. model 호출 -> 최종 답변 받기\n",
    "\n",
    "즉, 이 체인은 두 개의 단어를 입력받아 그 단어들의 길이를 계산하고, 이를 변형하여 새로운 숫자로 만든 뒤, 그 숫자들을 조합한 질문을 LLM에게 던져 답을 얻는 로직을 보여줍니다.\n",
    "chain1은 단순히 프롬프트와 모델을 연결한 기본적인 예시이며, chain_itemgetter가 핵심 로직을 담고 있습니다.\n",
    "\"\"\"\n",
    "chain_itemgetter = (\n",
    "    {\n",
    "        \"a\": itemgetter(\"word1\") | RunnableLambda(length_function),         # 'a': 입력 딕셔너리 'word1' 추출 -> length_function으로 길이 계산\n",
    "        \"b\": {\"text1\": itemgetter(\"word1\"), \"text2\": itemgetter(\"word2\")}   # 'b': 입력 딕셔너리 'word1'과 'word2'를 추출 -> {'text1': word1, 'text2': word2} 딕셔너리 생성   \n",
    "        | RunnableLambda(multiple_length_function),                         # 위 딕셔너리를 multiple_length_function에 전달하여 두 길이의 곱 계산\n",
    "    }\n",
    "    | prompt_itemgetter                     # 가공된 {\"a\": 값, \"b\": 값} 딕셔너리를 prompt 템플릿에 전달\n",
    "    | model                                 # 완성된 질문을 언어 모델에 전달하여 답변 얻기\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52b78ce",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* \n",
    "    * 셀 출력\n",
    "        * ✅ Google GenAI 모델 초기화 성공."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "426af0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 체인 실행 시작 ---\n",
      "과정: content='5 + 25 = 30 입니다.' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []} id='run--d1e1f6aa-cac4-4484-9766-3b1adc6dae55-0' usage_metadata={'input_tokens': 12, 'output_tokens': 12, 'total_tokens': 24, 'input_token_details': {'cache_read': 0}}\n",
      "최종 답변: 5 + 25 = 30 입니다.\n",
      "--- 체인 실행 종료 ---\n"
     ]
    }
   ],
   "source": [
    "# chain_itemgetter 실행\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n--- 체인 실행 시작 ---\")\n",
    "    try:\n",
    "        result = chain_itemgetter.invoke({\"word1\": \"hello\", \"word2\": \"world\"})\n",
    "        print(f\"과정: {result}\")\n",
    "        print(f\"최종 답변: {result.content}\")\n",
    "    except NameError:\n",
    "        print(\"❌ 'chain_itemgetter'가 정의되지 않았습니다. 이전 셀을 먼저 실행해 주세요.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 체인 실행 중 오류 발생: {e}\")\n",
    "    print(\"--- 체인 실행 종료 ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b02993",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* \n",
    "    * 셀 출력\n",
    "        * --- 체인 실행 시작 ---\n",
    "        * 과정: `content='5 + 25 = 30 입니다.' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []} id='run--d1e1f6aa-cac4-4484-9766-3b1adc6dae55-0' usage_metadata={'input_tokens': 12, 'output_tokens': 12, 'total_tokens': 24, 'input_token_details': {'cache_read': 0}}`\n",
    "        * 최종 답변: 5 + 25 = 30 입니다.\n",
    "        * --- 체인 실행 종료 ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5e8c45a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='5 + 25 = 30 입니다.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, id='run--7b4398e3-3cc6-4d83-9a1d-a95ecf1f8707-0', usage_metadata={'input_tokens': 12, 'output_tokens': 12, 'total_tokens': 24, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 교재와 동일하게 출력해보기\n",
    "\n",
    "chain_itemgetter.invoke({\"word1\": \"hello\", \"word2\": \"world\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7d09ae",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* \n",
    "    * AIMessage 객체 출력의 차이점\n",
    "\n",
    "    | 특성 / 출력 방식    | `chain.invoke(...)` 직접 출력 (`__repr__` 호출) | `print(result)` (`__str__` 호출)                  |\n",
    "    | :------------------ | :--------------------------------------------- | :------------------------------------------------ |\n",
    "    | **목적** | 개발자/디버깅용                                  | 사용자 친화적/코드 내 출력용                      |\n",
    "    | **표현 내용** | 객체의 모든 속성 (Content, 메타데이터, ID 등 상세 정보) | 객체의 핵심 내용 (주로 `content`만)              |\n",
    "    | **형식** | `AIMessage(content='...', response_metadata={...}, id='...', ...)` | `5 + 25 = 30 입니다.` (간결한 텍스트)             |\n",
    "    | **가독성** | 상세하지만 다소 복잡                               | 간결하고 읽기 쉬움                                |\n",
    "    | **주요 사용처** | 객체의 상태/정보 전체를 확인해야 할 때             | 최종 결과를 보여주거나 로그로 남길 때             |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
