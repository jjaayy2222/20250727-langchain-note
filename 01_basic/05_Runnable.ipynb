{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19356e6f",
   "metadata": {},
   "source": [
    "#### (1) Runnable ì´ë€?\n",
    "\n",
    "* LangChainì—ì„œ **ë‹¤ì–‘í•œ ì»´í¬ë„ŒíŠ¸(ì˜ˆ: LLM, í”„ë¡¬í”„íŠ¸, íŒŒì„œ ë“±)ë¥¼ ì—°ê²°í•˜ê³  ì‹¤í–‰í•˜ê¸° ìœ„í•œ ê¸°ë³¸ ì¸í„°í˜ì´ìŠ¤**\n",
    "\n",
    "* ì—¬ëŸ¬ ì»´í¬ë„ŒíŠ¸ë¥¼ ì—°ê²° -> ë³µì¡í•œ ë°ì´í„° ì²˜ë¦¬ **íŒŒì´í”„ë¼ì¸ êµ¬ì¶•** ê°€ëŠ¥\n",
    "    * ì˜ˆì‹œ: prompt -> LLMì— ì „ë‹¬ -> LLMì˜ ì¶œë ¥ì„ íŒŒì„œë¡œ ì „ë‹¬\n",
    "    * `invoke`, `batch`, `stream` ë“±ì˜ ë©”ì„œë“œë¥¼ í†µí•´ ì‹¤í–‰\n",
    "    * ê° Runnabelì€ ì…ë ¥ ë°ì´í„°ë¥¼ ë°›ì•„ ì²˜ë¦¬í•˜ê³ , ê·¸ ê²°ê³¼ë¥¼ ë‹¤ìŒ Runnableì— ì „ë‹¬í•˜ëŠ” ì—­í• \n",
    "\n",
    "* **LCELê³¼ì˜ ê´€ê³„** : LCELì€ Runnableì„ ê¸°ë°˜ìœ¼ë¡œ Chainì„ ë³´ë‹¤ ì‰½ê²Œ êµ¬ì„±í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ì„ ì–¸ì ì¸ ì–¸ì–´\n",
    "    * \n",
    "    * `RunnablePassthrough`: ì…ë ¥ì„ ë³€ê²½í•˜ì§€ ì•Šê±°ë‚˜ ì¶”ê°€ í‚¤ë¥¼ ë”í•˜ì—¬ ì „ë‹¬\n",
    "        * `RunnablePassthrough()`: ë‹¨ë…ìœ¼ë¡œ í˜¸ì¶œ -> ë‹¨ìˆœíˆ ì…ë ¥ì„ ë°›ì•„ ê·¸ëŒ€ë¡œ ì „ë‹¬\n",
    "        * `RunnablePassthrough()`: ì…ë ¥ì„ ë°›ì•„ assign í•¨ìˆ˜ì— ì „ë‹¬ëœ ì¶”ê°€ ì¸ìˆ˜ë¥¼ ì¶”ê°€\n",
    "    * `RunnableParallel`: ì—¬ëŸ¬ Runnableì„ ë³‘ë ¬ë¡œ ì‹¤í–‰\n",
    "    * `RunnableSequence`: `Runnable`ì˜ ì‹œí€€ìŠ¤ë¥¼ ì •ì˜í•¨\n",
    "    * `RunnableLambda`: ì‚¬ìš©ì ì •ì˜ í•¨ìˆ˜ë¥¼ `Runnable`ë¡œ ë˜í•‘í•¨\n",
    "    * \n",
    "    * ìœ„ì™€ ê°™ì´ ë‹¤ì–‘í•œ Runnable í´ë˜ìŠ¤ ì¡°í•©ìœ¼ë¡œ ì²´ì¸ êµ¬ì¶• ê°€ëŠ¥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb82a87",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2411517",
   "metadata": {},
   "source": [
    "* ê¸°ë³¸ í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c7a22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê¸°ë³¸ ëª¨ë“ˆ ì„í¬íŠ¸\n",
    "import os\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API KEYë¥¼ í™˜ê²½ë³€ìˆ˜ë¡œ ê´€ë¦¬í•˜ê¸° ìœ„í•œ ì„¤ì • íŒŒì¼\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API KEY ì •ë³´ ë¡œë“œ\n",
    "load_dotenv()                   # true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc449415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í™˜ê²½ ë³€ìˆ˜ í™•ì¸í•˜ê¸°\n",
    "\n",
    "# ë§ˆìŠ¤í‚¹ ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜\n",
    "def mask_key(key: str, visible_count: int = 2) -> str:\n",
    "    if not key or len(key) <= visible_count:\n",
    "        return '*' * len(key)\n",
    "    return key[:visible_count] + '*' * (len(key) - visible_count)\n",
    "\n",
    "# í™˜ê²½ë³€ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"GOOGLE_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# ë§ˆìŠ¤í‚¹ëœ í˜•íƒœë¡œ ì¶œë ¥\n",
    "print(f\"GOOGLE_API_KEY: {mask_key(api_key)}\")           # GOOGLE_API_KEY: AI*************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8679f034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangSmith ì¶”ì  ì„¤ì • (https://smith.langchain.com)\n",
    "\n",
    "\"\"\"\n",
    "- !pip install -qU langsmith\n",
    "- !pip install -qU langchain-teddynote\n",
    "    -> ì œë¯¸ë‚˜ì´ì™€ poetryì™€ì˜ ì˜ì¡´ì„± ì¶©ëŒë¡œ langchain_teddy ì„¤ì¹˜ X \n",
    "    -> langsmithë¡œ ì§„í–‰\n",
    "\"\"\"\n",
    "# LangSmith ì¶”ì ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "from langsmith import traceable         # @traceable ë°ì½”ë ˆì´í„° ì‚¬ìš© ì‹œ\n",
    "\n",
    "# LangSmith í™˜ê²½ ë³€ìˆ˜ í™•ì¸\n",
    "\n",
    "print(\"\\n--- LangSmith í™˜ê²½ ë³€ìˆ˜ í™•ì¸ ---\")\n",
    "langchain_tracing_v2 = os.getenv('LANGCHAIN_TRACING_V2')\n",
    "langchain_project = os.getenv('LANGCHAIN_PROJECT')\n",
    "langchain_api_key_status = \"ì„¤ì •ë¨\" if os.getenv('LANGCHAIN_API_KEY') else \"ì„¤ì •ë˜ì§€ ì•ŠìŒ\" # API í‚¤ ê°’ì€ ì§ì ‘ ì¶œë ¥í•˜ì§€ ì•ŠìŒ\n",
    "\n",
    "if langchain_tracing_v2 == \"true\" and os.getenv('LANGCHAIN_API_KEY') and langchain_project:\n",
    "    print(f\"âœ… LangSmith ì¶”ì  í™œì„±í™”ë¨ (LANGCHAIN_TRACING_V2='{langchain_tracing_v2}')\")\n",
    "    print(f\"âœ… LangSmith í”„ë¡œì íŠ¸: '{langchain_project}'\")\n",
    "    print(f\"âœ… LangSmith API Key: {langchain_api_key_status}\")\n",
    "    print(\"  -> ì´ì œ LangSmith ëŒ€ì‹œë³´ë“œì—ì„œ ì´ í”„ë¡œì íŠ¸ë¥¼ í™•ì¸í•´ ë³´ì„¸ìš”.\")\n",
    "else:\n",
    "    print(\"âŒ LangSmith ì¶”ì ì´ ì™„ì „íˆ í™œì„±í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‹¤ìŒì„ í™•ì¸í•˜ì„¸ìš”:\")\n",
    "    if langchain_tracing_v2 != \"true\":\n",
    "        print(f\"  - LANGCHAIN_TRACING_V2ê°€ 'true'ë¡œ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤ (í˜„ì¬: '{langchain_tracing_v2}').\")\n",
    "    if not os.getenv('LANGCHAIN_API_KEY'):\n",
    "        print(\"  - LANGCHAIN_API_KEYê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
    "    if not langchain_project:\n",
    "        print(\"  - LANGCHAIN_PROJECTê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98c6653",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* ì…€ ì¶œë ¥\n",
    "    * --- LangSmith í™˜ê²½ ë³€ìˆ˜ í™•ì¸ ---\n",
    "    * âœ… LangSmith ì¶”ì  í™œì„±í™”ë¨ (LANGCHAIN_TRACING_V2='true')\n",
    "    * âœ… LangSmith í”„ë¡œì íŠ¸: 'L***************'\n",
    "    * âœ… LangSmith API Key: ì„¤ì •ë¨\n",
    "    *   -> ì´ì œ LangSmith ëŒ€ì‹œë³´ë“œì—ì„œ ì´ í”„ë¡œì íŠ¸ë¥¼ í™•ì¸í•´ ë³´ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d6d944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain ë° Google GenAI ëª¨ë¸ ê´€ë ¨ ëª¨ë“ˆ ì„í¬íŠ¸\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI           # Google GenAI ì„í¬íŠ¸\n",
    "\n",
    "\n",
    "print(\"\\n--- LangChain ì²´ì¸ ì„¤ì • ---\")\n",
    "\n",
    "template = \"{topic}ì— ëŒ€í•´ 3ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´ì¤˜.\"                           # í…œí”Œë¦¿ ì •ì˜\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)                     # í”„ë¡¬í”„íŠ¸ë¥¼ í”„ë¡¬í”„íŠ¸í…œí”Œë¦¿ ê°ì²´ë¡œ ìƒì„±\n",
    "\n",
    "try:\n",
    "    model = ChatGoogleGenerativeAI(                                 # ëª¨ë¸ í˜¸ì¶œ\n",
    "        model=\"gemini-1.5-flash\",\n",
    "        temperature=0.1,\n",
    "    )\n",
    "    print(\"âœ… Google GenAI ëª¨ë¸ ì´ˆê¸°í™” ì„±ê³µ.\")\n",
    "except Exception as e:                                              # ë””ë²„ê¹… ë©”ì‹œì§€\n",
    "    print(f\"âŒ Google GenAI ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}\")\n",
    "    print(\"  -> GOOGLE_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "    \n",
    "# ì¶œë ¥ íŒŒì„œ\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# ì²´ì¸ êµ¬ì„±\n",
    "chain = prompt | model | output_parser                              # í”„ë¡¬í”„íŠ¸, ëª¨ë¸, ì¶œë ¥ íŒŒì„œ ì—°ê²° -> ì²´ì¸ êµ¬ì„±\n",
    "print(\"âœ… LangChain LCEL ì²´ì¸ êµ¬ì„± ì™„ë£Œ.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4771870e",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* ì…€ ì¶œë ¥\n",
    "    * --- LangChain ì²´ì¸ ì„¤ì • ---\n",
    "    * âœ… Google GenAI ëª¨ë¸ ì´ˆê¸°í™” ì„±ê³µ.\n",
    "    * âœ… LangChain LCEL ì²´ì¸ êµ¬ì„± ì™„ë£Œ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2b60a2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fce3b2d",
   "metadata": {},
   "source": [
    "#### (2) **`RunnablePassthrough`**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4d48ac",
   "metadata": {},
   "source": [
    "* **`RunnablePassthrough`**\n",
    "  \n",
    "  * **ì…ë ¥ ë°ì´í„°ë¥¼ ë³€í˜•í•˜ì§€ ì•Šê³  ê·¸ëŒ€ë¡œ í†µê³¼**ì‹œí‚¤ê±°ë‚˜, **í•„ìš”ì— ë”°ë¼ ì¼ë¶€ í‚¤/ê°’ì„ ì¶”ê°€**í•´ ì…ë ¥ì— í•¨ê»˜ ì „ë‹¬í•  ìˆ˜ ìˆëŠ” Runnable ê°ì²´\n",
    "\n",
    "  * Q. ì–¸ì œ ì“¸ê¹Œ?\n",
    "    * A1. ì—¬ëŸ¬ ì²´ì¸ì—ì„œ, **ì…ë ¥ ë°ì´í„°ì˜ íŠ¹ì • ë¶€ë¶„ì„ ê·¸ëŒ€ë¡œ ë„˜ê¸°ê³  ì‹¶ì„ ë•Œ**\n",
    "    * A2. **ê¸°ì¡´ ì…ë ¥ë°ì´í„° + ì¶”ê°€ ë°ì´í„° ì¡°í•©ì´ í•„ìš”**í•  ë•Œ\n",
    "      * ì˜ˆì‹œ: `assign` í™œìš©\n",
    "    \n",
    "  * íŠ¹ì§•\n",
    "      * `.invoke()` -> **ì…ë ¥ì„ ê·¸ëŒ€ë¡œ ë°˜í™˜**\n",
    "      * `.assign()` -> ì…ë ¥ dictì— **ì›í•˜ëŠ” ê³„ì‚° ê²°ê³¼ë¥¼ ë³‘í•©**í•´ì¤Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9227ff00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# êµì¬ì™€ ê°™ì´ ì‹œë„\n",
    "\n",
    "# prompt, model ìƒì„±\n",
    "prompt = PromptTemplate.from_template(\"{num}ì˜ 10ëŠ”?\")\n",
    "\n",
    "try:\n",
    "    model = ChatGoogleGenerativeAI(                                 # ê¸°ë³¸ ëª¨ë¸ í˜¸ì¶œ\n",
    "        model=\"gemini-1.5-flash\",\n",
    "        temperature=0.1,\n",
    "    )\n",
    "    print(\"âœ… Google GenAI ëª¨ë¸ ì´ˆê¸°í™” ì„±ê³µ.\")\n",
    "    print(\"---\")\n",
    "    \n",
    "except Exception as e:                                              # ë””ë²„ê¹… ë©”ì‹œì§€\n",
    "    print(f\"âŒ Google GenAI ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}\")\n",
    "    print(\"  -> GOOGLE_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "    print(\"---\")\n",
    "\n",
    "# Chain ìƒì„±\n",
    "chain = prompt | model\n",
    "\n",
    "# Chain ì‹¤í–‰\n",
    "chain.invoke({\"num\":5})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de87312e",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* \n",
    "    * ì…€ ì¶œë ¥\n",
    "        * âœ… Google GenAI ëª¨ë¸ ì´ˆê¸°í™” ì„±ê³µ.\n",
    "        * ---\n",
    "        * `AIMessage(content='5ì˜ 10ìŠ¹ì€ 5ë¥¼ 10ë²ˆ ê³±í•œ ê°’ì…ë‹ˆë‹¤.  ì¦‰, 5 x 5 x 5 x 5 x 5 x 5 x 5 x 5 x 5 x 5 = **9,765,625** ì…ë‹ˆë‹¤.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, id='run--a7f7c69f-7477-42fd-a200-93d5f949b716-0', usage_metadata={'input_tokens': 7, 'output_tokens': 67, 'total_tokens': 74, 'input_token_details': {'cache_read': 0}})`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af5979c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1ê°œì˜ ë³€ìˆ˜ë§Œ í…œí”Œë¦¿ì— í¬í•¨í•˜ê³  ìˆë‹¤ë©´ ê°’ë§Œ ì „ë‹¬í•˜ëŠ” ê²ƒë„ ê°€ëŠ¥\n",
    "chain.invoke(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d7b034",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* \n",
    "    * ì…€ ì¶œë ¥\n",
    "        * `AIMessage(content='5ì˜ 10ìŠ¹ì€ 5 x 5 x 5 x 5 x 5 x 5 x 5 x 5 x 5 x 5 = **9,765,625** ì…ë‹ˆë‹¤.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, id='run--2b1a9028-cdd9-4748-be61-0f20d3dab11f-0', usage_metadata={'input_tokens': 7, 'output_tokens': 51, 'total_tokens': 58, 'input_token_details': {'cache_read': 0}})`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb99dbde",
   "metadata": {},
   "source": [
    "* **`RunnablePassthrough()`**: ì…ë ¥ ê°’ì„ **ê·¸ëŒ€ë¡œ ë°›ì•„ ì „ë‹¬**\n",
    "    * `invoke()` ë©”ì†Œë“œì™€ ì‚¬ìš©\n",
    "    * chain êµ¬ì„± ì—°ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6596414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RunnablePassthrough ê´€ë ¨ ëª¨ë“ˆ ì„í¬íŠ¸\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# RunnablePassthrough()_1\n",
    "RunnablePassthrough().invoke({\"num: 10\"})                           # {'num: 10'}\n",
    "\n",
    "# RunnablePassthrough()_2\n",
    "result = RunnablePassthrough().invoke({\"num\": 10})\n",
    "print(result)                                                       # {'num': 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7f5fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RunnablePassthrough -> Chain êµ¬ì„±í•˜ê¸°\n",
    "\n",
    "runnablepassthrough_chain = {\"num\" : RunnablePassthrough()} | prompt | model\n",
    "\n",
    "# dict ê°’ì´ RunnablePassthrough()ë¡œ ë³€ê²½ë¨\n",
    "runnablepassthrough_chain.invoke(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61451e3f",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* \n",
    "    * ì…€ ì¶œë ¥\n",
    "        * `AIMessage(content='10ì˜ 10ìŠ¹ì€ 10,000,000,000 (100ì–µ) ì…ë‹ˆë‹¤.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, id='run--2c9d7372-8b1c-4da2-928d-da5bfe77de31-0', usage_metadata={'input_tokens': 8, 'output_tokens': 32, 'total_tokens': 40, 'input_token_details': {'cache_read': 0}})`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f73062a",
   "metadata": {},
   "source": [
    "* **`RunnablePassthrough()`**: ì…ë ¥ ê°’ì— **ì¶”ê°€ í‚¤ ë”í•˜ì—¬ ì „ë‹¬ ê°€ëŠ¥**\n",
    "    * `assign()` í•¨ìˆ˜ì— ì „ë‹¬ëœ ì¶”ê°€ ì¸ìˆ˜ ì „ë‹¬\n",
    "    * chain êµ¬ì„± ì—°ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3a51e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RunnablePassthrough.assign()_1\n",
    "RunnablePassthrough.assign().invoke({\"num\": 1})         # {'num': 1}\n",
    "\n",
    "# RunnablePassthrough.assign()_2\n",
    "result_assign = RunnablePassthrough.assign().invoke({\"num\": 1})\n",
    "print(result_assign)                                      # {`num``: 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c58314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RunnablePassthrough.assign()_3 - ìƒˆë¡­ê²Œ í• ë‹¹ëœ key/value ìŒ ë³‘í•©\n",
    "# ì…ë ¥ í‚¤: num, í• ë‹¹(assign) í‚¤: new_num\n",
    "\n",
    "(RunnablePassthrough.assign(new_num=lambda x: x[\"num\"] * 3)).invoke({\"num\": 1})             # {'num': 1, 'new_num': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095b5da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RunnablePassthrough.assign()_4 - assignì— í•¨ìˆ˜ ì¶”ê°€í•´ë³´ê¸°\n",
    "\n",
    "result_assign2 = RunnablePassthrough().assign(new_num=lambda x: x[\"num\"] * 3)\n",
    "print(result_assign2.invoke({\"num\": 5}))                                                    # {'num': 5, 'new_num': 15}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac1e186",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144bd4b8",
   "metadata": {},
   "source": [
    "#### (3) **`RunnableParallel`**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c7a3a4",
   "metadata": {},
   "source": [
    "* **`RunnableParallel`**\n",
    "  \n",
    "  * ì…ë ¥ ë°ì´í„°ë¥¼ **ì—¬ëŸ¬ ê°œì˜ Runnable ì²´ì¸ì— ë³‘ë ¬ë¡œ ì „ë‹¬**í•˜ê³ , ê° ì‹¤í–‰ **ê²°ê³¼ë¥¼ dictí˜•íƒœë¡œ ë°˜í™˜**\n",
    "\n",
    "  * Q. ì–¸ì œ ì“¸ê¹Œ?\n",
    "    * A. ì—¬ëŸ¬ ëª¨ë¸ or ë‹¤ë¥¸ ì—°ì‚°ì„ ë™ì‹œì— ëŒë ¤ **ë‹¤ì¤‘ ê²°ê³¼ê°€ í•„ìš”**í•œ ê²½ìš°\n",
    "      * ì˜ˆì‹œ_1: í•œ ì…ë ¥ìœ¼ë¡œ ì—¬ëŸ¬ LLM ì§ˆì˜\n",
    "      * ì˜ˆì‹œ_2: ì „ì²˜ë¦¬ + í›„ì²˜ë¦¬ ê²°ê³¼ ë™ì‹œ ë°˜í™˜ ë“±\n",
    "    \n",
    "  * íŠ¹ì§•\n",
    "      * **dictí˜•íƒœ** -> **Keyë³„ë¡œ runnable ë§¤í•‘ ê°€ëŠ¥**\n",
    "      * **ê²°ê³¼ ë˜í•œ Keyë³„ë¡œ ì¶”ì¶œ**\n",
    "      * ë³‘ë ¬ ì‹¤í–‰ì€ ë‚´ë¶€ì ìœ¼ë¡œ `await`/`gather`ë¡œ ì²˜ë¦¬, **I/O ì°¨ì›ì—ì„  ë™ì‹œì— ì‹¤í–‰**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5036e963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RunnableParallel ëª¨ë“ˆ ì„í¬íŠ¸\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "# RunnableParallel ì¸ìŠ¤í„´ìŠ¤ ìƒì„± -> ì—¬ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ë³‘ë ¬ ì‹¤í–‰ ê°€ëŠ¥\n",
    "runnable_parallel = RunnableParallel(\n",
    "    # RunnablePassthrough() = 'passed' í‚¤ì›Œë“œ ì¸ìë¡œ ì „ë‹¬ -> ì…ë ¥ëœ ë°ì´í„°ë¥¼ ê·¸ëŒ€ë¡œ í†µê³¼\n",
    "    passed=RunnablePassthrough(),\n",
    "    \n",
    "    # 'extra' í‚¤ì›Œë“œ ì¸ìë¡œ RunnablePassthrough.assign('mult' ëŒë‹¤ í•¨ìˆ˜) -> ë”•ì…”ë„ˆë¦¬ì˜ 'num' í‚¤ì— í•´ë‹¹í•˜ëŠ” ê°’ì„ 3ë°°ë¡œ ì¦ê°€\n",
    "    extra=RunnablePassthrough.assign(mult=lambda x: x[\"num\"] * 3),\n",
    "    \n",
    "    # ëŒë‹¤ í•¨ìˆ˜ = 'modified' í‚¤ì›Œë“œ ì¸ì -> ì…ë ¥ëœ ë”•ì…”ë„ˆë¦¬ì˜ 'num' í‚¤ì— í•´ë‹¹í•˜ëŠ” ê°’ì— 1 ë”í•˜ê¸°\n",
    "    modified=lambda x: x[\"num\"] + 1,\n",
    ")\n",
    "\n",
    "# {'num': 1} ë”•ì…”ë„ˆë¦¬ ì…ë ¥ -> invoke ë©”ì†Œë“œë¡œ í˜¸ì¶œí•´ë³´ê¸°\n",
    "runnable_parallel.invoke({\"num\": 1})                                \n",
    "\n",
    "# {'passed': {'num': 1}, 'extra': {'num': 1, 'mult': 3}, 'modified': 2} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5b5fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time                                                                                 #ì‹œê°„ ì¸¡ì •ì„ ìœ„í•´ ì„í¬íŠ¸\n",
    "\n",
    "# ëª¨ë¸ ì´ˆê¸°í™” (gemini-2.5-flash-lite ì‚¬ìš©)\n",
    "try:\n",
    "    model2 = ChatGoogleGenerativeAI(                                 \n",
    "        model=\"gemini-2.5-flash-lite\",\n",
    "        temperature=0.1,\n",
    "    )\n",
    "    print(\"âœ… Google GenAI ëª¨ë¸ ì´ˆê¸°í™” ì„±ê³µ.\")\n",
    "    print(\"---\")\n",
    "except Exception as e:                                              \n",
    "    print(f\"âŒ Google GenAI ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}\")\n",
    "    print(\"  -> GOOGLE_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "    print(\"---\")\n",
    "    exit() # ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ í”„ë¡œê·¸ë¨ ì¢…ë£Œ\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ êµ¬ì„±\n",
    "prompt_capital = PromptTemplate.from_template(\"{country}ì˜ ìˆ˜ë„ëŠ”?\")\n",
    "prompt_area = PromptTemplate.from_template(\"{country}ì˜ ë©´ì ì€?\")\n",
    "\n",
    "###################################################\n",
    "# ë™ê¸°ì  ì‹¤í–‰ê³¼ RunnableParaller ì‹¤í–‰ì„ ëª¨ë‘ ì§„í–‰í•˜ì—¬ ë¹„êµ #\n",
    "###################################################\n",
    "\n",
    "# ë™ê¸°ì  ì‹¤í–‰ì„ ìœ„í•œ ê°œë³„ ì²´ì¸ (RunnableParallel ì—†ì´ ìˆœì°¨ì ìœ¼ë¡œ í˜¸ì¶œ)\n",
    "chain_capital_sync = {\"country\": RunnablePassthrough()} | prompt_capital | model2\n",
    "chain_area_sync = {\"country\": RunnablePassthrough()} | prompt_area | model2\n",
    "\n",
    "# RunnableParallelì„ ì‚¬ìš©í•œ ë³‘ë ¬ ì²´ì¸\n",
    "# model2ë¥¼ ì‚¬ìš©í•˜ë„ë¡ ë³€ê²½\n",
    "combined_chain_parallel = RunnableParallel(capital=chain_capital_sync, area=chain_area_sync)\n",
    "\n",
    "# --- ë™ê¸°ì (ìˆœì°¨ì ) ì‹¤í–‰ ì‹œê°„ ì¸¡ì • ---\n",
    "print(\"--- ë™ê¸°ì (ìˆœì°¨ì ) ì‹¤í–‰ ì‹œì‘ ---\")\n",
    "start_time_sync = time.time()\n",
    "\n",
    "# ë‘ ì²´ì¸ì„ ìˆœì°¨ì ìœ¼ë¡œ í˜¸ì¶œ\n",
    "capital_result_sync = chain_capital_sync.invoke(\"ëŒ€í•œë¯¼êµ­\")\n",
    "area_result_sync = chain_area_sync.invoke(\"ëŒ€í•œë¯¼êµ­\")\n",
    "\n",
    "end_time_sync = time.time()\n",
    "\n",
    "print(f\"ìˆ˜ë„ ê²°ê³¼: {capital_result_sync}\")                                     \n",
    "print(f\"ë©´ì  ê²°ê³¼: {area_result_sync}\")                                         \n",
    "print(f\"ë™ê¸°ì  ì‹¤í–‰ ì‹œê°„: {end_time_sync - start_time_sync:.4f} ì´ˆ\")\n",
    "print(\"---\")\n",
    "\n",
    "# --- RunnableParallel (ë³‘ë ¬) ì‹¤í–‰ ì‹œê°„ ì¸¡ì • ---\n",
    "print(\"\\n--- RunnableParallel (ë³‘ë ¬) ì‹¤í–‰ ì‹œì‘ ---\")\n",
    "start_time_parallel = time.time()\n",
    "\n",
    "# RunnableParallel ì²´ì¸ í˜¸ì¶œ\n",
    "result_parallel = combined_chain_parallel.invoke(\"ëŒ€í•œë¯¼êµ­\")\n",
    "\n",
    "end_time_parallel = time.time()\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥ (ë”•ì…”ë„ˆë¦¬ í˜•íƒœì´ë¯€ë¡œ ì§ì ‘ ì ‘ê·¼)\n",
    "print(f\"ë³‘ë ¬ ì‹¤í–‰ ê²°ê³¼ - ìˆ˜ë„: {result_parallel['capital']}\")                    \n",
    "print(f\"ë³‘ë ¬ ì‹¤í–‰ ê²°ê³¼ - ë©´ì : {result_parallel['area']}\")                      \n",
    "print(f\"ë³‘ë ¬ ì‹¤í–‰ ì‹œê°„: {end_time_parallel - start_time_parallel:.4f} ì´ˆ\")\n",
    "print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e8771b",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* \n",
    "    * ì…€ ì¶œë ¥\n",
    "        * âœ… Google GenAI ëª¨ë¸ ì´ˆê¸°í™” ì„±ê³µ.\n",
    "        * ---\n",
    "        * --- ë™ê¸°ì (ìˆœì°¨ì ) ì‹¤í–‰ ì‹œì‘ ---\n",
    "        * ìˆ˜ë„ ê²°ê³¼: content='ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” **ì„œìš¸**ì…ë‹ˆë‹¤.' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': []} id='run--aee20300-a869-415d-a425-69e21ccd30ed-0' usage_metadata={'input_tokens': 7, 'output_tokens': 10, 'total_tokens': 17, 'input_token_details': {'cache_read': 0}}\n",
    "        * ë©´ì  ê²°ê³¼: content='ëŒ€í•œë¯¼êµ­ì˜ ë©´ì ì€ ì•½ **100,410 ì œê³±í‚¬ë¡œë¯¸í„°**ì…ë‹ˆë‹¤.\\n\\nì´ëŠ” ì„¸ê³„ì—ì„œ 109ë²ˆì§¸ë¡œ ë„“ì€ ë©´ì ì´ë©°, í•œë°˜ë„ ì „ì²´ ë©´ì ì˜ ì•½ 45%ì— í•´ë‹¹í•©ë‹ˆë‹¤.' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': []} id='run--e9164940-80c1-4e10-bd51-cd6203de12b1-0' usage_metadata={'input_tokens': 8, 'output_tokens': 55, 'total_tokens': 63, 'input_token_details': {'cache_read': 0}}\n",
    "        * ë™ê¸°ì  ì‹¤í–‰ ì‹œê°„: 1.9750 ì´ˆ\n",
    "        * ---\n",
    "\n",
    "        * --- RunnableParallel (ë³‘ë ¬) ì‹¤í–‰ ì‹œì‘ ---\n",
    "        * ë³‘ë ¬ ì‹¤í–‰ ê²°ê³¼ - ìˆ˜ë„: content='ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” **ì„œìš¸**ì…ë‹ˆë‹¤.' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': []} id='run--485dc267-1502-4323-a326-501811f157b5-0' usage_metadata={'input_tokens': 7, 'output_tokens': 10, 'total_tokens': 17, 'input_token_details': {'cache_read': 0}}\n",
    "        * ë³‘ë ¬ ì‹¤í–‰ ê²°ê³¼ - ë©´ì : content='ëŒ€í•œë¯¼êµ­ì˜ ë©´ì ì€ ì•½ **100,410 ì œê³±í‚¬ë¡œë¯¸í„°**ì…ë‹ˆë‹¤.\\n\\nì´ëŠ” ì„¸ê³„ì ìœ¼ë¡œ ë³¼ ë•Œ ì¤‘ê°„ ì •ë„ì˜ í¬ê¸°ì´ë©°, í•œë°˜ë„ ì „ì²´ ë©´ì ì˜ ì•½ 45%ì— í•´ë‹¹í•©ë‹ˆë‹¤.' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': []} id='run--ba33162d-bd75-4ca4-9c9c-e38534d4afc5-0' usage_metadata={'input_tokens': 8, 'output_tokens': 52, 'total_tokens': 60, 'input_token_details': {'cache_read': 0}}\n",
    "        * ë³‘ë ¬ ì‹¤í–‰ ì‹œê°„: 1.2327 ì´ˆ\n",
    "        * ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59702bf7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0d4d31",
   "metadata": {},
   "source": [
    "#### (4) **`RunnableLambda`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7364dd",
   "metadata": {},
   "source": [
    "* **`RunnableLambda`**\n",
    "  \n",
    "  * íŒŒì´ì¬ì˜ **any í•¨ìˆ˜(ëŒë‹¤ ë“±)ë¥¼ ì²´ì¸ ë‚´ì—ì„œ ì‹¤í–‰**í•´ **ê²°ê³¼ë¥¼ ë°˜í™˜**í•˜ëŠ” Runnable ê°ì²´\n",
    "\n",
    "  * Q. ì–¸ì œ ì“¸ê¹Œ?\n",
    "    * A1. ì²´ì¸ ì¤‘ê°„ì— ì§ì ‘ì  ë°ì´í„° ê°€ê³µ/ì „ì²˜ë¦¬ê°€ í•„ìš”í•  ë•Œ\n",
    "    * A2. ì…ë ¥ê°’ ë™ì  ê³„ì‚°/í¬ë§·íŒ…/íŒŒì‹± ë¡œì§ì´ í•„ìš”í•  ë•Œ\n",
    "    \n",
    "  * íŠ¹ì§•\n",
    "      * ë‚˜ë§Œì˜ ì»¤ìŠ¤í…€ ì—°ì‚°ì„ ì²´ì¸ì— ì‰½ê²Œ ì‚½ì… ê°€ëŠ¥\n",
    "      * `invoke()` ì‹œ ì…ë ¥ê°’ ë°›ì•„ í•¨ìˆ˜ ì ìš©, ë°˜í™˜ê°’ ë°˜í™˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e94b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RunnalbeLambda_1\n",
    "\n",
    "# í•„ìš”í•œ ì„í¬íŠ¸\n",
    "from datetime import datetime\n",
    "\n",
    "def get_today(a):\n",
    "    # ì˜¤ëŠ˜ ë‚ ì§œ ê°€ì ¸ì˜¤ê¸°\n",
    "    return datetime.today().strftime(\"%b-%d\")\n",
    "\n",
    "# ì¶œë ¥í•˜ê¸°\n",
    "get_today(None)                                      # 'Jul-30'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00756346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RunnalbeLambda_2\n",
    "\n",
    "# í•„ìš”í•œ ëª¨ë“ˆ ì„í¬íŠ¸\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# ì…ë ¥ê°’ì˜ ì œê³±ì„ ë°˜í™˜í•˜ëŠ” lambda ì ìš©\n",
    "runnable_lambda = RunnableLambda(lambda x: x[\"num\"] ** 2)\n",
    "print(runnable_lambda.invoke({\"num\": 7}))                  # 49"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3097ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RunnableLambda_3 - Chain ìƒì„±\n",
    "\n",
    "def get_today(a):\n",
    "    return datetime.today().strftime(\"%b-%d\")\n",
    "\n",
    "\n",
    "# í•„ìš”í•œ ëª¨ë“ˆ ì„í¬íŠ¸\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
    "prompt_lambda = PromptTemplate.from_template(\n",
    "    \"{today}ê°€ ìƒì¼ì¸ ìœ ëª…ì¸ (n) ëª…ì„ ë‚˜ì—´í•˜ì„¸ìš”. ìƒë…„ì›”ì¼ì„ í‘œê¸°í•˜ì„¸ìš”.\"\n",
    ")\n",
    "\n",
    "# model = gemini-2.5-flash-lite ì‚¬ìš©\n",
    "try:\n",
    "    model2 = ChatGoogleGenerativeAI(                                 # ëª¨ë¸ í˜¸ì¶œ\n",
    "        model=\"gemini-2.5-flash-lite\",\n",
    "        temperature=0.1,\n",
    "    )\n",
    "    print(\"âœ… Google GenAI ëª¨ë¸ ì´ˆê¸°í™” ì„±ê³µ.\")\n",
    "    print(\"---\")\n",
    "except Exception as e:                                              # ë””ë²„ê¹… ë©”ì‹œì§€\n",
    "    print(f\"âŒ Google GenAI ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}\")\n",
    "    print(\"  -> GOOGLE_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "    print(\"---\")\n",
    "\n",
    "\n",
    "# chain ìƒì„±\n",
    "chain_lambda =(\n",
    "    {\"today\":RunnableLambda(get_today), \"n\":RunnablePassthrough()}\n",
    "    | prompt_lambda\n",
    "    | model2\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# ì²´ì¸ ê°ì²´ì˜ êµ¬ì„± ì¶œë ¥ (í˜„ì¬ êµì¬ì—ì„œ ë³´ì—¬ì£¼ëŠ” ë‹¨ê³„)\n",
    "print(\"--- Chain êµ¬ì„± ì •ë³´ ---\")\n",
    "print(chain_lambda)\n",
    "print(\"---\")\n",
    "\n",
    "# ì²´ì¸ ì‹¤í–‰ ë° ì‹¤ì œ ê²°ê³¼ ì¶œë ¥\n",
    "print(\"\\n--- ì²´ì¸ ì‹¤í–‰ ê²°ê³¼ ---\")\n",
    "try:\n",
    "    # \"n\"ì— í•´ë‹¹í•˜ëŠ” ê°’ì„ invoke() ë©”ì„œë“œì— ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ì „ë‹¬í•©ë‹ˆë‹¤.\n",
    "    # ì˜ˆ: 3ëª…ì˜ ìœ ëª…ì¸\n",
    "    result = chain_lambda.invoke({\"n\": 3})\n",
    "    print(result)\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ì²´ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67023db8",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* \n",
    "    * ì…€ ì¶œë ¥ \n",
    "        * âœ… Google GenAI ëª¨ë¸ ì´ˆê¸°í™” ì„±ê³µ.\n",
    "\n",
    "        ---\n",
    "\n",
    "    * --- Chain êµ¬ì„± ì •ë³´ ---\n",
    "        * chain_lambda ê°ì²´ ìì²´ì˜ **í‘œí˜„** = ì–´ë–»ê²Œ êµ¬ì„±ë˜ì—ˆëŠ”ì§€ë¥¼ í‘œí˜€ì¤Œ\n",
    "        * `RunnabelLambda`, `PromptTemplate`, `ChatGoogleGenerativeAI`, `StrOutParser` ê°™ì€ ì»´í¬ë„ŒíŠ¸ë“¤ì„ ì„±ê³µì ìœ¼ë¡œ ì—°ê²°í•´ì„œ í•˜ë‚˜ì˜ ì²´ì¸ì„ ë§Œë“¤ì—ˆìŒì„ ë³´ì—¬ì¤Œ \n",
    "          * `first={today: RunnableLambda(get_today), n: RunnablePassthrough()}` = ì²´ì¸ì˜ ì²« ë²ˆì§¸ ë‹¨ê³„ì—ì„œ ì…ë ¥(n)ê³¼ get_today í•¨ìˆ˜(today)ë¥¼ ì–´ë–»ê²Œ ì²˜ë¦¬í•˜ëŠ”ì§€ ë³´ì—¬ì¤Œ\n",
    "          * `middle=[PromptTemplate(input_variables=['today'], input_types={}, partial_variables={}, template='{today}ê°€ ìƒì¼ì¸ ìœ ëª…ì¸ (n) ëª…ì„ ë‚˜ì—´í•˜ì„¸ìš”. ìƒë…„ì›”ì¼ì„ í‘œê¸°í•˜ì„¸ìš”.'), ChatGoogleGenerativeAI(model='models/gemini-2.5-flash-lite', google_api_key=SecretStr('**********'), temperature=0.1, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x11f2079b0>, default_metadata=(), model_kwargs={})]` = í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ê³¼ ì‹¤ì œ ì–¸ì–´ ëª¨ë¸(ChatGoogleGenerativeAI)ì´ ì–´ë–»ê²Œ ì—°ê²°ë˜ì–´ ìˆëŠ”ì§€ ë‚˜íƒ€ëƒ„\n",
    "          * `last=StrOutputParser()` = ë§ˆì§€ë§‰ìœ¼ë¡œ StrOutputParser()ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì˜ ì‘ë‹µì„ ë¬¸ìì—´ë¡œ íŒŒì‹±í•˜ëŠ” ë‹¨ê³„ë¥¼ ë³´ì—¬ì¤Œ\n",
    "        \n",
    "        ---\n",
    "\n",
    "    * --- ì²´ì¸ ì‹¤í–‰ ê²°ê³¼ ---\n",
    "        * 7ì›” 30ì¼ì— ìƒì¼ì„ ë§ì€ ìœ ëª…ì¸ 5ëª…ì„ ìƒë…„ì›”ì¼ê³¼ í•¨ê»˜ ì•Œë ¤ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\n",
    "\n",
    "        * 1.  **í—¨ë¦¬ í¬ë“œ (Henry Ford)**\n",
    "        *     *   ìƒë…„ì›”ì¼: 1863ë…„ 7ì›” 30ì¼\n",
    "\n",
    "        * 2.  **ì—ë°€ë¦¬ ë¸Œë¡ í…Œ (Emily BrontÃ«)**\n",
    "        *     *   ìƒë…„ì›”ì¼: 1818ë…„ 7ì›” 30ì¼\n",
    "\n",
    "        * 3.  **í´ ì•¤ë”ìŠ¨ (Paul Anderson)**\n",
    "        *     *   ìƒë…„ì›”ì¼: 1932ë…„ 7ì›” 30ì¼\n",
    "\n",
    "        * 4.  **ë¦¬ì‚¬ ì¿¤íŠ¸ë¡œ (Lisa Kudrow)**\n",
    "        *     *   ìƒë…„ì›”ì¼: 1963ë…„ 7ì›” 30ì¼\n",
    "\n",
    "        * 5.  **í¬ë¦¬ìŠ¤í† í¼ ë©œë¡œë‹ˆ (Christopher Meloni)**\n",
    "        *     *   ìƒë…„ì›”ì¼: 1961ë…„ 7ì›” 30ì¼"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47b0ecf",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01e5ff5",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* \n",
    "    * **ë¬¸ì œì ** ë°œê²¬ : 3ëª… ì¶œë ¥ ìš”êµ¬ -> 5ëª…ì´ ì¶œë ¥\n",
    "      * ì£¼ìš” ì›ì¸1. modelì˜ **ììœ ì„±**\n",
    "          * í˜„ì¬ ì„¤ì •: temperature = 0.1 -> randomness or creativity ë‚®ìŒ \n",
    "      * ì£¼ìš” ì›ì¸2. **í”„ë¡¬í”„íŠ¸ì˜ ëª¨í˜¸ì„±**\n",
    "          * **ì •í™•íˆ**ë¼ëŠ” ê°•ì¡°ê°€ ë¶€ì¡±í–ˆì„ ìˆ˜ ìˆìŒ\n",
    "          * `ìœ ëª…ì¸ (n) ëª…ì„ ë‚˜ì—´í•˜ì„¸ìš”` -> `(n)` = ëª¨ë¸ì—ê²Œ `n`ì´ **ë‹¨ìˆœí•œ í”Œë ˆì´ìŠ¤í™€ë”**ì¸ì§€, ì•„ë‹ˆë©´ **ì •í™•íˆ ì§€ì¼œì•¼ í•  ìˆ«ì ì œí•œ**ì¸ì§€ **í˜¼ë™**ì„ ì¤„ ìˆ˜ ìˆìŒ\n",
    "            * ì¦‰, ëª¨ë¸ì´ **(n)ì˜ ì˜ë¯¸ë¥¼ ìˆ«ì ì œí•œìœ¼ë¡œ ì •í™•íˆ ì´í•´í•˜ì§€ ëª»í–ˆì„ ê°€ëŠ¥ì„±**\n",
    "          * í”„ë¡¬í”„íŠ¸ê°€ ê¸¸ì–´ì§€ê±°ë‚˜ ë³µì¡í•´ì§ˆìˆ˜ë¡, ëª¨ë¸ì€ íŠ¹ì • ìˆ«ì ì œí•œë³´ë‹¤ëŠ” **ì „ì²´ì ì¸ ë‚´ìš©ê³¼ ì˜ë¯¸ë¥¼ íŒŒì•…**í•˜ëŠ” ë° ì§‘ì¤‘í•  í•„ìš”ê°€ ìˆìŒ\n",
    "      * ì£¼ìš” ì›ì¸3. ë°ì´í„°ì˜ ë‹¤ì–‘ì„±\n",
    "          * ëª¨ë¸ì´ í•™ìŠµí•œ ë°ì´í„°ì—ëŠ” **ëª‡ ëª…**ì´ë¼ëŠ” ìˆ«ìê°€ ëª…í™•íˆ ì§€ì¼œì§€ì§€ ì•Šì€ ëª©ë¡ì´ë‚˜ ì„¤ëª…ì´ ë§ì„ ìˆ˜ ìˆìŒ\n",
    "          * ì´ëŸ¬í•œ í•™ìŠµ ë°ì´í„°ì˜ í¸í–¥ì´ ëª¨ë¸ì˜ ì‘ë‹µì— ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ë„ ìˆìŒ\n",
    "      * ì£¼ìš” ì›ì¸4. modelì˜ ìµœì í™” ëª©í‘œ\n",
    "          * temperatureê°€ ë‚®ì„ ë•Œ ëª¨ë¸ì€ ìì‹ ì´ í•™ìŠµí•œ ë°ì´í„°ì—ì„œ \"ê°€ì¥ ìì—°ìŠ¤ëŸ½ê³  ê·¸ëŸ´ë“¯í•œ\" ë‹µë³€ì„ ë‚´ë†“ìœ¼ë ¤ê³  í•¨ \n",
    "              * ë§Œì•½ í•™ìŠµ ë°ì´í„°ì— **ìœ ëª…ì¸ ë‚˜ì—´**ì´ë¼ëŠ” í”„ë¡¬í”„íŠ¸ì— ëŒ€í•´ 3ëª…ë³´ë‹¤ **5ëª…ì„ ë‚˜ì—´í•˜ëŠ” ê²½ìš°ê°€ ë” í”í•˜ê±°ë‚˜** ëª¨ë¸ì´ **5ëª…ì„ ë‚˜ì—´í•˜ëŠ” íŒ¨í„´ì„ ë” ê°•ë ¥í•˜ê²Œ í•™ìŠµ**í–ˆë‹¤ë©´, temperatureê°€ ë‚®ì„ìˆ˜ë¡ ì˜¤íˆë ¤ ê·¸ ê°•ë ¥í•œ íŒ¨í„´ì„ ë”°ë¥´ë ¤ í•  ê°€ëŠ¥ì„±ì´ ìˆìŒ\n",
    "              * 3ëª…ìœ¼ë¡œ ì œí•œí•˜ëŠ” ê²ƒë³´ë‹¤ 5ëª…ìœ¼ë¡œ ì œí•œí•˜ì§€ ì•Šê³  ë‚˜ì—´í•˜ëŠ” ê²ƒì´ ëª¨ë¸ ì…ì¥ì—ì„œëŠ” ë” \"ìì—°ìŠ¤ëŸ¬ìš´\" ë‹µë³€ì´ë¼ê³  íŒë‹¨í–ˆì„ ê²ƒ\n",
    "          * ë‚´ë¶€ì  ì„ í˜¸ë„\n",
    "              * ëª¨ë¸ ë‚´ë¶€ì ìœ¼ë¡œ íŠ¹ì • ìœ í˜•ì˜ ì •ë³´(**ì´ë¦„ ëª©ë¡**)ë¥¼ ìƒì„±í•  ë•Œ **ì¼ì • ê°œìˆ˜ ì´ìƒì„ ì„ í˜¸í•˜ëŠ” ê²½í–¥**ì´ ìˆì„ ìˆ˜ ìˆìŒ\n",
    "              * ë‚®ì€ ì˜¨ë„ëŠ” ì´ëŸ° ë‚´ë¶€ì ì¸ ì„ í˜¸ë„ë¥¼ ë” ê°•í•˜ê²Œ í‘œì¶œí•˜ê²Œ ë§Œë“¤ ìˆ˜ë„ ìˆìŒ\n",
    "\n",
    "    * **ëª¨ë¸ì˜ temperatureê°€ ë‚®ì•„ë„ í”„ë¡¬í”„íŠ¸ê°€ ëª¨í˜¸í•˜ë©´ ë‹¤ë¥¸ ë‚´ë¶€ì  ìš°ì„ ìˆœìœ„ë¥¼ ë”°ë¥´ê²Œ ë¨**\n",
    "    ---\n",
    "\n",
    "    * í•´ê²° ë°©ì•ˆ\n",
    "        * temperature ë” ë‚®ì¶”ê¸°? -> **X**\n",
    "            * temperatureê°€ ë‚®ë‹¤ê³  í•´ì„œ ì§€ì‹œë¥¼ ë¬´ì¡°ê±´ ë”°ë¥´ëŠ” ê²ƒì´ ì•„ë‹˜\n",
    "            * modelì˜ ë‚´ë¶€ì ì¸ í•™ìŠµ íŒ¨í„´ê³¼ í”„ë¡¬í”„íŠ¸ì˜ ëª…í™•ì„±ê³¼ í•¨ê»˜ ê³ ë ¤í•´ì•¼ í•¨\n",
    "        * **í”„ë¡¬í”„íŠ¸ ê°œì„  í•„ìš”**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c2c37e",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a8603f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # RunnableLambda_4 - Chain ìƒì„± + í”„ë¡¬í”„íŠ¸ ê°œì„ \n",
    "\n",
    "from datetime import datetime\n",
    "import re\n",
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough, RunnableParallel\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "\n",
    "# ë‚ ì§œ í•¨ìˆ˜\n",
    "def get_today(_): return datetime.today().strftime(\"%b-%d\")\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
    "prompt_lambda2 = PromptTemplate.from_template(\"\"\"\n",
    "ë‹¹ì‹ ì€ ë°ì´í„°ë¥¼ ì—„ê²©íˆ ë‹¤ë£¨ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n",
    "\n",
    "ê·œì¹™:\n",
    "- '{today}ê°€ ìƒì¼ì¸ ìœ ëª…ì¸ {n_val}ëª…:'ìœ¼ë¡œ ì‹œì‘\n",
    "- í•œ ë¬¸ì¥ì— í•œ ëª…: 1. ì´ë¦„ (ìƒë…„ì›”ì¼)\n",
    "- ì˜ˆì‹œë§Œ ë”°ë¥´ê¸°, ì„¤ëª…ãƒ»ê³µë°±ãƒ»ì¸ì‚¿ë§ ì—†ì´\n",
    "\n",
    "ì˜ˆì‹œ:\n",
    "{today}ê°€ ìƒì¼ì¸ ìœ ëª…ì¸ 3ëª…:\n",
    "1. ì•Œë² ë¥´íŠ¸ ì•„ì¸ìŠˆíƒ€ì¸ (1879ë…„ 3ì›” 14ì¼)\n",
    "2. ê¹€ì—°ì•„ (1990ë…„ 9ì›” 5ì¼)\n",
    "3. BTSì˜ ì •êµ­ (1997ë…„ 9ì›” 1ì¼)\n",
    "\n",
    "---\n",
    "\n",
    "{today}ê°€ ìƒì¼ì¸ ìœ ëª…ì¸ {n_val}ëª…ì„ ìœ„ì™€ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”.\n",
    "\"\"\".strip())\n",
    "\n",
    "# ëª¨ë¸ ì´ˆê¸°í™”\n",
    "model2 = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash-lite\",\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "# í›„ì²˜ë¦¬ í•¨ìˆ˜\n",
    "def post_process(data):\n",
    "    text = data[\"text\"]\n",
    "    n = data[\"n_val\"]\n",
    "    if isinstance(n, dict):\n",
    "        n = n.get(\"n_val\", 1)\n",
    "    try:\n",
    "        limit = int(n)\n",
    "    except:\n",
    "        print(f\"[ERROR] ì˜ëª»ëœ n_val ê°’: {n} ({type(n)})\")\n",
    "        return \"\"\n",
    "\n",
    "    lines = [\n",
    "        l.strip()\n",
    "        for l in text.strip().split(\"\\n\")\n",
    "        if re.match(r\"^\\d+\\.\\s.+\\(\\d{4}\", l.strip())\n",
    "    ]\n",
    "    return \"\\n\".join(lines[:limit])\n",
    "\n",
    "# ì²´ì¸ êµ¬ì„±\n",
    "chain_lambda2 = (\n",
    "    {\n",
    "        \"today\": RunnableLambda(get_today),\n",
    "        \"n_val\": RunnablePassthrough()\n",
    "    }\n",
    "    | RunnableParallel(\n",
    "        text=(prompt_lambda2 | model2 | StrOutputParser()),\n",
    "        n_val=itemgetter(\"n_val\")  # ğŸ”¥ í•µì‹¬ ìˆ˜ì •: ì¤‘ì²© dict ë°©ì§€\n",
    "    )\n",
    "    | RunnableLambda(post_process)\n",
    ")\n",
    "\n",
    "# ì‹¤í–‰\n",
    "num = 3\n",
    "print(\"âœ… ì‹¤í–‰ ì¤‘...\")\n",
    "result = chain_lambda2.invoke({\"n_val\": num})\n",
    "print(\"\\nğŸ¯ ìµœì¢… ê²°ê³¼:\\n\" + result)\n",
    "\n",
    "\n",
    "# ì²´ì¸ ê°ì²´ì˜ êµ¬ì„± ì¶œë ¥ (í˜„ì¬ êµì¬ì—ì„œ ë³´ì—¬ì£¼ëŠ” ë‹¨ê³„)\n",
    "print(\"\\n--- Chain êµ¬ì„± ì •ë³´ ---\")\n",
    "print(chain_lambda2)\n",
    "print(\"---\")\n",
    "\n",
    "# ì²´ì¸ ì‹¤í–‰ ë° ì‹¤ì œ ê²°ê³¼ ì¶œë ¥\n",
    "print(\"\\n--- ì²´ì¸ ì‹¤í–‰ ê²°ê³¼ ---\")\n",
    "try:\n",
    "    # \"n\"ì— í•´ë‹¹í•˜ëŠ” ê°’ì„ invoke() ë©”ì„œë“œì— ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ì „ë‹¬í•©ë‹ˆë‹¤.\n",
    "    # ì˜ˆ: 3ëª…ì˜ ìœ ëª…ì¸\n",
    "    result = chain_lambda2.invoke({\"n_val\": 3})\n",
    "    print(result)\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ì²´ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54707ae",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* \n",
    "    * ì…€ ì¶œë ¥\n",
    "        * âœ… ì‹¤í–‰ ì¤‘...\n",
    "\n",
    "        * ğŸ¯ ìµœì¢… ê²°ê³¼:\n",
    "        * 1. í—¨ë¦¬ í¬ë“œ (1863ë…„ 7ì›” 30ì¼)\n",
    "        * 2. ë‹ ì•”ìŠ¤íŠ¸ë¡± (1930ë…„ 8ì›” 5ì¼)\n",
    "        * 3. í†° í–‰í¬ìŠ¤ (1956ë…„ 7ì›” 9ì¼)\n",
    "\n",
    "        ---\n",
    "\n",
    "        * --- Chain êµ¬ì„± ì •ë³´ ---\n",
    "        * first=`{today: RunnableLambda(get_today), n_val: RunnablePassthrough()}`\n",
    "            * **chin_lambda2ì˜ êµ¬ì„± ìì²´**ë¥¼ ë³´ì—¬ì¤Œ\n",
    "        * middle=`[{text: PromptTemplate(input_variables=['n_val', 'today'], input_types={}, partial_variables={}, template=\"ë‹¹ì‹ ì€ ë°ì´í„°ë¥¼ ì—„ê²©íˆ ë‹¤ë£¨ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\\n\\nê·œì¹™:\\n- '{today}ê°€ ìƒì¼ì¸ ìœ ëª…ì¸ {n_val}ëª…:'ìœ¼ë¡œ ì‹œì‘\\n- í•œ ë¬¸ì¥ì— í•œ ëª…: 1. ì´ë¦„ (ìƒë…„ì›”ì¼)\\n- ì˜ˆì‹œë§Œ ë”°ë¥´ê¸°, ì„¤ëª…ãƒ»ê³µë°±ãƒ»ì¸ì‚¿ë§ ì—†ì´\\n\\nì˜ˆì‹œ:\\n{today}ê°€ ìƒì¼ì¸ ìœ ëª…ì¸ 3ëª…:\\n1. ì•Œë² ë¥´íŠ¸ ì•„ì¸ìŠˆíƒ€ì¸ (1879ë…„ 3ì›” 14ì¼)\\n2. ê¹€ì—°ì•„ (1990ë…„ 9ì›” 5ì¼)\\n3. BTSì˜ ì •êµ­ (1997ë…„ 9ì›” 1ì¼)\\n\\n---\\n\\n{today}ê°€ ìƒì¼ì¸ ìœ ëª…ì¸ {n_val}ëª…ì„ ìœ„ì™€ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”.\") | ChatGoogleGenerativeAI(model='models/gemini-2.5-flash-lite', google_api_key=SecretStr('**********'), temperature=0.1, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x1081a8eb0>, default_metadata=(), model_kwargs={})| StrOutputParser(),n_val: RunnableLambda(itemgetter('n_val'))}]`\n",
    "            * í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ê³¼ ì‹¤ì œ ì–¸ì–´ ëª¨ë¸(ChatGoogleGenerativeAI)ì´ ì–´ë–»ê²Œ ì—°ê²°ë˜ì–´ ìˆëŠ”ì§€ ë‚˜íƒ€ëƒ„\n",
    "        * last=`RunnableLambda(post_process)`\n",
    "            * ë§ˆì§€ë§‰ìœ¼ë¡œ StrOutputParser()ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì˜ ì‘ë‹µì„ ë¬¸ìì—´ë¡œ íŒŒì‹±í•˜ëŠ” ë‹¨ê³„ë¥¼ ë³´ì—¬ì¤Œ\n",
    "\n",
    "        ---\n",
    "\n",
    "        * --- ì²´ì¸ ì‹¤í–‰ ê²°ê³¼ ---\n",
    "        * 1. í—¨ë¦¬ í¬ë“œ (1863ë…„ 7ì›” 30ì¼)\n",
    "        * 2. ë‹ ì•”ìŠ¤íŠ¸ë¡± (1930ë…„ 8ì›” 5ì¼)\n",
    "        * 3. í†° í–‰í¬ìŠ¤ (1956ë…„ 7ì›” 9ì¼)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47e0030",
   "metadata": {},
   "source": [
    "#### (5) **`itemgetter`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a07e563",
   "metadata": {},
   "source": [
    "* **`operator.itemgetter`** \n",
    "  * ê°œë…\n",
    "    * íŒŒì´ì¬ì˜ í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ì¸ operator ëª¨ë“ˆì— ìˆëŠ” í•¨ìˆ˜\n",
    "    * **ë”•ì…”ë„ˆë¦¬**ë‚˜ **ë¦¬ìŠ¤íŠ¸** ê°™ì€ ê°ì²´ì—ì„œ **íŠ¹ì • í‚¤(key) ë˜ëŠ” ì¸ë±ìŠ¤(index)ì— í•´ë‹¹í•˜ëŠ” ê°’**ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì¶”ì¶œí•˜ê¸° ìœ„í•œ **í˜¸ì¶œ ê°€ëŠ¥í•œ(callable)** ê°ì²´ë¥¼ ë°˜í™˜\n",
    "\n",
    "  * Q. ì–¸ì œ ì“¸ê¹Œ?\n",
    "    * A1. **ë”•ì…”ë„ˆë¦¬**ë‚˜ **ë¦¬ìŠ¤íŠ¸**ì²˜ëŸ¼ ì—¬ëŸ¬ ê°’ì´ ëª¨ì—¬ ìˆëŠ” ë°ì´í„°ì—ì„œ **íŠ¹ì • í‚¤ë‚˜ ì¸ë±ìŠ¤ì— í•´ë‹¹í•˜ëŠ” ê°’** í•˜ë‚˜ ë˜ëŠ” ì—¬ëŸ¬ ê°œë¥¼ **íš¨ìœ¨ì ìœ¼ë¡œ êº¼ë‚´ê³  ì‹¶ì„ ë•Œ ì‚¬ìš©**\n",
    "        * ë”•ì…”ë„ˆë¦¬ ì˜ˆì‹œ: {'name': 'ì•¨ë¦¬ìŠ¤', 'age': 30}\n",
    "        * ë¦¬ìŠ¤íŠ¸ ì˜ˆì‹œ: ['ì‚¬ê³¼', 'ë°”ë‚˜ë‚˜', 'ì˜¤ë Œì§€']\n",
    "    * A2. íŠ¹íˆ **ë°ì´í„°ë¥¼ ê°€ê³µ**í•˜ê±°ë‚˜, **íŠ¹ì • í•„ë“œë§Œ** ë‹¤ìŒ ë‹¨ê³„ë¡œ ë„˜ê²¨ì•¼ í•  ë•Œ\n",
    "\n",
    "  * íŠ¹ì§•\n",
    "    * **ê°„ê²°í•˜ê³  ëª…í™•**\n",
    "        * `lambda x: x['key']` ê°™ì€ ë³µì¡í•˜ê²Œ ëŒë‹¤ í•¨ìˆ˜ ì‚¬ìš©í•  í•„ìš” X\n",
    "        * `itemgetter('key')` -> **ì§ê´€ì ìœ¼ë¡œ íŠ¹ì • ê°’ í˜¸ì¶œ ê°€ëŠ¥**\n",
    "    * **ì¬ì‚¬ìš©ì„± ë° íš¨ìœ¨ì„±**\n",
    "        *  **í•œ ë²ˆ ë§Œë“¤ì–´ì§„ itemgetter ê°ì²´ëŠ” ì—¬ëŸ¬ ë°ì´í„°ì— ë°˜ë³µì ìœ¼ë¡œ ì ìš© ê°€ëŠ¥**\n",
    "        *  **C ì–¸ì–´ë¡œ êµ¬í˜„** -> ëŒë‹¤ í•¨ìˆ˜ë³´ë‹¤ ì•½ê°„ ë” ë¹ ë¥¼ ìˆ˜ ìˆìŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a95ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# itemgetter_1\n",
    "\n",
    "####################################\n",
    "# 1. í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° í´ë˜ìŠ¤, ëª¨ë“ˆ ì„í¬íŠ¸\n",
    "####################################\n",
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ (ê¸°íƒ€ ë°˜ë³µë˜ëŠ” ì„í¬íŠ¸ ìƒëµ)\n",
    "from operator import itemgetter\n",
    "from langchain_core.prompts import ChatPromptTemplate               # ì±„íŒ… í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ìœ„í•œ í´ë˜ìŠ¤ ì„í¬íŠ¸\n",
    "\n",
    "####################################\n",
    "# 2. ì…ë ¥ ê°€ê³µ ë° ì¤€ë¹„\n",
    "####################################\n",
    "\n",
    "# ë¬¸ì¥ì˜ ê¸¸ì´ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜\n",
    "def length_function(text):\n",
    "    return len(text)\n",
    "\n",
    "\n",
    "# ë‘ ë¬¸ì¥ì˜ ê¸¸ì´ë¥¼ ê³±í•œ ê°’ì„ ë°˜í™˜í•˜ëŠ” ë‚´ë¶€(helper)í•¨ìˆ˜\n",
    "def _multiple_length_function(text1, text2):\n",
    "    return len(text1) * len(text2)\n",
    "\n",
    "\n",
    "# _multiple_length_function í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‘ ë¬¸ì¥ì˜ ê¸¸ì´ë¥¼ ê³±í•œ ê°’ì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜\n",
    "# ë”•ì…”ë„ˆë¦¬ í˜•íƒœì˜ ì…ë ¥ ê°’ì„ ì²˜ë¦¬í•˜ë„ë¡ ì„¤ê³„ë¨\n",
    "def multiple_length_function(_dict):\n",
    "    return _multiple_length_function(_dict[\"text1\"], _dict[\"text2\"])\n",
    "\n",
    "####################################\n",
    "# 3. LLM í˜¸ì¶œ\n",
    "####################################\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜: {a}ì™€ {b} ê°’ì„ ë°›ì•„ ì§ˆë¬¸ ìƒì„±\n",
    "prompt_itemgetter = ChatPromptTemplate.from_template(\"{a} + {b} ëŠ” ë¬´ì—‡ì¸ê°€ìš”?\")\n",
    "\n",
    "# ëª¨ë¸ ì´ˆê¸°í™” (gemini-1.5-flash)\n",
    "try:\n",
    "    model = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-1.5-flash\", # ë¹ ë¥¸ ì‘ë‹µì„ ìœ„í•œ Gemini 1.5 Flash ëª¨ë¸ ì‚¬ìš©\n",
    "        temperature=0.1, # ì¼ê´€ëœ ë‹µë³€ì„ ìœ„í•´ ë‚®ì€ ì˜¨ë„ ì„¤ì •\n",
    "    )\n",
    "    print(\"âœ… Google GenAI ëª¨ë¸ ì´ˆê¸°í™” ì„±ê³µ.\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Google GenAI ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}\")\n",
    "    print(\"  -> GOOGLE_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "    print(\"---\")\n",
    "    exit() # ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ í”„ë¡œê·¸ë¨ ì¢…ë£Œ\n",
    "\n",
    "# ê¸°ë³¸ì ì¸ chain êµ¬ì„± ì˜ˆì‹œ\n",
    "chain1 = prompt_itemgetter | model          # ì´ ì˜ˆì‹œì—ì„œ ì‚¬ìš©ë˜ì§€ëŠ” ì•Šì§€ë§Œ ì²´ì¸ êµ¬ì„±ì˜ ê¸°ë³¸ì ì¸ í‹€ì„ ë³´ì—¬ì¤Œ\n",
    "\n",
    "# main chain ìƒì„± : ì…ë ¥ ê°€ê³µ -> í”„ë¡¬í”„íŠ¸ ì±„ìš°ê¸° -> ëª¨ë¸ í˜¸ì¶œ\n",
    "\"\"\"\n",
    "ì´ ì²´ì¸ì€ ì…ë ¥ ê°’ì„ ê°€ê³µí•´ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì— ì „ë‹¬í•˜ì—¬ ì™„ì„±ëœ ì§ˆë¬¸ì„ ì–¸ì–´ ëª¨ë¸ë¡œ í˜¸ì¶œí•˜ì—¬ ë‹¤ì‹œ ì „ë‹¬í•œ í›„ ë‹µë³€ì„ ì–»ì–´ë‚´ëŠ” ê³¼ì •ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\n",
    "    1. ì…ë ¥ ê°€ê³µ : 'word1'ê³¼ 'word2' ë‘ ê°œì˜ ë¬¸ìì—´ ì…ë ¥ì„ ë°›ì•„ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
    "        - 'a' ê°’\n",
    "            - 'word1'ì˜ ê¸¸ì´\n",
    "            - ì…ë ¥ ë”•ì…”ë„ˆë¦¬ -> 'word1' ì¶”ì¶œ -> ì•ì„œ ì •ì˜í•œ def length_functionìœ¼ë¡œ ê¸¸ì´ ê³„ì‚°\n",
    "        -'b' ê°’\n",
    "            - 'word1' ê¸¸ì´ì™€ 'word2' ê¸¸ì´ë¥¼ ê³±í•œ ê°’\n",
    "            - íŒŒì´ì¬ ë‚´ì¥ ë¼ì´ë¸ŒëŸ¬ë¦¬ í•¨ìˆ˜ì¸ itemgetter ì‚¬ìš©\n",
    "            - ì…ë ¥ ë”•ì…”ë„ˆë¦¬ì—ì„œ 'word1'ê³¼ 'word2'ë¥¼ ì¶”ì¶œí•˜ì—¬ {'text1': word1, 'text2': word2} í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬ ìƒì„±\n",
    "            - ì•ì„œ ì •ì˜í•œ def multiple_length_functionìœ¼ë¡œ ë‘ ê¸¸ì˜ ê³± ê³„ì‚°\n",
    "\n",
    "    2. ê°€ê³µëœ {\"a\": ê°’, \"b\": ê°’} ë”•ì…”ë„ˆë¦¬ë¥¼ prompt í…œí”Œë¦¿ì— ì „ë‹¬\n",
    "        - {\"a\" : word1ì˜ ê¸¸ì´, \"b\": word1 ê¸¸ì´ * word2 ê¸¸ì´}\n",
    "        - dict í˜•íƒœ -> ì§ˆë¬¸ì´ ì™„ì„±ë˜ë©´ modelì—ê²Œ ì „ë‹¬\n",
    "        \n",
    "    3. model í˜¸ì¶œ -> ìµœì¢… ë‹µë³€ ë°›ê¸°\n",
    "\n",
    "ì¦‰, ì´ ì²´ì¸ì€ ë‘ ê°œì˜ ë‹¨ì–´ë¥¼ ì…ë ¥ë°›ì•„ ê·¸ ë‹¨ì–´ë“¤ì˜ ê¸¸ì´ë¥¼ ê³„ì‚°í•˜ê³ , ì´ë¥¼ ë³€í˜•í•˜ì—¬ ìƒˆë¡œìš´ ìˆ«ìë¡œ ë§Œë“  ë’¤, ê·¸ ìˆ«ìë“¤ì„ ì¡°í•©í•œ ì§ˆë¬¸ì„ LLMì—ê²Œ ë˜ì ¸ ë‹µì„ ì–»ëŠ” ë¡œì§ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\n",
    "chain1ì€ ë‹¨ìˆœíˆ í”„ë¡¬í”„íŠ¸ì™€ ëª¨ë¸ì„ ì—°ê²°í•œ ê¸°ë³¸ì ì¸ ì˜ˆì‹œì´ë©°, chain_itemgetterê°€ í•µì‹¬ ë¡œì§ì„ ë‹´ê³  ìˆìŠµë‹ˆë‹¤.\n",
    "\"\"\"\n",
    "chain_itemgetter = (\n",
    "    {\n",
    "        \"a\": itemgetter(\"word1\") | RunnableLambda(length_function),         # 'a': ì…ë ¥ ë”•ì…”ë„ˆë¦¬ 'word1' ì¶”ì¶œ -> length_functionìœ¼ë¡œ ê¸¸ì´ ê³„ì‚°\n",
    "        \"b\": {\"text1\": itemgetter(\"word1\"), \"text2\": itemgetter(\"word2\")}   # 'b': ì…ë ¥ ë”•ì…”ë„ˆë¦¬ 'word1'ê³¼ 'word2'ë¥¼ ì¶”ì¶œ -> {'text1': word1, 'text2': word2} ë”•ì…”ë„ˆë¦¬ ìƒì„±   \n",
    "        | RunnableLambda(multiple_length_function),                         # ìœ„ ë”•ì…”ë„ˆë¦¬ë¥¼ multiple_length_functionì— ì „ë‹¬í•˜ì—¬ ë‘ ê¸¸ì´ì˜ ê³± ê³„ì‚°\n",
    "    }\n",
    "    | prompt_itemgetter                     # ê°€ê³µëœ {\"a\": ê°’, \"b\": ê°’} ë”•ì…”ë„ˆë¦¬ë¥¼ prompt í…œí”Œë¦¿ì— ì „ë‹¬\n",
    "    | model                                 # ì™„ì„±ëœ ì§ˆë¬¸ì„ ì–¸ì–´ ëª¨ë¸ì— ì „ë‹¬í•˜ì—¬ ë‹µë³€ ì–»ê¸°\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52b78ce",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* \n",
    "    * ì…€ ì¶œë ¥\n",
    "        * âœ… Google GenAI ëª¨ë¸ ì´ˆê¸°í™” ì„±ê³µ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "426af0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ì²´ì¸ ì‹¤í–‰ ì‹œì‘ ---\n",
      "ê³¼ì •: content='5 + 25 = 30 ì…ë‹ˆë‹¤.' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []} id='run--d1e1f6aa-cac4-4484-9766-3b1adc6dae55-0' usage_metadata={'input_tokens': 12, 'output_tokens': 12, 'total_tokens': 24, 'input_token_details': {'cache_read': 0}}\n",
      "ìµœì¢… ë‹µë³€: 5 + 25 = 30 ì…ë‹ˆë‹¤.\n",
      "--- ì²´ì¸ ì‹¤í–‰ ì¢…ë£Œ ---\n"
     ]
    }
   ],
   "source": [
    "# chain_itemgetter ì‹¤í–‰\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n--- ì²´ì¸ ì‹¤í–‰ ì‹œì‘ ---\")\n",
    "    try:\n",
    "        result = chain_itemgetter.invoke({\"word1\": \"hello\", \"word2\": \"world\"})\n",
    "        print(f\"ê³¼ì •: {result}\")\n",
    "        print(f\"ìµœì¢… ë‹µë³€: {result.content}\")\n",
    "    except NameError:\n",
    "        print(\"âŒ 'chain_itemgetter'ê°€ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì´ì „ ì…€ì„ ë¨¼ì € ì‹¤í–‰í•´ ì£¼ì„¸ìš”.\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì²´ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "    print(\"--- ì²´ì¸ ì‹¤í–‰ ì¢…ë£Œ ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b02993",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* \n",
    "    * ì…€ ì¶œë ¥\n",
    "        * --- ì²´ì¸ ì‹¤í–‰ ì‹œì‘ ---\n",
    "        * ê³¼ì •: `content='5 + 25 = 30 ì…ë‹ˆë‹¤.' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []} id='run--d1e1f6aa-cac4-4484-9766-3b1adc6dae55-0' usage_metadata={'input_tokens': 12, 'output_tokens': 12, 'total_tokens': 24, 'input_token_details': {'cache_read': 0}}`\n",
    "        * ìµœì¢… ë‹µë³€: 5 + 25 = 30 ì…ë‹ˆë‹¤.\n",
    "        * --- ì²´ì¸ ì‹¤í–‰ ì¢…ë£Œ ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5e8c45a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='5 + 25 = 30 ì…ë‹ˆë‹¤.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, id='run--7b4398e3-3cc6-4d83-9a1d-a95ecf1f8707-0', usage_metadata={'input_tokens': 12, 'output_tokens': 12, 'total_tokens': 24, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# êµì¬ì™€ ë™ì¼í•˜ê²Œ ì¶œë ¥í•´ë³´ê¸°\n",
    "\n",
    "chain_itemgetter.invoke({\"word1\": \"hello\", \"word2\": \"world\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7d09ae",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* \n",
    "    * AIMessage ê°ì²´ ì¶œë ¥ì˜ ì°¨ì´ì \n",
    "\n",
    "    | íŠ¹ì„± / ì¶œë ¥ ë°©ì‹    | `chain.invoke(...)` ì§ì ‘ ì¶œë ¥ (`__repr__` í˜¸ì¶œ) | `print(result)` (`__str__` í˜¸ì¶œ)                  |\n",
    "    | :------------------ | :--------------------------------------------- | :------------------------------------------------ |\n",
    "    | **ëª©ì ** | ê°œë°œì/ë””ë²„ê¹…ìš©                                  | ì‚¬ìš©ì ì¹œí™”ì /ì½”ë“œ ë‚´ ì¶œë ¥ìš©                      |\n",
    "    | **í‘œí˜„ ë‚´ìš©** | ê°ì²´ì˜ ëª¨ë“  ì†ì„± (Content, ë©”íƒ€ë°ì´í„°, ID ë“± ìƒì„¸ ì •ë³´) | ê°ì²´ì˜ í•µì‹¬ ë‚´ìš© (ì£¼ë¡œ `content`ë§Œ)              |\n",
    "    | **í˜•ì‹** | `AIMessage(content='...', response_metadata={...}, id='...', ...)` | `5 + 25 = 30 ì…ë‹ˆë‹¤.` (ê°„ê²°í•œ í…ìŠ¤íŠ¸)             |\n",
    "    | **ê°€ë…ì„±** | ìƒì„¸í•˜ì§€ë§Œ ë‹¤ì†Œ ë³µì¡                               | ê°„ê²°í•˜ê³  ì½ê¸° ì‰¬ì›€                                |\n",
    "    | **ì£¼ìš” ì‚¬ìš©ì²˜** | ê°ì²´ì˜ ìƒíƒœ/ì •ë³´ ì „ì²´ë¥¼ í™•ì¸í•´ì•¼ í•  ë•Œ             | ìµœì¢… ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ê±°ë‚˜ ë¡œê·¸ë¡œ ë‚¨ê¸¸ ë•Œ             |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
