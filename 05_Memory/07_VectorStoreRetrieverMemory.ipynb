{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8af94d6a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc91be71",
   "metadata": {},
   "source": [
    "* 출처: LangChain 공식 문서 또는 해당 교재명\n",
    "* 원본 URL: https://smith.langchain.com/hub/teddynote/summary-stuff-documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e325a8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60044868",
   "metadata": {},
   "source": [
    "## **`VectorStoreRetrieverMemory`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fc3081",
   "metadata": {},
   "source": [
    "* 대화 내용을 `벡터(숫자 배열)로 변환`하여 저장하고, **필요할 때 `검색`해서 불러오는 메모리**\n",
    "  \n",
    "* 즉, `벡터 스토어`에 메모리 저장 → 호출될 때마다 가장 **`눈에 띄는 상위 K개의 문서`를 쿼리** \n",
    "\n",
    "<br>\n",
    "\n",
    "* **다른 메모리 클래스와의 `차이점`**: 대화 내용의 **`순서를 명시적으로 추적하지 않는다는 점`** \n",
    "\n",
    "<br>\n",
    "\n",
    "* **역할**: \n",
    "\n",
    "    * `매우 방대한 양의 정보를 저장`하고, 사용자가 질문하는 내용과 **`가장 유사한 정보를 찾아냄`**\n",
    "\n",
    "    * **`≒ 원하는 정보를 검색창에 입력하면 수많은 자료 중에서 찾아주는 도서관 검색 시스템`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81ca8725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 환경변수 처리 및 클라이언트 생성\n",
    "from langsmith import Client\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "# 클라이언트 생성 \n",
    "api_key = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "client = Client(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5087a040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangSmith 추적 설정하기 (https:smith.langchin.com)\n",
    "# LangSmith 추적을 위한 라이브러리 임포트\n",
    "from langsmith import traceable                                                             # @traceable 데코레이터 사용 시\n",
    "\n",
    "# LangSmith 환경 변수 확인\n",
    "\n",
    "print(\"\\n--- LangSmith 환경 변수 확인 ---\")\n",
    "langchain_tracing_v2 = os.getenv('LANGCHAIN_TRACING_V2')\n",
    "langchain_project = os.getenv('LANGCHAIN_PROJECT')\n",
    "langchain_api_key_status = \"설정됨\" if os.getenv('LANGCHAIN_API_KEY') else \"설정되지 않음\"      # API 키 값은 직접 출력하지 않음\n",
    "org = \"설정됨\" if os.getenv('LANGCHAIN_ORGANIZATION') else \"설정되지 않음\"                      # 직접 출력하지 않음\n",
    "\n",
    "if langchain_tracing_v2 == \"true\" and os.getenv('LANGCHAIN_API_KEY') and langchain_project:\n",
    "    print(f\"✅ LangSmith 추적 활성화됨 (LANGCHAIN_TRACING_V2='{langchain_tracing_v2}')\")\n",
    "    print(f\"✅ LangSmith 프로젝트: '{langchain_project}'\")\n",
    "    print(f\"✅ LangSmith API Key: {langchain_api_key_status}\")\n",
    "    print(\"  -> 이제 LangSmith 대시보드에서 이 프로젝트를 확인해 보세요.\")\n",
    "else:\n",
    "    print(\"❌ LangSmith 추적이 완전히 활성화되지 않았습니다. 다음을 확인하세요:\")\n",
    "    if langchain_tracing_v2 != \"true\":\n",
    "        print(f\"  - LANGCHAIN_TRACING_V2가 'true'로 설정되어 있지 않습니다 (현재: '{langchain_tracing_v2}').\")\n",
    "    if not os.getenv('LANGCHAIN_API_KEY'):\n",
    "        print(\"  - LANGCHAIN_API_KEY가 설정되어 있지 않습니다.\")\n",
    "    if not langchain_project:\n",
    "        print(\"  - LANGCHAIN_PROJECT가 설정되어 있지 않습니다.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a80de88",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 셀 출력\n",
    "\n",
    "    ```markdown\n",
    "    --- LangSmith 환경 변수 확인 ---\n",
    "    ✅ LangSmith 추적 활성화됨 (LANGCHAIN_TRACING_V2='true')\n",
    "    ✅ LangSmith 프로젝트: 'LangChain-prantice'\n",
    "    ✅ LangSmith API Key: 설정됨\n",
    "    -> 이제 LangSmith 대시보드에서 이 프로젝트를 확인해 보세요.\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b7596d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "\n",
    "# .env 파일에서 환경변수 불러오기\n",
    "load_dotenv()\n",
    "\n",
    "# 환경변수에서 API 키 가져오기\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# OpenAI API 키 설정\n",
    "openai.api_key = api_key\n",
    "\n",
    "# OpenAI를 불러오기\n",
    "# ✅ 디버깅 함수: API 키가 잘 불러와졌는지 확인\n",
    "def debug_api_key():\n",
    "    if api_key is None:\n",
    "        print(\"❌ API 키를 불러오지 못했습니다. .env 파일과 변수명을 확인하세요.\")\n",
    "    elif api_key.startswith(\"sk-\") and len(api_key) > 20:\n",
    "        print(\"✅ API 키를 성공적으로 불러왔습니다.\")\n",
    "    else:\n",
    "        print(\"⚠️ API 키 형식이 올바르지 않은 것 같습니다. 값을 확인하세요.\")\n",
    "\n",
    "# 디버깅 함수 실행\n",
    "debug_api_key()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129b5eb3",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 셀 출력\n",
    "\n",
    "    ```markdown\n",
    "    ✅ API 키를 성공적으로 불러왔습니다.\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d224852",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# .env 파일에서 환경변수 불러오기\n",
    "load_dotenv()\n",
    "\n",
    "# 환경변수에서 API 키 가져오기\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# OpenAI API 키 설정\n",
    "openai.api_key = api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58747f06",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd58150",
   "metadata": {},
   "source": [
    "* 벡터 스토어 초기화하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfab2a0",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 사전 터미널에 설치 필요한 벡터스토어 관련 내용\n",
    "\n",
    "  * CPU ver.\n",
    "\n",
    "  ```bash\n",
    "\n",
    "    pip install faiss-cpu\n",
    "  ```\n",
    "\n",
    "<br>\n",
    "\n",
    "* \n",
    "  * GPU ver.\n",
    "  ```bash\n",
    "\n",
    "    pip install faiss-gpu\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb9c382",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss                                                # Meta AI Research에서 개발한 라이브러리\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "\n",
    "# 임베딩 모델 정의하기\n",
    "embeddings_model = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",                         # 실제 임베딩 모델명 넣기\n",
    "    api_key=api_key                                         # 임베딩 모델용 api_key를 환경변수 처리 후 넣기\n",
    ")\n",
    "\n",
    "# Vector Store 초기화하기\n",
    "embedding_size = 1536\n",
    "index = faiss.IndexFlatL2(embedding_size)\n",
    "vectorstore = FAISS(                                        # FAISS 형태 최신 코드 형태로 바꿔서 써야 오류 생기지 않음\n",
    "    embedding_function=embeddings_model, \n",
    "    index=index, \n",
    "    docstore=InMemoryDocstore({}), \n",
    "    index_to_docstore_id={})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1788b1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1623846",
   "metadata": {},
   "source": [
    "* **`K = 1`**\n",
    "\n",
    "  * 실제 사용에서는 `k`를 더 높은 값으로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c22116a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import VectorStoreRetrieverMemory\n",
    "\n",
    "# 벡터 조회가 여전히 의미적으로 관련성 있는 정보를 반환한다는 것을 보여주기 위해서 설정함\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})\n",
    "memory = VectorStoreRetrieverMemory(retriever=retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf298b3",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 셀 출력\n",
    "\n",
    "    ```python\n",
    "    /var/folders/h3/l7wnkv352kqftv0t8ctl2ld40000gn/T/ipykernel_15381/1689916482.py:5: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
    "    memory = VectorStoreRetrieverMemory(retriever=retriever)\n",
    "    ```\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "* 셀 출력 해석\n",
    "\n",
    "  * 경고 메시지\n",
    "    ```markdown\n",
    "\n",
    "        LangChainDeprecationWarning: Please see the migration guide...\n",
    "    \n",
    "    ```\n",
    "    * `DeprecationWarning` = **`이 기능은 곧 없어질 예정이니, 새 방법을 쓰세요`** 라는 의미\n",
    "  \n",
    "    * `VectorStoreRetrieverMemory`라는 기능이 `앞으로 사라질 예정`이라, 지금은 쓸 수 있지만 `미래 버전에서는 없어질 수 있다`는 의미\n",
    "  \n",
    "    * `LangChain`의 새로운 [가이드 사이드](https://python.langchain.com/docs/versions/migrating_memory/) 안내\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "  * 해당 기능이 사라지는 이유\n",
    "\n",
    "    * `LangChain`의 버전이 올라가면서 **`메모리 시스템 구조를 새롭게 통합 중`**\n",
    "\n",
    "    * 이전 방식(`VectorStoreRetrieverMemory`): `유지•관리 어려움` + `새로운 기능들과의 호환성이 떨어짐`\n",
    "\n",
    "    * **`더 유연하고 표준화된 메모리 구조로 교체하는 중`**\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "  * 최신 코드에서 쓰는 방법\n",
    "    * 권장_1: `RunnableWithMessageHistory` + `VectorStoreRetriever` 조합\n",
    "    * 권장_1의 예시\n",
    "    ```python\n",
    "    # 필요한 라이브러리 임포트\n",
    "      from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "      from langchain_community.vectorstores import FAISS\n",
    "      from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "      from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "      from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "      from langchain_core.chat_history import BaseChatMessageHistory\n",
    "\n",
    "      # 1. 벡터스토어 + 리트리버 준비\n",
    "      embeddings = OpenAIEmbeddings()\n",
    "      # 예시 문서\n",
    "      docs = [\n",
    "          {\"page_content\": \"Python is a programming language.\"},\n",
    "          {\"page_content\": \"LangChain helps build LLM-powered applications.\"},\n",
    "          {\"page_content\": \"FAISS is a vector database for similarity search.\"}\n",
    "      ]\n",
    "      vectorstore = FAISS.from_documents(docs, embedding=embeddings)\n",
    "      retriever = vectorstore.as_retriever()\n",
    "\n",
    "      # 2. 프롬프트 템플릿 (대화 기록 포함)\n",
    "      prompt = ChatPromptTemplate.from_messages([\n",
    "          (\"system\", \"당신은 친절한 AI 어시스턴트입니다. 질문에 답하세요.\"),\n",
    "          MessagesPlaceholder(variable_name=\"history\"),  # 대화 기록 자리\n",
    "          (\"human\", \"{input}\"),\n",
    "      ])\n",
    "\n",
    "      # 3. LLM 모델\n",
    "      llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "      # 4. 리트리버와 LLM을 연결하는 Runnable\n",
    "      #    (검색 결과를 context로 넣어주는 간단한 체인)\n",
    "      def retrieval_chain(inputs):\n",
    "          query = inputs[\"input\"]\n",
    "          docs = retriever.get_relevant_documents(query)\n",
    "          context = \"\\n\".join([d.page_content for d in docs])\n",
    "          return prompt.format_messages(history=inputs[\"history\"], input=f\"{query}\\n\\n참고자료:\\n{context}\")\n",
    "\n",
    "      # 5. 세션별 대화 기록 저장소\n",
    "      store = {}\n",
    "      def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "          if session_id not in store:\n",
    "              store[session_id] = ChatMessageHistory()\n",
    "          return store[session_id]\n",
    "\n",
    "      # 6. RunnableWithMessageHistory로 감싸기\n",
    "      with_history = RunnableWithMessageHistory(\n",
    "          runnable = retrieval_chain | llm,\n",
    "          get_session_history = get_session_history,\n",
    "          input_messages_key = \"input\",       # 사용자 입력 키\n",
    "          history_messages_key = \"history\"    # 대화 기록 키\n",
    "      )\n",
    "\n",
    "      # 7. 실행 예시\n",
    "      session_id = \"user-123\"\n",
    "\n",
    "      response1 = with_history.invoke(\n",
    "          {\"input\": \"LangChain이 뭐야?\"},\n",
    "          config={\"configurable\": {\"session_id\": session_id}}\n",
    "      )\n",
    "      print(response1)\n",
    "\n",
    "      response2 = with_history.invoke(\n",
    "          {\"input\": \"FAISS에 대해서도 알려줘\"},\n",
    "          config={\"configurable\": {\"session_id\": session_id}}\n",
    "      )\n",
    "      print(response2)\n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "* \n",
    "    * 권장_2: `ConversationBufferMemory` 또는 `ConversationBufferWindowMemory` 같은 새 메모리 클래스를 쓰는 걸 권장\n",
    "    * 권장_2의 예시\n",
    "    ```python\n",
    "      from langchain.memory import ConversationBufferMemory\n",
    "      from langchain.chains import ConversationChain\n",
    "      from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "      memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "      llm = ChatOpenAI()\n",
    "      conversation = ConversationChain(llm=llm, memory=memory)\n",
    "\n",
    "      conversation.predict(input=\"안녕!\")\n",
    "    ```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecec572e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임의의 대화 저장해보기\n",
    "\n",
    "memory.save_context(\n",
    "    inputs={\n",
    "        \"human\": \"안녕하세요, 오늘 면접에 참석해주셔서 감사합니다. 자기소개 부탁드립니다.\"\n",
    "    },\n",
    "    outputs={\n",
    "        \"ai\": \"안녕하세요. 저는 컴퓨터 과학을 전공한 신입 개발자입니다. 대학에서는 주로 자바와 파이썬을 사용했으며, 최근에는 웹 개발 프로젝트에 참여하여 실제 사용자를 위한 서비스를 개발하는 경험을 했습니다.\"\n",
    "    },\n",
    ")\n",
    "memory.save_context(\n",
    "    inputs={\"human\": \"프로젝트에서 어떤 역할을 맡았나요?\"},\n",
    "    outputs={\n",
    "        \"ai\": \"제가 맡은 역할은 백엔드 개발자였습니다. 사용자 데이터 처리와 서버 로직 개발을 담당했으며, RESTful API를 구현하여 프론트엔드와의 통신을 담당했습니다. 또한, 데이터베이스 설계에도 참여했습니다.\"\n",
    "    },\n",
    ")\n",
    "memory.save_context(\n",
    "    inputs={\n",
    "        \"human\": \"팀 프로젝트에서 어려움을 겪었던 경험이 있다면 어떻게 해결했나요?\"\n",
    "    },\n",
    "    outputs={\n",
    "        \"ai\": \"프로젝트 초기에 의사소통 문제로 몇 가지 어려움이 있었습니다. 이를 해결하기 위해 저희 팀은 정기적인 미팅을 갖고 각자의 진행 상황을 공유했습니다. 또한, 문제가 발생했을 때는 적극적으로 의견을 나누고, 합리적인 해결책을 찾기 위해 노력했습니다.\"\n",
    "    },\n",
    ")\n",
    "memory.save_context(\n",
    "    inputs={\"human\": \"개발자로서 자신의 강점은 무엇이라고 생각하나요?\"},\n",
    "    outputs={\n",
    "        \"ai\": \"제 강점은 빠른 학습 능력과 문제 해결 능력입니다. 새로운 기술이나 도구를 빠르게 습득할 수 있으며, 복잡한 문제에 직면했을 때 창의적인 해결책을 제시할 수 있습니다. 또한, 팀워크를 중시하며 동료들과 협력하는 것을 중요하게 생각합니다.\"\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6019409d",
   "metadata": {},
   "source": [
    "* 다음의 질문을 했을 때 Vector Store 로 부터 1개(k=1 이기 때문)의 가장 관련성 높은 대화를 반환\n",
    "\n",
    "    ```markdown\n",
    "    질문: **\"면접자 전공은 무엇인가요?\"**\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a0c9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리에 질문을 통해 가장 연관성 높은 1개 대화를 추출\n",
    "\n",
    "print(memory.load_memory_variables({\"prompt\": \"면접자 전공은 무엇인가요?\"})[\"history\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bbe48f",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 셀 출력\n",
    "\n",
    "    ```markdown\n",
    "    human: 안녕하세요, 오늘 면접에 참석해주셔서 감사합니다. 자기소개 부탁드립니다.\n",
    "    ai: 안녕하세요. 저는 컴퓨터 과학을 전공한 신입 개발자입니다. 대학에서는 주로 자바와 파이썬을 사용했으며, 최근에는 웹 개발 프로젝트에 참여하여 실제 사용자를 위한 서비스를 개발하는 경험을 했습니다.\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a457a85",
   "metadata": {},
   "source": [
    "* 이번에는 다른 질문을 통해 가장 연관성 높은 1개 대화를 추출합니다.\n",
    "\n",
    "    ```markdown\n",
    "    질문: **\"면접자가 프로젝트에서 맡은 역할은 무엇인가요?\"**\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef002c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    memory.load_memory_variables(\n",
    "        {\"human\": \"면접자가 프로젝트에서 맡은 역할은 무엇인가요?\"}\n",
    "    )[\"history\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc24202",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 셀 출력\n",
    "\n",
    "    ```markdown\n",
    "    human: 프로젝트에서 어떤 역할을 맡았나요?\n",
    "    ai: 제가 맡은 역할은 백엔드 개발자였습니다. 사용자 데이터 처리와 서버 로직 개발을 담당했으며, RESTful API를 구현하여 프론트엔드와의 통신을 담당했습니다. 또한, 데이터베이스 설계에도 참여했습니다.\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b463071",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7475faca",
   "metadata": {},
   "source": [
    "### **`업데이트된 방식`** 시도"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c0d2f7",
   "metadata": {},
   "source": [
    "* 교재 코드 오류 발생\n",
    "  \n",
    "  * 원인_1: 임베딩 모델 제대로 생성 X\n",
    "\n",
    "    * `OpenAIEmbeddings()` 초기화 시 **`필수 인자 누락`**\n",
    "      * `langchain_openai.OpenAIEmbeddings`는 기본값 없음 → `model`과 `API 키`를 지정해야 함\n",
    "      * `gpt-4o-mini` = 텍스트 생성 모델 → 벡터스토어에 쓸 임베딩은 **`별도의 임베딩 전용 모델 지정 필요`**\n",
    "        * `GPT-4o-mini`는 그대로 대화/생성에 쓰고, `검색용 벡터`는 `임베딩 모델로 생성하는 구조`가 일반적\n",
    "        * 임베딩 모델 예시: **`text-embedding-3-small`** 또는 **`text-embedding-3-large`**\n",
    "\n",
    "  * 원인_2: **`FAISS` 최신 초기화 방식 변경**\n",
    "    * 예전처럼 `FAISS(embeddings_model, index, ...)`로 `직접 생성`하는 방식은 **`deprecated`**\n",
    "    * **`최신`** 버전에서는 `FAISS.from_texts()` 또는 `FAISS()에 embedding_function=` 키워드로 넘겨야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8011c8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d863a288",
   "metadata": {},
   "source": [
    "* 최신 방식으로 수정한 코드로 도전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b855af0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .env 파일에서 API 키 불러오기\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "def safe_chat_request(prompt, model=\"gpt-4o-mini\", max_retries=5):\n",
    "    \"\"\"429 에러 방지를 위한 안전 호출 함수\"\"\"\n",
    "    retries = 0\n",
    "    wait_time = 1  # 첫 대기 시간(초)\n",
    "\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.7\n",
    "            )\n",
    "            return response.choices[0].message[\"content\"]\n",
    "\n",
    "        except openai.error.RateLimitError:\n",
    "            print(f\"[경고] 요청이 너무 많습니다. {wait_time}초 후 재시도...\")\n",
    "            time.sleep(wait_time)\n",
    "            wait_time *= 2  # 지수 백오프\n",
    "            retries += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[에러] {e}\")\n",
    "            break\n",
    "\n",
    "    return \"요청 실패: 재시도 횟수를 초과했습니다.\"\n",
    "\n",
    "# 사용 예시\n",
    "if __name__ == \"__main__\":\n",
    "    answer = safe_chat_request(\"안녕하세요, 오늘 날씨에 맞는 점심 메뉴 추천해줘\")\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fb2e5f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요! 오늘 날씨가 어떤지에 따라 추천해드릴 수 있는 점심 메뉴가 달라질 수 있어요. 날씨가 맑고 따뜻하다면 샐러드나 가벼운 파스타가 좋고, 비나 추운 날씨라면 따뜻한 국물 요리나 찌개가 좋습니다. 오늘의 날씨는 어떤가요?\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from openai import OpenAI\n",
    "#from openai.error import RateLimitError  # 예외 경로 변경됨\n",
    "from openai import RateLimitError\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "def safe_chat_request(prompt, model=\"gpt-4o-mini\", max_retries=5):\n",
    "    retries = 0\n",
    "    wait_time = 1\n",
    "\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.7\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "\n",
    "        except RateLimitError:\n",
    "            print(f\"[경고] 요청이 너무 많습니다. {wait_time}초 후 재시도...\")\n",
    "            time.sleep(wait_time)\n",
    "            wait_time *= 2\n",
    "            retries += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[에러] {e}\")\n",
    "            break\n",
    "\n",
    "    return \"요청 실패: 재시도 횟수를 초과했습니다.\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    answer = safe_chat_request(\"안녕하세요, 오늘 날씨에 맞는 점심 메뉴 추천해줘\")\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4df2c9d",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 셀 출력_1 (2.2s)\n",
    "\n",
    "    ```markdown\n",
    "    안녕하세요! 오늘 날씨에 따라 점심 메뉴를 추천해드릴게요. 날씨가 맑고 따뜻하다면 신선한 샐러드나 냉면이 좋고, 비가 오거나 쌀쌀하다면 따뜻한 국물 요리인 김치찌개나 우동이 좋을 것 같아요. 날씨가 어떤지 좀 더 구체적으로 말씀해주시면 더 좋은 추천을 해드릴 수 있을 것 같아요!\n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "* 셀 출력_2 (2.1s)\n",
    "\n",
    "  * 모듈 임포트 중 과거 버전 → 최신 방식으로 변경 추가 후 다시 실행\n",
    "    * 과거 임포트 방법 → 오류 발생 → `셀 출력_1`에서 제외하고 실행\n",
    "\n",
    "    ```python\n",
    "        from openai.error import RateLimitError \n",
    "    ```\n",
    "\n",
    "    * 최신 임포트 방법 → 추가 → `셀 출력_2`에서 추가 및 실행\n",
    "\n",
    "    ```python\n",
    "        from openai import RateLimitError\n",
    "    ```\n",
    "\n",
    "    ```markdown\n",
    "    안녕하세요! 오늘 날씨가 어떤지에 따라 추천해드릴 수 있는 점심 메뉴가 달라질 수 있어요. 날씨가 맑고 따뜻하다면 샐러드나 가벼운 파스타가 좋고, 비나 추운 날씨라면 따뜻한 국물 요리나 찌개가 좋습니다. 오늘의 날씨는 어떤가요?\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025012aa",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4d2353",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "📌 실행 구조\n",
    "\n",
    "*  문서 임베딩 생성 → `OpenAI Embeddings API` 사용\n",
    "*  벡터스토어에 저장 → 예: `FAISS`, `Chroma`, `Weaviate` 등\n",
    "*  질의 시 → `질문`을 `임베딩`으로 `변환` → `벡터스토어`에서 `유사도 검색`\n",
    "*  검색 결과 + 사용자 질문을 GPT 모델에 전달 → `최종 답변 생성`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51353e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from openai import OpenAI, RateLimitError\n",
    "from dotenv import load_dotenv\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# 1. 안전 호출 함수 (429 방지)\n",
    "def safe_chat_request(messages, model=\"gpt-4o-mini\", max_retries=5):\n",
    "    retries, wait_time = 0, 1\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            resp = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                temperature=0.7\n",
    "            )\n",
    "            return resp.choices[0].message.content\n",
    "        except RateLimitError:\n",
    "            print(f\"[경고] 요청이 많습니다. {wait_time}초 후 재시도...\")\n",
    "            time.sleep(wait_time)\n",
    "            wait_time *= 2\n",
    "            retries += 1\n",
    "        except Exception as e:\n",
    "            print(f\"[에러] {e}\")\n",
    "            break\n",
    "    return None\n",
    "\n",
    "# 2. 텍스트 → 벡터 변환\n",
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "    emb = client.embeddings.create(\n",
    "        model=model,\n",
    "        input=text\n",
    "    )\n",
    "    return np.array(emb.data[0].embedding, dtype=\"float32\")\n",
    "\n",
    "# 3. 샘플 문서 벡터스토어 구축\n",
    "docs = [\n",
    "    \"파이썬은 데이터 분석과 AI 개발에 널리 사용되는 언어입니다.\",\n",
    "    \"FAISS는 Facebook AI Research에서 만든 벡터 검색 라이브러리입니다.\",\n",
    "    \"벡터스토어는 문서 검색과 추천 시스템에 활용됩니다.\"\n",
    "]\n",
    "\n",
    "# FAISS 초기화\n",
    "dimension = len(get_embedding(\"test\"))\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "# 문서 임베딩 저장\n",
    "doc_embeddings = [get_embedding(doc) for doc in docs]\n",
    "index.add(np.array(doc_embeddings))\n",
    "\n",
    "# 4. 검색 + GPT 연결\n",
    "def search_and_answer(query):\n",
    "    query_vec = get_embedding(query)\n",
    "    D, I = index.search(np.array([query_vec]), k=2)  # 상위 2개 검색\n",
    "    retrieved_docs = [docs[i] for i in I[0]]\n",
    "\n",
    "    # GPT에 전달할 메시지 구성\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"당신은 검색 기반 질문에 답하는 AI입니다.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"다음 문서들을 참고해서 답변해 주세요:\\n{retrieved_docs}\\n\\n질문: {query}\"}\n",
    "    ]\n",
    "    return safe_chat_request(messages)\n",
    "\n",
    "# 실행 예시\n",
    "if __name__ == \"__main__\":\n",
    "    answer = search_and_answer(\"벡터스토어가 뭐야?\")\n",
    "    print(\"답변:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2994d6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import faiss\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.memory import VectorStoreRetrieverMemory\n",
    "\n",
    "# =========================================\n",
    "# 1. OpenAI API 키 설정\n",
    "# =========================================\n",
    "# 환경변수에 API 키 저장\n",
    "# .env 파일에서 환경변수 불러오기\n",
    "load_dotenv()\n",
    "\n",
    "# 환경변수에서 API 키 가져오기\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# OpenAI API 키 설정\n",
    "openai.api_key = api_key\n",
    "\n",
    "# =========================================\n",
    "# 2. 임베딩 모델 정의\n",
    "# =========================================\n",
    "# gpt-4o-mini는 텍스트 생성용, 벡터 검색에는 임베딩 전용 모델 사용\n",
    "embeddings_model = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",                 # 또는 \"text-embedding-3-large\"\n",
    "    api_key=api_key\n",
    ")\n",
    "\n",
    "# =========================================\n",
    "# 3. 빈 FAISS 인덱스 초기화\n",
    "# =========================================\n",
    "embedding_size = len(embeddings_model.embed_query(\"테스트\"))      # 모델 차원 자동 계산\n",
    "index = faiss.IndexFlatL2(embedding_size)                       # L2 거리 기반 인덱스\n",
    "vectorstore = FAISS(                                            # FAISS 최신 방식으로 변수 지정\n",
    "    embedding_function=embeddings_model,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore({}),\n",
    "    index_to_docstore_id={}\n",
    ")\n",
    "\n",
    "# =========================================\n",
    "# 4. RetrieverMemory 연결\n",
    "# =========================================\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})\n",
    "memory = VectorStoreRetrieverMemory(retriever=retriever)\n",
    "\n",
    "# =========================================\n",
    "# 5. 대화 내용 저장\n",
    "# =========================================\n",
    "memory.save_context(\n",
    "    inputs={\"human\": \"안녕하세요, 오늘 면접에 참석해주셔서 감사합니다. 자기소개 부탁드립니다.\"},\n",
    "    outputs={\"ai\": \"안녕하세요. 저는 컴퓨터 과학을 전공한 신입 개발자입니다. 대학에서는 주로 자바와 파이썬을 사용했으며, 최근에는 웹 개발 프로젝트에 참여하여 실제 사용자를 위한 서비스를 개발하는 경험을 했습니다.\"}\n",
    ")\n",
    "memory.save_context(\n",
    "    inputs={\"human\": \"프로젝트에서 어떤 역할을 맡았나요?\"},\n",
    "    outputs={\"ai\": \"제가 맡은 역할은 백엔드 개발자였습니다. 사용자 데이터 처리와 서버 로직 개발을 담당했으며, RESTful API를 구현하여 프론트엔드와의 통신을 담당했습니다. 또한, 데이터베이스 설계에도 참여했습니다.\"}\n",
    ")\n",
    "memory.save_context(\n",
    "    inputs={\"human\": \"팀 프로젝트에서 어려움을 겪었던 경험이 있다면 어떻게 해결했나요?\"},\n",
    "    outputs={\"ai\": \"프로젝트 초기에 의사소통 문제로 몇 가지 어려움이 있었습니다. 이를 해결하기 위해 저희 팀은 정기적인 미팅을 갖고 각자의 진행 상황을 공유했습니다. 또한, 문제가 발생했을 때는 적극적으로 의견을 나누고, 합리적인 해결책을 찾기 위해 노력했습니다.\"}\n",
    ")\n",
    "memory.save_context(\n",
    "    inputs={\"human\": \"개발자로서 자신의 강점은 무엇이라고 생각하나요?\"},\n",
    "    outputs={\"ai\": \"제 강점은 빠른 학습 능력과 문제 해결 능력입니다. 새로운 기술이나 도구를 빠르게 습득할 수 있으며, 복잡한 문제에 직면했을 때 창의적인 해결책을 제시할 수 있습니다. 또한, 팀워크를 중시하며 동료들과 협력하는 것을 중요하게 생각합니다.\"}\n",
    ")\n",
    "\n",
    "# =========================================\n",
    "# 6. 메모리 검색 테스트\n",
    "# =========================================\n",
    "print(memory.load_memory_variables({\"prompt\": \"면접자 전공은 무엇인가요?\"})[\"history\"])\n",
    "print(memory.load_memory_variables({\"human\": \"면접자가 프로젝트에서 맡은 역할은 무엇인가요?\"})[\"history\"])\n",
    "\n",
    "# =========================================\n",
    "# 7. gpt-4o-mini와 결합 예시\n",
    "# =========================================\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "question = \"이 면접자의 강점을 한 문장으로 요약해줘.\"\n",
    "context = memory.load_memory_variables({\"human\": question})[\"history\"]\n",
    "\n",
    "response = llm.invoke(f\"다음 대화 기록을 참고해서 답변해줘:\\n{context}\\n\\n질문: {question}\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd14b462",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 셀 출력 오류: 크레딧 소진으로 현재 해결 어려움 → 다른 임베딩 모델 사용해보기로 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4cf791",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d6a340",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* **임베딩 모델 교체 필요: `OpenAI 모델` → `all-MiniLM-L6-v2`**\n",
    "\n",
    "* **`sentence-transformers/all-MiniLM-L6-v2`**\n",
    "  * 역할: 문장이나 짧은 문단을 384차원 벡터로 변환 → **의미 기반 검색, 유사도 계산** 등에 사용\n",
    "  * 기반: `BERT` 계열의 `MiniLM` 아키텍처를 `1B(10억) 문장 쌍 데이터로 파인튜닝`\n",
    "  * 언어: `주로 영어`에 최적화, 하지만 한국어도 어느 정도 처리 가능 (다만 정확도는 다국어 모델보다 낮음)\n",
    "  * 속도: `CPU에서도 빠르게 동작`, GPU 사용 시 더 빠름\n",
    "\n",
    "* 용량\n",
    "  * 모델 파일 크기: `약 90MB` (`PyTorch weights 기준`)\n",
    "  * 설치 후 캐시 포함: `약 100~120MB` 정도 차지\n",
    "  * 메모리 사용량: `CPU`에서 로드 시 약 `400~500MB` RAM 사용\n",
    "\n",
    "* 저장 위치: 전약 or 해당 프로젝트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d452a7f0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d210561e",
   "metadata": {},
   "source": [
    "* 임베딩 모델을 활용한 전체 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061e0c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_1\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# 로컬 임베딩 모델 로드\n",
    "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "docs = [\n",
    "    \"파이썬은 데이터 분석과 AI 개발에 널리 사용되는 언어입니다.\",\n",
    "    \"FAISS는 Facebook AI Research에서 만든 벡터 검색 라이브러리입니다.\",\n",
    "    \"벡터스토어는 문서 검색과 추천 시스템에 활용됩니다.\"\n",
    "]\n",
    "\n",
    "# 문서 임베딩\n",
    "doc_embeddings = embed_model.encode(docs, convert_to_numpy=True)\n",
    "\n",
    "# FAISS 인덱스 생성\n",
    "dimension = doc_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(doc_embeddings)\n",
    "\n",
    "# 검색 함수\n",
    "def search(query, k=2):\n",
    "    query_vec = embed_model.encode([query], convert_to_numpy=True)\n",
    "    D, I = index.search(query_vec, k)\n",
    "    return [docs[i] for i in I[0]]\n",
    "\n",
    "print(search(\"벡터스토어 설명해줘\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7319c28f",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 셀 출력 (23.4s)\n",
    "\n",
    "    ```python\n",
    "    /Users/jay/.pyenv/versions/lc_env/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
    "    from .autonotebook import tqdm as notebook_tqdm\n",
    "    ['벡터스토어는 문서 검색과 추천 시스템에 활용됩니다.', '파이썬은 데이터 분석과 AI 개발에 널리 사용되는 언어입니다.']\n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "* 셀 출력 경고 메시지 \n",
    "\n",
    "  * 분석\n",
    "\n",
    "    * `/Users/jay/.pyenv/versions/lc_env/lib/python3.13/site-packages/tqdm/auto.py:21`\n",
    "      * 경고가 발생한 파일 경로와 줄 번호\n",
    "      * tqdm/auto.py → tqdm의 자동 환경 감지 모드 파일\n",
    "      * :21 → 21번째 줄에서 경고를 발생시켰다는 뜻\n",
    "  \n",
    "    * `TqdmWarning`\n",
    "      * `tqdm` 라이브러리가 정의한 경고 클래스 이름\n",
    "      * `Warning`이므로 코드 실행은 계속되지만, 기능 일부가 제한될 수 있다는 신호\n",
    "\n",
    "    * `IProgress not found.`\n",
    "      * `IProgress` = **`Jupyter Notebook/Lab`에서 진행바를 표시하는 위젯 클래스**\n",
    "      * `ipywidgets` 패키지 안에 포함되어 있음\n",
    "      * 현재 환경에서 IProgress를 찾지 못했다는 뜻 → 즉, ipywidgets가 없거나 버전이 맞지 않음\n",
    "\n",
    "\n",
    "    * `Please update jupyter and ipywidgets.`\n",
    "      * 해결 방법 안내_1: **`jupyter` (노트북 실행 환경)**\n",
    "      * 해결 방법 안내_2: **`ipywidgets` (진행바, 슬라이더, 버튼 같은 UI 위젯 제공)**\n",
    "      * 이 둘을 설치하거나 최신 버전으로 업데이트하라는 의미\n",
    "\n",
    "\n",
    "    * `See https://ipywidgets.readthedocs.io/en/stable/user_install.html`\n",
    "    * = [공식가이드](https://ipywidgets.readthedocs.io/en/stable/user_install.html) 참고하라는 의미\n",
    "\n",
    "\n",
    "    * `from .autonotebook import tqdm as notebook_tqdm`\n",
    "      * `tqdm`이 `Jupyter 환경`이면 `notebook용 진행바`를, `터미널 환경`이면 `CLI용 진행바`를 **자동으로 선택하려는 코드**\n",
    "      * 여기서 `notebook용 진행바를 쓰려고 하다가` **IProgress가 없어서 경고 발생**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d829afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_2\n",
    "# vscode 터미널: `pip install ipywidgets` 설치 후\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# 로컬 임베딩 모델 로드\n",
    "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "docs = [\n",
    "    \"파이썬은 데이터 분석과 AI 개발에 널리 사용되는 언어입니다.\",\n",
    "    \"FAISS는 Facebook AI Research에서 만든 벡터 검색 라이브러리입니다.\",\n",
    "    \"벡터스토어는 문서 검색과 추천 시스템에 활용됩니다.\"\n",
    "]\n",
    "\n",
    "# 문서 임베딩\n",
    "doc_embeddings = embed_model.encode(docs, convert_to_numpy=True)\n",
    "\n",
    "# FAISS 인덱스 생성\n",
    "dimension = doc_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(doc_embeddings)\n",
    "\n",
    "# 검색 함수\n",
    "def search(query, k=2):\n",
    "    query_vec = embed_model.encode([query], convert_to_numpy=True)\n",
    "    D, I = index.search(query_vec, k)\n",
    "    return [docs[i] for i in I[0]]\n",
    "\n",
    "print(search(\"벡터스토어 설명해줘\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1e73f9",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 셀 출력 (3.5s)\n",
    "\n",
    "    ```markdown\n",
    "    ['벡터스토어는 문서 검색과 추천 시스템에 활용됩니다.', '파이썬은 데이터 분석과 AI 개발에 널리 사용되는 언어입니다.']\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b34ace",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adf4887",
   "metadata": {},
   "source": [
    "* 교재 속 내용으로 코드 수정해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7f96f784",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.memory import VectorStoreRetrieverMemory\n",
    "from langchain_openai import ChatOpenAI                                     # GPT 호출용 (원하면 제거 가능)\n",
    "\n",
    "# =========================================\n",
    "# 1. 로컬 임베딩 모델 로드\n",
    "# =========================================\n",
    "# 'cache_folder'를 지정하면 모델이 프로젝트 폴더 안에 저장됨\n",
    "# 처음 실행 시 Hugging Face Hub에서 다운로드 후 캐시에 저장\n",
    "embed_model = SentenceTransformer(\n",
    "    'sentence-transformers/all-MiniLM-L6-v2',\n",
    "    cache_folder=\"./models\"                                                 # 프로젝트 루트의 models 폴더에 저장\n",
    ")\n",
    "\n",
    "# =========================================\n",
    "# 2. 빈 FAISS 인덱스 초기화\n",
    "# =========================================\n",
    "# 모델의 임베딩 차원 수를 자동 계산\n",
    "embedding_size = embed_model.get_sentence_embedding_dimension()\n",
    "\n",
    "# L2 거리 기반 인덱스 생성\n",
    "index = faiss.IndexFlatL2(embedding_size)\n",
    "\n",
    "# LangChain의 FAISS 래퍼로 감싸기\n",
    "vectorstore = FAISS(\n",
    "    embedding_function=embed_model.encode,                                  # 로컬 임베딩 함수 사용\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore({}),\n",
    "    index_to_docstore_id={}\n",
    ")\n",
    "\n",
    "# =========================================\n",
    "# 3. RetrieverMemory 연결\n",
    "# =========================================\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})                # k=1 → 가장 유사한 1개만 검색해보기\n",
    "memory = VectorStoreRetrieverMemory(retriever=retriever)\n",
    "\n",
    "# =========================================\n",
    "# 4. 대화 내용 저장\n",
    "# =========================================\n",
    "memory.save_context(\n",
    "    inputs={\"human\": \"안녕하세요, 오늘 면접에 참석해주셔서 감사합니다. 자기소개 부탁드립니다.\"},\n",
    "    outputs={\"ai\": \"안녕하세요. 저는 컴퓨터 과학을 전공한 신입 개발자입니다. 대학에서는 주로 자바와 파이썬을 사용했으며, 최근에는 웹 개발 프로젝트에 참여하여 실제 사용자를 위한 서비스를 개발하는 경험을 했습니다.\"}\n",
    ")\n",
    "memory.save_context(\n",
    "    inputs={\"human\": \"프로젝트에서 어떤 역할을 맡았나요?\"},\n",
    "    outputs={\"ai\": \"제가 맡은 역할은 백엔드 개발자였습니다. 사용자 데이터 처리와 서버 로직 개발을 담당했으며, RESTful API를 구현하여 프론트엔드와의 통신을 담당했습니다. 또한, 데이터베이스 설계에도 참여했습니다.\"}\n",
    ")\n",
    "memory.save_context(\n",
    "    inputs={\"human\": \"팀 프로젝트에서 어려움을 겪었던 경험이 있다면 어떻게 해결했나요?\"},\n",
    "    outputs={\"ai\": \"프로젝트 초기에 의사소통 문제로 몇 가지 어려움이 있었습니다. 이를 해결하기 위해 저희 팀은 정기적인 미팅을 갖고 각자의 진행 상황을 공유했습니다. 또한, 문제가 발생했을 때는 적극적으로 의견을 나누고, 합리적인 해결책을 찾기 위해 노력했습니다.\"}\n",
    ")\n",
    "memory.save_context(\n",
    "    inputs={\"human\": \"개발자로서 자신의 강점은 무엇이라고 생각하나요?\"},\n",
    "    outputs={\"ai\": \"제 강점은 빠른 학습 능력과 문제 해결 능력입니다. 새로운 기술이나 도구를 빠르게 습득할 수 있으며, 복잡한 문제에 직면했을 때 창의적인 해결책을 제시할 수 있습니다. 또한, 팀워크를 중시하며 동료들과 협력하는 것을 중요하게 생각합니다.\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfd4c1b",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 실행 시간 (8.2s)\n",
    "\n",
    "* `embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b82d884",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================================\n",
    "# 5. 메모리 검색 테스트\n",
    "# =========================================\n",
    "print(\"질문: 면접자 전공은 무엇인가요?\", \"\\n\")\n",
    "print(\"검색 결과:\", memory.load_memory_variables({\"prompt\": \"면접자 전공은 무엇인가요?\"})[\"history\"])\n",
    "print(\"\\n\", \"===\"*50, \"\\n\")\n",
    "print(\"질문: 면접자가 프로젝트에서 맡은 역할은 무엇인가요?\", \"\\n\")\n",
    "print(\"검색 결과:\", memory.load_memory_variables({\"human\": \"면접자가 프로젝트에서 맡은 역할은 무엇인가요?\"})[\"history\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772db3b6",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 셀 출력 (0.7s)\n",
    "\n",
    "    ```markdown\n",
    "    질문: 면접자 전공은 무엇인가요? \n",
    "\n",
    "    검색 결과: human: 프로젝트에서 어떤 역할을 맡았나요?\n",
    "    ai: 제가 맡은 역할은 백엔드 개발자였습니다. 사용자 데이터 처리와 서버 로직 개발을 담당했으며, RESTful API를 구현하여 프론트엔드와의 통신을 담당했습니다. 또한, 데이터베이스 설계에도 참여했습니다.\n",
    "\n",
    "    ====================================================================================================================================================== \n",
    "\n",
    "    질문: 면접자가 프로젝트에서 맡은 역할은 무엇인가요? \n",
    "\n",
    "    검색 결과: human: 프로젝트에서 어떤 역할을 맡았나요?\n",
    "    ai: 제가 맡은 역할은 백엔드 개발자였습니다. 사용자 데이터 처리와 서버 로직 개발을 담당했으며, RESTful API를 구현하여 프론트엔드와의 통신을 담당했습니다. 또한, 데이터베이스 설계에도 참여했습니다.\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f15fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# 6. (선택) gpt-4o-mini와 결합 예시\n",
    "# =========================================\n",
    "# OpenAI API 키가 있어야 동작합니다. 없으면 이 부분은 주석 처리하세요.\n",
    "# .env 파일에서 API 키 불러오기\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\", \n",
    "    api_key=api_key)\n",
    "\n",
    "question = \"이 면접자의 강점을 한 문장으로 요약해줘.\"\n",
    "context = memory.load_memory_variables({\"human\": question})[\"history\"]\n",
    "\n",
    "response = llm.invoke(f\"다음 대화 기록을 참고해서 답변해줘:\\n{context}\\n\\n질문: {question}\")\n",
    "print(\"\\nGPT 응답:\", response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8987e3",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 셀 출력 (2.4s)\n",
    "\n",
    "    ```markdown\n",
    "    GPT 응답: 면접자는 팀 내 의사소통 문제를 해결하기 위해 정기적인 미팅과 의견 공유를 적극적으로 활용한 강점을 가지고 있습니다.\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c00a0a8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29086ade",
   "metadata": {},
   "source": [
    "* *next: LCEL Chain 에 메모리 추가*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec9567e",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
