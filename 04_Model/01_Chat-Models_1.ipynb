{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c6c0226",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e416518f",
   "metadata": {},
   "source": [
    "* 출처: LangChain 공식 문서 또는 해당 교재명\n",
    "* 원본 URL: https://smith.langchain.com/hub/teddynote/summary-stuff-documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6793a387",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a8e3a7",
   "metadata": {},
   "source": [
    "## **다양한 `LLM` 모델 활용**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057e303b",
   "metadata": {},
   "source": [
    "* 모델 혹은 **`LLM`(Large Language Model)** 단계는 **이전 프롬프트 단계에서 구성된 입력을 기반으로 대규모 언어 모델을 활용하여 응답을 생성하는 과정**\n",
    "\n",
    "* 이 단계는 RAG 시스템의 핵심적인 부분\n",
    "\n",
    "* **언어 모델의 능력을 최대한 활용하여 사용자의 질문에 대해 정확하고 자연스러운 답변을 생성**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f17f9d9",
   "metadata": {},
   "source": [
    "### **`LLM`** 필요성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44531b6d",
   "metadata": {},
   "source": [
    "* **사용자 의도 이해** \n",
    "  * `LLM`은 `다양한 언어`의 `구조와 의미`를 깊이 이해하고 있으며, 이를 바탕으로 `복잡한 질문`에 `답`할 수 있음\n",
    "  * **`자연어 이해(NLU)`** 와 **`자연어 생성(NLG)`** 능력이 결합되어, 보다 `자연스럽고 유익한 응답` 제공\n",
    "\n",
    "* **문맥적 적응성**\n",
    "  * `LLM`은 주어진 `문맥`을 `고려`하여 응답 생성 → 사용자의 질문에 더욱 정확하게 대응 가능\n",
    "  * `사전학습`된 지식`외` + **사용자가 제공한 정보에 기반한 답변** 을 `문맥을 참고`하여 `답변`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36205411",
   "metadata": {},
   "source": [
    "### **`LLM`** 중요성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd41807",
   "metadata": {},
   "source": [
    "* `LLM 단계` = 사용자의 질문에 대한 **`답변의 질`** + **`자연스러움`** 을 결정짓는 **핵심 요소**\n",
    "\n",
    "* `LLM` = `지금까지의 모든 데이터와 정보를 종합`하여 **사용자의 질문에 최적화된 답변을 생성**\n",
    "\n",
    "* `LLM의 성능` → **`RAG` 시스템의 전체적인 성능과 사용자 만족도에 직접적으로 영향** 을 미침 → `RAG` 시스템을 사용하는 많은 응용 분야에서 매우 중요한 역할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be4975fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 환경변수 처리 및 클라어트 생성\n",
    "from langsmith import Client\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langsmith import Client\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "# 클라이언트 생성 \n",
    "api_key = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "client = Client(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1815f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangSmith 추적 설정하기 (https:smith.langchin.com)\n",
    "# LangSmith 추적을 위한 라이브러리 임포트\n",
    "from langsmith import traceable                                                             # @traceable 데코레이터 사용 시\n",
    "\n",
    "# LangSmith 환경 변수 확인\n",
    "\n",
    "print(\"\\n--- LangSmith 환경 변수 확인 ---\")\n",
    "langchain_tracing_v2 = os.getenv('LANGCHAIN_TRACING_V2')\n",
    "langchain_project = os.getenv('LANGCHAIN_PROJECT')\n",
    "langchain_api_key_status = \"설정됨\" if os.getenv('LANGCHAIN_API_KEY') else \"설정되지 않음\"      # API 키 값은 직접 출력하지 않음\n",
    "org = \"설정됨\" if os.getenv('LANGCHAIN_ORGANIZATION') else \"설정되지 않음\"                     # 직접 출력하지 않음\n",
    "\n",
    "if langchain_tracing_v2 == \"true\" and os.getenv('LANGCHAIN_API_KEY') and langchain_project:\n",
    "    print(f\"✅ LangSmith 추적 활성화됨 (LANGCHAIN_TRACING_V2='{langchain_tracing_v2}')\")\n",
    "    print(f\"✅ LangSmith 프로젝트: '{langchain_project}'\")\n",
    "    print(f\"✅ LangSmith API Key: {langchain_api_key_status}\")\n",
    "    print(\"  -> 이제 LangSmith 대시보드에서 이 프로젝트를 확인해 보세요.\")\n",
    "else:\n",
    "    print(\"❌ LangSmith 추적이 완전히 활성화되지 않았습니다. 다음을 확인하세요:\")\n",
    "    if langchain_tracing_v2 != \"true\":\n",
    "        print(f\"  - LANGCHAIN_TRACING_V2가 'true'로 설정되어 있지 않습니다 (현재: '{langchain_tracing_v2}').\")\n",
    "    if not os.getenv('LANGCHAIN_API_KEY'):\n",
    "        print(\"  - LANGCHAIN_API_KEY가 설정되어 있지 않습니다.\")\n",
    "    if not langchain_project:\n",
    "        print(\"  - LANGCHAIN_PROJECT가 설정되어 있지 않습니다.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb5b9e0",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 셀 출력\n",
    "\n",
    "    ```markdown\n",
    "    --- LangSmith 환경 변수 확인 ---\n",
    "    ✅ LangSmith 추적 활성화됨 (LANGCHAIN_TRACING_V2='true')\n",
    "    ✅ LangSmith 프로젝트: 'LangChain-prantice'\n",
    "    ✅ LangSmith API Key: 설정됨\n",
    "    -> 이제 LangSmith 대시보드에서 이 프로젝트를 확인해 보세요.\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57f626a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada19473",
   "metadata": {},
   "source": [
    "## **`OpenAI`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8104b4",
   "metadata": {},
   "source": [
    "### **개요**\n",
    "\n",
    "- `OpenAI`는 **채팅 전용** `Large Language Model (LLM)`을 제공\n",
    "- 이 모델을 생성할 때 `다양한 옵션`을 `지정`할 수 있으며, 이러한 옵션들은 모델의 동작 방식에 영향을 미침"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0192aa5e",
   "metadata": {},
   "source": [
    "### **옵션 상세 설명**\n",
    "\n",
    "- `temperature`\n",
    "\n",
    "  - 샘플링 온도를 설정하는 옵션\n",
    "  - 값은 `0과 2 사이`에서 선택 가능\n",
    "    - `높은 값(예: 0.8)` = **출력을 더 무작위하게 만듦 ≒ 높은 창의성** \n",
    "    - `낮은 값(예: 0.2)` = 출력을 더 집중되고 결정론적으로 만듦 ≒ **낮은 창의성**\n",
    "\n",
    "- `max_tokens`\n",
    "\n",
    "  - 채팅 완성에서 **`생성할 토큰의 최대 개수` 지정**\n",
    "  - 모델이 **한 번에 생성할 수 있는 텍스트의 길이를 제어**\n",
    "\n",
    "- `model_name`\n",
    "\n",
    "  - 적용 가능한 모델을 선택하는 옵션\n",
    "  - 더 자세한 정보는 [OpenAI 모델 문서](https://platform.openai.com/docs/models)에서 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800ce118",
   "metadata": {},
   "source": [
    "**모델 스펙**\n",
    "\n",
    "- [링크](https://platform.openai.com/docs/models): https://platform.openai.com/docs/models\n",
    "\n",
    "| Model               | Input (1M) | Cached Input (1M) | Output (1M) | Context Window | Max Output Tokens | Knowledge Cutoff |\n",
    "|---------------------|------------|-------------------|-------------|----------------|-------------------|------------------|\n",
    "| gpt-4.1             | $2.00      | $0.50             | $8.00       | 1,047,576      | 32,768            | Jun 01, 2024     |\n",
    "| gpt-4.1-mini        | $0.40      | $0.10             | $1.60       | 1,047,576      | 32,768            | Jun 01, 2024     |\n",
    "| gpt-4.1-nano        | $0.10      | $0.025            | $0.40       | 1,047,576      | 32,768            | Jun 01, 2024     |\n",
    "| gpt-4o              | $2.50      | $1.25             | $10.00      | 128,000        | 16,384            | Oct 01, 2023     |\n",
    "| gpt-4o-mini         | $0.15      | $0.075            | $0.60       | 128,000        | 16,384            | Oct 01, 2023     |\n",
    "| o1                  | $15.00     | $7.50             | $60.00      | 128,000        | 65,536            | Oct 01, 2023     |\n",
    "| o1-mini             | $1.10      | $0.55             | $4.40       | 128,000        | 65,536            | Oct 01, 2023     |\n",
    "| o1-pro              | $150.00    | –                 | $600.00     | 128,000        | 65,536            | Oct 01, 2023     |\n",
    "| o3-mini             | $1.10      | $0.55             | $4.40       | 200,000        | 100,000           | Oct 01, 2023     |\n",
    "| gpt-4.5-preview     | $75.00     | $37.50            | $150.00     | –              | –                 | –                |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad771f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI 모델 호출해보기\n",
    "\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# .env 파일 로드\n",
    "load_dotenv()\n",
    "\n",
    "# 환경 변수에서 OpenAI API 키 읽기\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# OpenAI API 키 설정\n",
    "openai.api_key = api_key\n",
    "\n",
    "# 호출 확인 (Chat API 호출)\n",
    "response = openai.completions.create(\n",
    "    model=\"gpt-4o-mini\",                            # gpt-4o-mini 모델 사용\n",
    "    prompt=\"Hello, world!\",\n",
    "    max_tokens=5\n",
    ")\n",
    "\n",
    "print(response.choices[0].text.strip())             # This is a simple HTML (1.1s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465d8aac",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "*  셀 출력 (조건 = max_tokens=5) (1.1s)\n",
    "\n",
    "    ```markdown\n",
    "    This is a simple HTML\n",
    "    ```\n",
    "\n",
    "* 모델 답변 이유\n",
    "  \n",
    "  * **모델의 기본적인 패턴 인식**\n",
    "    * `OpenAI` 모델은 많은 데이터를 학습하면서 **`Hello, world!`**라는 문구가 종종 `프로그래밍` 관련 예시로 사용된다는 사실을 인식\n",
    "    * **특히 이 문장은 프로그래밍 언어에서 자주 보이는 첫 번째 예제**\n",
    "\n",
    "    * 예를 들어, `Python`, `JavaScript`, `HTML` 등에서 **`Hello, World!`** 를 출력하는 간단한 코드 많음\n",
    "    * 그래서 모델은 그 문장을 보고 `이건 HTML 코드의 예시일 가능성이 크다`라고 추론해서 **`This is a simple HTML`** 을 출력한 것으로 보임\n",
    "  \n",
    "  * **모델의 자연어 처리 방식**\n",
    "    * 모델은 주어진 텍스트의 맥락에 맞는 답을 생성하는데, **`Hello, world!`** 문장은 프로그래밍 예제에서 자주 사용 → **`간단한 HTML 코드`** 를 연상\n",
    "    * 모델은 특정 문장을 **`이런 유형의 응답이 일반적으로 나오는 경우가 많다`** 고 학습해왔기 때문에 그런 출력을 낸 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1812ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다른 물음 던져보기\n",
    "response = openai.completions.create(\n",
    "    model=\"gpt-4o-mini\",                            # gpt-4o-mini 모델 사용\n",
    "    prompt=\"Say hello to the world in a friendly way.\",\n",
    "    max_tokens=50\n",
    ")\n",
    "\n",
    "print(response.choices[0].text.strip())             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec00b82f",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 셀 출력 (프롬프트 변경, max_tokens=50)(1.7s)\n",
    "\n",
    "    ```markdown\n",
    "\n",
    "    I'm your assistant, available anytime you need me. My purpose is to support and assist you with any questions or tasks you have. How can I help you today? 😊\"\n",
    "    ```\n",
    "    ```markdown\n",
    "    Hello! I'm your friendly assistant, here to help you anytime.\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d4aa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ChatOpenAI 객체 생성\n",
    "gpt = ChatOpenAI(\n",
    "    temperature=0,\n",
    "    model_name=\"gpt-4o-mini\",                                   # 모델명\n",
    ")\n",
    "\n",
    "# 스트리밍 출력을 위하여 invoke() 대신 stream()을 사용\n",
    "answer = gpt.stream(\"사랑이 뭔가요?\")\n",
    "\n",
    "# 답변 출력\n",
    "#stream_response(answer)\n",
    "for chunk in answer:\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2b1a8e",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 셀 출력 (temperature=0)(3.1s)\n",
    "\n",
    "    ```plaintext\n",
    "    사랑은 복잡하고 다면적인 감정으로, 사람들 간의 깊은 유대감이나 애정, 헌신을 포함합니다. 사랑은 가족, 친구, 연인 등 다양한 관계에서 나타날 수 있으며, 각기 다른 형태와 표현을 가집니다. \n",
    "\n",
    "    사랑은 종종 기쁨, 행복, 안정감을 주지만, 때로는 고통이나 상실감과 같은 어려운 감정을 동반하기도 합니다. 사랑은 서로를 이해하고 지지하며, 함께 성장하는 과정에서 중요한 역할을 합니다. \n",
    "\n",
    "    철학적, 심리학적, 문화적 관점에서 사랑은 다양한 해석이 가능하며, 각 개인의 경험에 따라 그 의미가 달라질 수 있습니다.\n",
    "    ```\n",
    "\n",
    "  ***\n",
    "\n",
    "* 교재 속 코드(이전 방식) → 최신 방식으로 교체\n",
    "  * 교재 속 코드 \n",
    "  \n",
    "  ```python\n",
    "    stream_response(answer)\n",
    "  ```\n",
    "\n",
    "  * 최신 방식 \n",
    "  ```python\n",
    "    for chunk in answer:\n",
    "        print(chunk.content, end=\"\", flush=True)\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a281ec",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e89b399",
   "metadata": {},
   "source": [
    "## **`Anthropic`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebe8ca9",
   "metadata": {},
   "source": [
    "* `Anthropic`은 인공지능(AI) `안전성`과 `연구`에 중점을 둔 미국의 스타트업 기업\n",
    "\n",
    "  * **설립 연도**: 2021년\n",
    "  * **위치**: 미국 샌프란시스코\n",
    "  * **창립자**: `OpenAI 출신 직원들` (Daniela Amodei와 Dario Amodei 등)\n",
    "  * **기업 형태**: `공익기업` (`Public Benefit Corporation`)으로 등록"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd8810f",
   "metadata": {},
   "source": [
    "### **`Claude`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5354ce9",
   "metadata": {},
   "source": [
    "* Claude는 Anthropic의 대표적인 대규모 언어 모델(LLM) 제품군입니다. \n",
    "\n",
    "  * **API 키 발급**: [https://console.anthropic.com/settings/keys](https://console.anthropic.com/settings/keys)\n",
    "  * **모델 리스트**: [https://docs.anthropic.com/en/docs/about-claude/models](https://docs.anthropic.com/en/docs/about-claude/models)\n",
    "\n",
    "\n",
    "![Claude](images/anthropic-20241023.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85c4012",
   "metadata": {},
   "source": [
    "**업데이트 된 모델 스펙**\n",
    "\n",
    "* 현재 앤트로픽의 모델은 **Claude 4**로 최신 버전 출시\n",
    "* 제공된 모델 정보는 업데이트 필요 → 수정한 표는 아래와 같음\n",
    "\n",
    "\n",
    "| 모델 이름               | 모델              | Anthropic API   | AWS Bedrock                    | GCP Vertex AI           |\n",
    "| ------------------- | --------------- | --------------- | ------------------------------ | ----------------------- |\n",
    "| **Claude 4**        | claude-4        | claude-4        | anthropic.claude-4-v1:0        | claude-4\\@latest        |\n",
    "| **Claude 4 Haiku**  | claude-4-haiku  | claude-4-haiku  | anthropic.claude-4-haiku-v1:0  | claude-4-haiku\\@latest  |\n",
    "| **Claude 4 Sonnet** | claude-4-sonnet | claude-4-sonnet | anthropic.claude-4-sonnet-v1:0 | claude-4-sonnet\\@latest |\n",
    "| **Claude 4 Opus**   | claude-4-opus   | claude-4-opus   | anthropic.claude-4-opus-v1:0   | claude-4-opus\\@latest   |\n",
    "\n",
    "* **주요 변경 사항**:\n",
    "\n",
    "  * **Claude 4 모델**: 최신 모델로, `Claude 4`, `Claude 4 Haiku`, `Claude 4 Sonnet`, `Claude 4 Opus` 포함\n",
    "  * **API 명**: 최신 API 명명 규칙에 맞게 `claude-4`, `claude-4-haiku`, `claude-4-sonnet`, `claude-4-opus`로 통합\n",
    "  * **AWS Bedrock**와 **GCP Vertex AI**는 여전히 최신 버전으로 업데이트되어 있으며, **@latest**나 **-v1:0** 같은 태그가 사용\n",
    "\n",
    "* **참고**:\n",
    "\n",
    "  * `Claude 3.x` 시리즈는 여전히 일부 서비스에 사용될 수 있지만, 이제 대부분 **Claude 4** 모델로 업그레이드됨\n",
    "  * `연말 출시 예정`으로 되어 있던 **Claude 3.5** 모델들은 이제 실제로 **Claude 4** 모델로 출시되었으므로, 사용 시 최신 모델을 확인하는 것이 중요"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c190db1",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f64a21d",
   "metadata": {},
   "source": [
    "* **`Anthropic` 무료 API 리미트**\n",
    "\n",
    "  * 모델별 사용 리미트 (1회당)\n",
    "\n",
    "| 모델                | 요청 횟수 (분당) | 입력 토큰 (분당)       | 출력 토큰 (분당)      |\n",
    "|---------------------|-----------------|------------------------|----------------------|\n",
    "| **Claude Opus 4.x**  | 5회             | 최대 10,000 토큰 (캐시 읽기 제외) | 최대 4,000 토큰       |\n",
    "| **Claude Sonnet 4**  | 5회             | 최대 10,000 토큰 (캐시 읽기 제외) | 최대 4,000 토큰       |\n",
    "| **Claude Sonnet 3.7**| 5회             | 최대 10,000 토큰 (캐시 읽기 제외) | 최대 4,000 토큰       |\n",
    "| **Claude Haiku 3.5** | 5회             | 최대 25,000 토큰 (캐시 읽기 제외) | 최대 5,000 토큰       |\n",
    "| **Claude Haiku 3**   | 5회             | 최대 25,000 토큰 (캐시 읽기 제외) | 최대 5,000 토큰       |\n",
    "\n",
    "  * 기타 리미트\n",
    "\n",
    "| 항목                  | 리미트 값             |\n",
    "|-----------------------|-----------------------|\n",
    "| **배치 요청**          | 5회/분 (모든 모델 합산) |\n",
    "| **웹 검색 도구 사용**  | 20회/초 (모든 모델 합산) |\n",
    "| **파일 API 저장소**    | 100GB (조직 전체)     |\n",
    "\n",
    "  * 하루에 사용할 수 있는 횟수\n",
    "\n",
    "    * 각 모델은 **5회/분** 요청할 수 있으며, 하루 동안 최대 **7,200회** 요청할 수 있습니다.\n",
    "    * 계산: **5회/분 × 60분 × 24시간 = 7,200회/일**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07745418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anthropic 모델 호출해보기\n",
    "\n",
    "import anthropic\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# .env 파일 로드\n",
    "load_dotenv()\n",
    "\n",
    "# 환경 변수에서 Anthropic API 키 읽기\n",
    "api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "print(\"\\n--- ANTHROPIC API KEY 환경 변수 확인 ---\")\n",
    "\n",
    "if api_key:\n",
    "    print(f\"✅ api_key: {api_key}\")\n",
    "    print(\" 앤트로픽 API 키가 환경변수로 잘 설정되었습니다.\")\n",
    "else:\n",
    "    print(\"❌ 앤트로픽 API 키가 환경변수로 제대로 설정되지 않았습니다.\")\n",
    "    if not api_key:\n",
    "        print(\" ANTHROPIC_API_KEY 환경변수가 설정되지 않았거나, 비어 있습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f441e9e2",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 셀 출력\n",
    "\n",
    "    ```markdown\n",
    "    --- ANTHROPIC API KEY 환경 변수 확인 ---\n",
    "    ✅ api_key: sk-****\n",
    "    앤트로픽 API 키가 환경변수로 잘 설정되었습니다.\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabdec9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anthropic API 클라이언트 생성\n",
    "client = None\n",
    "try:\n",
    "    client = anthropic.Anthropic(api_key=api_key)\n",
    "    print(\"\\n--- 클라이언트 생성 확인 ---\")\n",
    "    if client:\n",
    "        print(\"✅ 클라이언트가 성공적으로 생성되었습니다.\")\n",
    "    else:\n",
    "        print(\"❌ 클라이언트 생성에 실패했습니다.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ 클라이언트 생성 중 오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f081f4",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 셀 출력\n",
    "\n",
    "    ```markdown\n",
    "    --- 클라이언트 생성 확인 ---\n",
    "    ✅ 클라이언트가 성공적으로 생성되었습니다.\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73cd530",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "\n",
    "# Anthropic 클라이언트 생성\n",
    "client = anthropic.Anthropic()\n",
    "\n",
    "# 클라이언트 사용하여 메시지 스트림 생성\n",
    "with client.messages.stream(\n",
    "    max_tokens=50,  # 최대 토큰 수\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Say hello to the world in a friendly way.\"}],  # 사용자 입력\n",
    "    model=\"claude-3-haiku\",  # 사용할 모델\n",
    ") as stream:\n",
    "    # 스트림을 통해 생성된 텍스트 출력\n",
    "    for text in stream.text_stream:\n",
    "        print(text, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1a2946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 교재 속 예시\n",
    "\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "# ChatAnthropic 객체 생성\n",
    "anthropic = ChatAnthropic(model_name=\"claude-3-5-sonnet-20241022\")\n",
    "\n",
    "# 스트리밍 출력을 위하여 invoke() 대신 stream()을 사용합니다.\n",
    "answer = anthropic.stream(\"사랑이 뭔가요?\")\n",
    "\n",
    "# 답변 출력\n",
    "for chunk in answer:\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa848fe",
   "metadata": {},
   "source": [
    "* *클로드 API 오류로 실습 현재 불가*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479a63a9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6beb367",
   "metadata": {},
   "source": [
    "## **`Perplexity`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a83d4a3",
   "metadata": {},
   "source": [
    "* [링크](https://www.perplexity.ai/): https://www.perplexity.ai/\n",
    "\n",
    "* **설립연도**: 2022년\n",
    "* **주요 투자자**: Jeff Bezos, Nvidia, Databricks, Bessemer Venture Partners, IVP, Wayra 등\n",
    "* **최근 펀딩**: 5억 달러 (2024년 10월)\n",
    "* **기업 가치**: 약 90억 달러 (2024년 11월 기준)\n",
    "* **월간 활성 사용자**: 1,500만 명"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d55a33",
   "metadata": {},
   "source": [
    "### **`Perplexity Pro` 정확한 특징**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0faf87ea",
   "metadata": {},
   "source": [
    "* **일일 Pro 검색**: 300회\n",
    "* **AI 모델 선택**: `GPT-5`, `Claude 4 Sonnet`/`Opus`, `Sonar`/`Sonar Pro`, `Grok`[5] 등\n",
    "* **파일 분석**: `MD`, `PDF`, `CSV`, `이미지 파일` 등 지원\n",
    "* **가격**: 월 `$20` 또는 연 `$200`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bead1c9c",
   "metadata": {},
   "source": [
    "### **`Perplexity API` 사용 방법**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420d1b64",
   "metadata": {},
   "source": [
    "* **가격**\n",
    "\n",
    "  * ![perplexity-pricing.png](../04_Model/images/perplexity-pricing.png)\n",
    "\n",
    "  * **API 크레딧 획득**\n",
    "   * Perplexity Pro 구독 시 **매월 $5 상당의 API 크레딧 제공**\n",
    "\n",
    "* **API 모델 옵션**\n",
    "   * Llama 3 기반 모델\n",
    "   * Perplexity 온라인 LLM\n",
    "   * 인용 기능 포함\n",
    "\n",
    "* API 키 발급: [API 콘솔](https://www.perplexity.ai/settings/api)\n",
    "\n",
    "  * API 키 발급 후 `.env` 파일에 키 저장\n",
    "\n",
    "    ```bash\n",
    "    PPLX_API_KEY=이곳에 API 키를 입력하세요.\n",
    "    ```\n",
    "\n",
    "* 이후\n",
    "  \n",
    "  * `.env`(환경변수)로 불러오기\n",
    "\n",
    "    ```bash\n",
    "    import os\n",
    "\n",
    "    os.environ[\"PPLX_API_KEY\"] = \"이곳에 API 키를 입력하세요.\"\n",
    "    ```\n",
    "\n",
    "* **참고**\n",
    "\n",
    "  * [API 문서](https://docs.perplexity.ai/api-reference/chat-completions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175f3e82",
   "metadata": {},
   "source": [
    "* `Llama 3.1` 모델 사양\n",
    "\n",
    "| 모델명                          | 파라미터 수 | 컨텍스트 길이 | 모델 유형             | 주요 특징                                                                 |\n",
    "|-------------------------------|-------------|---------------|----------------------|--------------------------------------------------------------------------|\n",
    "| `Meta-Llama-3.1-8B`           | 8B          | 128,000 토큰  | 기본 모델             | 경량화된 모델로 로컬 환경에서의 텍스트 생성에 적합                        |\n",
    "| `Meta-Llama-3.1-70B`          | 70B         | 128,000 토큰  | 기본 모델             | 기업용 하드웨어에 최적화된 모델로, 텍스트 요약, 분류, 감정 분석 등에 우수한 성능 제공 |\n",
    "| `Meta-Llama-3.1-405B`         | 405B        | 128,000 토큰  | 기본 모델             | 가장 큰 공개 모델로, 고급 추론, 멀티모달 처리, 대규모 데이터 생성 등에 뛰어난 성능 발휘 |\n",
    "| `Meta-Llama-3.1-8B-Instruct`  | 8B          | 128,000 토큰  | 지시문 조정 모델       | 사용자 지시문에 따른 응답 생성에 최적화된 모델                            |\n",
    "| `Meta-Llama-3.1-70B-Instruct` | 70B         | 128,000 토큰  | 지시문 조정 모델       | 사용자 지시문에 따른 응답 생성에 최적화된 모델                            |\n",
    "| `Meta-Llama-3.1-405B-Instruct`| 405B        | 128,000 토큰  | 지시문 조정 모델       | 사용자 지시문에 따른 응답 생성에 최적화된 모델                            |\n",
    "\n",
    "\n",
    "  * **컨텍스트 길이 확장** \n",
    "    * `Llama 3.1 모델` = 이전 버전인 Llama 3의 8,192 토큰에서 `128,000 토큰`으로 컨텍스트 길이가 대폭 확장 → `더 긴 대화`나 `문서`도 처리\n",
    "\n",
    "  * **다국어 지원**\n",
    "    * 영어, 독일어, 프랑스어, 이탈리아어, 포르투갈어, 힌디어, 스페인어, 태국어 등 8개 언어를 지원\n",
    "\n",
    "  * **모델 유형**\n",
    "    * `기본 모델`과 `지시문 조정 모델`로 구분\n",
    "    * `지시문 조정 모델` = **사용자 지시문에 따른 응답 생성에 최적화**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7a183a",
   "metadata": {},
   "source": [
    "### **`ChatPerplexity` 매개변수**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2402296",
   "metadata": {},
   "source": [
    "* `model`  \n",
    "  * 사용할 언어 모델을 지정 \n",
    "    * 예시: `llama-3.1-sonar-small-128k-online`\n",
    "  * 기본 성능과 능력을 결정\n",
    "\n",
    "* `temperature`  \n",
    "  * 응답의 무작위성을 조절 (`0.0-1.0`)\n",
    "  * 0은 결정적, 1은 가장 무작위한 응답 생성\n",
    "\n",
    "* `top_p`  \n",
    "  * 토큰 샘플링의 확률 임계값 설정 (`0.0-1.0`)\n",
    "  * **높을수록 더 다양한 출력 허용**\n",
    "\n",
    "* `search_domain_filter`  \n",
    "  * 검색 결과를 지정된 도메인으로 제한\n",
    "  * `리스트` 형태로 제공 - 예시: [`perplexity.ai`]\n",
    "\n",
    "* `return_images`: 응답에 `이미지 포함 여부를 결정`하는 불리언 플래그\n",
    "\n",
    "* `return_related_questions`: `관련 질문 제안 기능`을 `활성화/비활성화`하는 불리언 플래그\n",
    "\n",
    "* `top_k`\n",
    "  * 사용할 검색 `결과의 수 제한` \n",
    "  * **0은 제한 없음을 의미**\n",
    "\n",
    "* `streaming`: 응답을 `스트리밍으로 받을지 완성된 형태로 받을지 결정`하는 불리언 플래그\n",
    "\n",
    "* `presence_penalty`\n",
    "  * 토큰 반복에 대한 페널티 (`-2.0에서 2.0`)\n",
    "  * `높을수록 재사용을 억제`\n",
    "\n",
    "* `frequency_penalty`  \n",
    "  * `일반적/희귀 토큰 선호도` 조정 (`-2.0에서 2.0`)\n",
    "  * `높을수록 희귀 토큰 선호`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4659e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 퍼플렉시티 모델 호출해보기\n",
    "from perplexipy import PerplexityClient\n",
    "from langchain_perplexity import ChatPerplexity\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# .env 파일 로드\n",
    "load_dotenv()\n",
    "\n",
    "# 환경 변수에서 Perplexity API 키 읽기\n",
    "api_key = os.getenv(\"PPLX_API_KEY\")\n",
    "\n",
    "print(\"\\n--- Perplexity API KEY 환경 변수 확인 ---\")\n",
    "\n",
    "if api_key:\n",
    "    print(f\"✅ api_key: {api_key}\")\n",
    "    print(\" Perplexity API 키가 환경변수로 잘 설정되었습니다.\")\n",
    "else:\n",
    "    print(\"❌ Perplexity API 키가 환경변수로 제대로 설정되지 않았습니다.\")\n",
    "    if not api_key:\n",
    "        print(\" Perplexity_API_KEY 환경변수가 설정되지 않았거나, 비어 있습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05ef41c",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 셀 출력\n",
    "\n",
    "    ```markdown\n",
    "    --- Perplexity API KEY 환경 변수 확인 ---\n",
    "    ✅ api_key: pplx-***\n",
    "    Perplexity API 키가 환경변수로 잘 설정되었습니다.\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61904097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 환경 변수에 PERPLEXITY_API_KEY 설정 필요\n",
    "client = PerplexityClient(key=os.getenv(\"PPLX_API_KEY\"))\n",
    "\n",
    "# 쿼리 수행\n",
    "result = client.query(\"Briefly say hello to the world in Swedish.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d9a22a",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 셀 출력 (2.8s)\n",
    "\n",
    "    ```markdown\n",
    "    A brief way to say hello to the world in Swedish is simply **\"Hej\"** (pronounced like \"hey\"). It is the most common, friendly, and versatile greeting used in nearly all situations, whether formal or informal[1][2][3][5]. For a slightly more cheerful variant, Swedes sometimes say **\"Hej hej!\"** which translates to \"Hi hi!\"[3].\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02570d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치 쿼리\n",
    "results = client.queryBatch(\"Different ways to declare a Python list:\")\n",
    "for r in results:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67aa118",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 셀 출력 (5.3s)\n",
    "\n",
    "    ```markdown\n",
    "    There are several common ways to declare or create a list in Python:\n",
    "\n",
    "    - **Using square brackets:** You can directly write square brackets `[]` with comma-separated elements inside, e.g., `my_list = [1, 2, 3]`. This is the most straightforward and popular method to create lists, including mixed data types such as integers, strings, or floats[2][3][5].\n",
    "\n",
    "    - **Using the `list()` constructor:** You can pass any iterable (like a string, tuple, or another list) to the `list()` function to create a new list. For example, `my_list = list((1, 2, 3))` creates a list from a tuple[2][3].\n",
    "\n",
    "    - **Using list comprehensions:** This is a concise syntax to generate lists by iterating over iterables and optionally applying an expression or condition, e.g., `my_list = [x*2 for x in range(5)]` creates `[0, 2, 4, 6, 8]`[2].\n",
    "\n",
    "    Additionally, after creating a list, you can add or modify elements using methods like `append()`, `insert()`, or `extend()`[1][4][5].\n",
    "\n",
    "    Summary table:\n",
    "\n",
    "    | Method                            | Syntax Example                            | Description                                |\n",
    "    |----------------------------------|------------------------------------------|--------------------------------------------|\n",
    "    | Square brackets (List literal)   | `my_list = [1, 'a', 3.14]`                | Directly declare list with elements       |\n",
    "    | `list()` constructor             | `my_list = list((1, 2, 3))`               | Create list from any iterable              |\n",
    "    | List comprehension               | `my_list = [x for x in range(5)]`         | Create list using concise iteration syntax |\n",
    "\n",
    "    These are the primary and most idiomatic ways to declare lists in Python[2][3][5].\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6423600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치 쿼리2\n",
    "results = client.queryBatch(\"2024년 노벨문학상 수상자를 조사해 주세요\")\n",
    "for r in results:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a47865",
   "metadata": {},
   "source": [
    "<small>    \n",
    "\n",
    "* 셀 출력 (6.0s)\n",
    "    \n",
    "    ```markdown\n",
    "\n",
    "    2024년 노벨문학상 수상자는 한국의 소설가 **한강**입니다. 한강은 역사적 트라우마에 맞서고 인간 삶의 연약함을 강렬한 시적 산문으로 표현한 작품들로 인정받아 수상하였으며, 이는 한국인 중 최초이자 아시아 여성 작가로서도 최초의 노벨문학상 수상입니다[1][2][3].\n",
    "\n",
    "    스웨덴 한림원은 한강의 문학을 \"역사적 트라우마에 맞서고 인간의 삶의 연약함을 드러내는 강렬한 시적 산문\"이라고 평가했습니다[3]. 한강은 2016년 소설 『채식주의자』로 세계 3대 문학상 중 하나인 부커상을 수상한 바 있으며, 이번 수상으로 노벨문학상 역대 121번째 수상자, 18번째 여성 수상자가 되었습니다[2].\n",
    "\n",
    "    이 수상은 한국 문학과 아시아 문학 모두에 매우 의미 있는 사건이며, 한국인의 노벨상 수상으로는 평화상 수상자인 고 김대중 전 대통령 이후 두 번째 사례입니다[2][4].\n",
    "\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48dc1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스트리밍 응답\n",
    "for r in client.queryStreamable(\"2024년 노벨문학상 수상자를 조사해 주세요\"):\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538cb276",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 셀 출력 (5.4s)\n",
    "\n",
    "    ```markdown\n",
    "\n",
    "    202\n",
    "    4\n",
    "    년\n",
    "    노\n",
    "    벨\n",
    "    문\n",
    "    학\n",
    "    상\n",
    "    수\n",
    "    상\n",
    "    자는\n",
    "    한국\n",
    "    의\n",
    "    소\n",
    "    설\n",
    "    가\n",
    "    **\n",
    "    한\n",
    "    강\n",
    "    **\n",
    "    입니다\n",
    "    .\n",
    "    한\n",
    "    강\n",
    "    작\n",
    "    가는\n",
    "    아\n",
    "    시아\n",
    "    여성\n",
    "    으로\n",
    "    는\n",
    "    최초\n",
    "    ,\n",
    "    한국\n",
    "    인\n",
    "    으로\n",
    "    도\n",
    "    최초\n",
    "    로\n",
    "    이\n",
    "    권\n",
    "    위\n",
    "    있는\n",
    "    상\n",
    "    을\n",
    "    받\n",
    "    았습니다\n",
    "    [1]\n",
    "    [2]\n",
    "    [4].\n",
    "\n",
    "\n",
    "    스\n",
    "    웨\n",
    "    덴\n",
    "    한\n",
    "    림\n",
    "    원\n",
    "    은\n",
    "    한\n",
    "    강\n",
    "    작\n",
    "    가\n",
    "    에게\n",
    "    상\n",
    "    을\n",
    "    수\n",
    "    여\n",
    "    한\n",
    "    이유\n",
    "    로\n",
    "    “\n",
    "    역\n",
    "    사\n",
    "    적\n",
    "    트\n",
    "    라우\n",
    "    마\n",
    "    에\n",
    "    맞\n",
    "    서\n",
    "    고\n",
    "    인간\n",
    "    삶\n",
    "    의\n",
    "    연\n",
    "    약\n",
    "    함\n",
    "    을\n",
    "    드\n",
    "    러\n",
    "    내\n",
    "    는\n",
    "    강\n",
    "    렬\n",
    "    한\n",
    "    시\n",
    "    적\n",
    "    산\n",
    "    문\n",
    "    ”\n",
    "    을\n",
    "    썼\n",
    "    다는\n",
    "    점\n",
    "    을\n",
    "    꼽\n",
    "    았습니다\n",
    "    .\n",
    "    그녀\n",
    "    의\n",
    "    작품\n",
    "    은\n",
    "    몸\n",
    "    과\n",
    "    영\n",
    "    혼\n",
    "    ,\n",
    "    산\n",
    "    자\n",
    "    와\n",
    "    죽\n",
    "    은\n",
    "    자\n",
    "    사이\n",
    "    의\n",
    "    연결\n",
    "    에\n",
    "    대한\n",
    "    독\n",
    "    특\n",
    "    한\n",
    "    인\n",
    "    식을\n",
    "    담\n",
    "    고\n",
    "    있으며\n",
    "    ,\n",
    "    시\n",
    "    적\n",
    "    이고\n",
    "    실\n",
    "    험\n",
    "    적인\n",
    "    스타일\n",
    "    로\n",
    "    현대\n",
    "    산\n",
    "    문의\n",
    "    혁\n",
    "    신\n",
    "    을\n",
    "    이\n",
    "    끌\n",
    "    었다\n",
    "    고\n",
    "    평가\n",
    "    받\n",
    "    았습니다\n",
    "    [2]\n",
    "    [4].\n",
    "\n",
    "\n",
    "    한\n",
    "    강\n",
    "    은\n",
    "    \n",
    "    53\n",
    "    세\n",
    "    의\n",
    "    비교\n",
    "    적\n",
    "    젊\n",
    "    은\n",
    "    나\n",
    "    이에\n",
    "    수\n",
    "    상\n",
    "    했\n",
    "    으며\n",
    "    ,\n",
    "    이는\n",
    "    노\n",
    "    벨\n",
    "    문\n",
    "    학\n",
    "    상\n",
    "    수\n",
    "    상\n",
    "    자\n",
    "    평균\n",
    "    연\n",
    "    령\n",
    "    들\n",
    "    에\n",
    "    비\n",
    "    해\n",
    "    이\n",
    "    례\n",
    "    적\n",
    "    입니다\n",
    "    .\n",
    "    국내\n",
    "    외\n",
    "    에서\n",
    "    부\n",
    "    커\n",
    "    상\n",
    "    등\n",
    "    여러\n",
    "    국제\n",
    "    문\n",
    "    학\n",
    "    상을\n",
    "    받은\n",
    "    경\n",
    "    력\n",
    "    과\n",
    "    더\n",
    "    불\n",
    "    어\n",
    "    이번\n",
    "    노\n",
    "    벨\n",
    "    문\n",
    "    학\n",
    "    상\n",
    "    수\n",
    "    상\n",
    "    은\n",
    "    한국\n",
    "    문\n",
    "    학\n",
    "    과\n",
    "    문화\n",
    "    의\n",
    "    세계\n",
    "    적\n",
    "    위\n",
    "    상을\n",
    "    높\n",
    "    이는\n",
    "    큰\n",
    "    성\n",
    "    과\n",
    "    로\n",
    "    여\n",
    "    겨\n",
    "    집\n",
    "    니다\n",
    "    [1]\n",
    "    [4].\n",
    "\n",
    "\n",
    "    한\n",
    "    강\n",
    "    작\n",
    "    가\n",
    "    의\n",
    "    대표\n",
    "    작\n",
    "    으로\n",
    "    는\n",
    "    『\n",
    "    채\n",
    "    식\n",
    "    주의\n",
    "    자\n",
    "    』\n",
    "    가\n",
    "    널\n",
    "    리\n",
    "    알려\n",
    "    져\n",
    "    있으며\n",
    "    ,\n",
    "    이\n",
    "    작품\n",
    "    을\n",
    "    통해\n",
    "    전\n",
    "    세계\n",
    "    독\n",
    "    자\n",
    "    들에게\n",
    "    큰\n",
    "    반\n",
    "    향\n",
    "    을\n",
    "    일\n",
    "    으\n",
    "    켰\n",
    "    습니다\n",
    "    [3].\n",
    "    \n",
    "\n",
    "\n",
    "    요\n",
    "    약\n",
    "    하면\n",
    "    ,\n",
    "    \n",
    "    202\n",
    "    4\n",
    "    년\n",
    "    노\n",
    "    벨\n",
    "    문\n",
    "    학\n",
    "    상\n",
    "    은\n",
    "    한국\n",
    "    의\n",
    "    한\n",
    "    강\n",
    "    작\n",
    "    가\n",
    "    가\n",
    "    수\n",
    "    상\n",
    "    했\n",
    "    으며\n",
    "    ,\n",
    "    그녀\n",
    "    는\n",
    "    역사\n",
    "    적\n",
    "    트\n",
    "    라우\n",
    "    마\n",
    "    와\n",
    "    인간\n",
    "    의\n",
    "    연\n",
    "    약\n",
    "    함\n",
    "    을\n",
    "    주\n",
    "    제로\n",
    "    한\n",
    "    시\n",
    "    적\n",
    "    산\n",
    "    문\n",
    "    으로\n",
    "    큰\n",
    "    평가\n",
    "    를\n",
    "    받\n",
    "    았습니다\n",
    "    .\n",
    "    이는\n",
    "    한국\n",
    "    문\n",
    "    학\n",
    "    사\n",
    "    에서\n",
    "    매우\n",
    "    중요한\n",
    "    이\n",
    "    정\n",
    "    표\n",
    "    가\n",
    "    되\n",
    "    었습니다\n",
    "    [1]\n",
    "    [2]\n",
    "    [4].\n",
    "\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443baa6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용할 수 있는 모델 리스트 조회\n",
    "print(client.models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f97d0d",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 셀 출력\n",
    "\n",
    "    ```python\n",
    "    OrderedDict({'sonar-reasoning-pro': ModelInfo(parameterCount='8B', contextLength=127072, modelType='Sonar', availability='Perplexity'), 'sonar-reasoning': ModelInfo(parameterCount='8B', contextLength=127072, modelType='Sonar', availability='Perplexity'), 'sonar-pro': ModelInfo(parameterCount='8B', contextLength=127072, modelType='Sonar', availability='Perplexity'), 'sonar': ModelInfo(parameterCount='8B', contextLength=127072, modelType='Sonar', availability='Perplexity'), 'r1-1776': ModelInfo(parameterCount='8B', contextLength=127072, modelType='Sonar', availability='Perplexity')})\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1b08af",
   "metadata": {},
   "source": [
    "* 교재에 있는 모델과 다름을 확인\n",
    "\n",
    "  * `sonar-reasoning-pro`\n",
    "\n",
    "  * `sonar-reasoning`\n",
    "\n",
    "  * `sonar-pro`\n",
    "\n",
    "  * `sonar`\n",
    "\n",
    "  * `r1-1776`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec198991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용할 수 있는 모델 중 sonar로 시도\n",
    "# 이전 시도에서 에러가 났던 매개변수 제거\n",
    "\n",
    "from langchain_perplexity import ChatPerplexity\n",
    "import os\n",
    "\n",
    "# API 키 환경 변수에서 불러오기\n",
    "api_key = os.getenv(\"PPLX_API_KEY\")\n",
    "\n",
    "# 사용할 모델 중 하나 선택 (예: sonar-pro)\n",
    "perplexity = ChatPerplexity(\n",
    "    model=\"sonar-pro\",                      # 사용할 모델 지정\n",
    "    temperature=0.7,                        # 텍스트 다양성 조정\n",
    "    top_p=0.9,                              # 샘플링 파라미터\n",
    "    streaming=False                         # 스트리밍 여부 설정\n",
    ")\n",
    "\n",
    "# 메시지 입력 예시\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"2024년 노벨문학상 수상자를 조사해 주세요\"}\n",
    "]\n",
    "\n",
    "# Perplexity API 호출\n",
    "response = perplexity.invoke(messages)\n",
    "\n",
    "# 응답 내용 출력\n",
    "print(response.content)\n",
    "\n",
    "# 응답에서 인용된 출처들 출력\n",
    "print(\"\\n--- 인용된 출처 ---\")\n",
    "for i, citation in enumerate(response.citations):\n",
    "    print(f\"[{i+1}] {citation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c835f7",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 셀 출력 (5.2s)\n",
    "\n",
    "    ```markdown\n",
    "    WARNING! top_p is not a default parameter.\n",
    "                        top_p was transferred to model_kwargs.\n",
    "                        Please confirm that top_p is what you intended.\n",
    "    2024년 노벨문학상 수상자는 **한국의 소설가 한강**입니다[1][2][3][4][5].\n",
    "\n",
    "    한강은 한국인 최초로 노벨문학상을 수상했으며, 아시아 여성 작가로서도 최초의 수상자입니다[2][5]. 스웨덴 한림원은 2024년 10월 10일(현지 시각) 한강의 수상 이유로 \"**역사적 트라우마에 맞서고 인간 삶의 연약함을 폭로하는 강렬한 시적 산문**\"을 꼽았습니다[2][3][4]. 한강은 이미 2016년 소설 『채식주의자』로 맨부커상(Man Booker International Prize)을 수상하며 세계적으로 주목받은 바 있습니다[2].\n",
    "\n",
    "    이번 수상은 2000년 고(故) 김대중 대통령의 노벨평화상 이후 24년 만에 한국인으로서는 두 번째 노벨상 수상이기도 합니다[4][5]. 한강은 121번째 노벨문학상 수상자이자, 18번째 여성 수상자입니다[2][4].\n",
    "\n",
    "    --- 인용된 출처 ---\n",
    "    ```\n",
    "\n",
    "* 오류 메시지\n",
    "\n",
    "    ```python\n",
    "    ---------------------------------------------------------------------------\n",
    "    AttributeError                            Traceback (most recent call last)\n",
    "    Cell In[63], line 31\n",
    "        29 # 응답에서 인용된 출처들 출력\n",
    "        30 print(\"\\n--- 인용된 출처 ---\")\n",
    "    ---> 31 for i, citation in enumerate(response.citations):\n",
    "        32     print(f\"[{i+1}] {citation}\")\n",
    "\n",
    "    File ~/.pyenv/versions/lc_env/lib/python3.13/site-packages/pydantic/main.py:991, in BaseModel.__getattr__(self, item)\n",
    "        988     return super().__getattribute__(item)  # Raises AttributeError if appropriate\n",
    "        989 else:\n",
    "        990     # this is the current error\n",
    "    --> 991     raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}')\n",
    "\n",
    "    AttributeError: 'AIMessage' object has no attribute 'citations'\n",
    "\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8ecb32",
   "metadata": {},
   "source": [
    "* [참고 가이드](https://docs.perplexity.ai/guides/chat-completions-guide?utm_source=chatgpt.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce831b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용할 수 있는 모델 중 sonar로 시도\n",
    "# 이전 시도에서 에러가 났던 매개변수 제거\n",
    "\n",
    "from langchain_perplexity import ChatPerplexity\n",
    "import os\n",
    "import re                                   # re 모듈 임포트해서 정규 표현식 기능 사용 가능하도록 함\n",
    "\n",
    "# API 키 환경 변수에서 불러오기\n",
    "api_key = os.getenv(\"PPLX_API_KEY\")\n",
    "\n",
    "# 사용할 모델 중 하나 선택 (예: sonar-pro)\n",
    "perplexity = ChatPerplexity(\n",
    "    model=\"sonar-pro\",                      # 사용할 모델 지정\n",
    "    temperature=0.7,                        # 텍스트 다양성 조정\n",
    "    #top_p=0.9,                             # 주석 처리 (오류 방지)\n",
    "    streaming=False                         # 스트리밍 여부 설정\n",
    ")\n",
    "\n",
    "# 메시지 입력 예시\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"2024년 노벨문학상 수상자를 조사해 주세요\"}\n",
    "]\n",
    "\n",
    "# Perplexity API 호출\n",
    "response = perplexity.invoke(messages)\n",
    "\n",
    "# 응답 내용 출력\n",
    "print(response.content)\n",
    "\n",
    "# 응답에서 인용된 출처들 추출 (정규 표현식을 사용하여 [1][2][3] 등의 형식 추출)\n",
    "citations = re.findall(r'\\[(\\d+)\\]', response.content)\n",
    "\n",
    "# 인용된 출처 출력\n",
    "print(\"\\n--- 인용된 출처 ---\")\n",
    "for i, citation in enumerate(citations):\n",
    "    print(f\"[{i+1}] {citation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a0c7ae",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "* 셀 출력 (8.9s)\n",
    "\n",
    "    ```markdown\n",
    "    2024년 노벨문학상은 **한국의 소설가 한강**이 수상했습니다[1][2][3][4].\n",
    "\n",
    "    한강은 2024년 10월 스웨덴 한림원으로부터 “역사적 트라우마에 맞서고 인간 삶의 연약함을 폭로하는 강렬한 시적 산문”을 쓴 작가로 선정되었으며, 한국인으로는 최초의 노벨문학상 수상자입니다[2][3][4]. 한강은 2016년 『채식주의자』로 맨부커 인터내셔널상을 수상한 경력이 있으며, 이번 수상은 2000년 김대중 전 대통령의 노벨평화상 이후 두 번째 한국인의 노벨상 수상 기록입니다[2][3][4]. \n",
    "\n",
    "    또한 한강은 아시아 여성 작가로는 최초로 노벨문학상을 수상했으며, 전체 역대 121번째, 여성으로는 18번째 수상자입니다[2][3][4]. 스웨덴 한림원은 한강의 문학 세계를 “몸과 영혼, 산 자와 죽은 자 사이의 연결에 대한 독특한 인식”과 “시적이고 실험적인 스타일로 현대 산문의 혁신가”라고 평가했습니다[3].\n",
    "\n",
    "    --- 인용된 출처 ---\n",
    "    [1] 1\n",
    "    [2] 2\n",
    "    [3] 3\n",
    "    [4] 4\n",
    "    [5] 2\n",
    "    [6] 3\n",
    "    [7] 4\n",
    "    [8] 2\n",
    "    [9] 3\n",
    "    [10] 4\n",
    "    [11] 2\n",
    "    [12] 3\n",
    "    [13] 4\n",
    "    [14] 3\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4deb3703",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b621a7",
   "metadata": {},
   "source": [
    "* *next: **`Cohere`***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
